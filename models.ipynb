{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle \n",
    "\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.util import pr\n",
    "from nltk.corpus import stopwords\n",
    "import string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum text\n",
    "# sb.set()\n",
    "# pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SE2019 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "Non-Hate    5195\n",
       "Hate        3781\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Dataset/SE2019/notstemmer_data.csv\")\n",
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "      <th>hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hate</td>\n",
       "      <td>hurray saving us many ways #lockthemup #buildt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hate</td>\n",
       "      <td>would young fighting age men vast majority one...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hate</td>\n",
       "      <td>illegals dump kids border like road kill refu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Non-Hate</td>\n",
       "      <td>ny times nearly white states pose array proble...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Non-Hate</td>\n",
       "      <td>orban brussels european leaders ignoring peopl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      class                                               text  hate\n",
       "0      Hate  hurray saving us many ways #lockthemup #buildt...     1\n",
       "1      Hate  would young fighting age men vast majority one...     1\n",
       "2      Hate   illegals dump kids border like road kill refu...     1\n",
       "3  Non-Hate  ny times nearly white states pose array proble...     0\n",
       "4  Non-Hate  orban brussels european leaders ignoring peopl...     0"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "      <th>hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [class, text, hate]\n",
       "Index: []"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['text'].isnull()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set : (6283,) (6283,)\n",
      "Test Set  : (2693,) (2693,)\n"
     ]
    }
   ],
   "source": [
    "x = np.array(df[\"text\"])\n",
    "y = np.array(df[\"class\"])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.30, random_state=42) #random state ensure same sample\n",
    "print(\"Train Set :\", x_train.shape, y_train.shape) \n",
    "print(\"Test Set  :\", x_test.shape, y_test.shape) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "array(['austria wonder countri refug bill see socialwelfar also help get televisionset refug famili styria ',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['austria wonderful country refugees bill see socialwelfare also help get televisionset refugees family styria ',\n",
       "       'please tell bitch next piercing line judgmental everyone fucking sees shut fuck',\n",
       "       'afghan migrant whose deportation thwarted hero swedish student actually sentenced assault #foxnews',\n",
       "       ' slavery post white women rape many black women men children u could ever imagine cause ',\n",
       "       'bewildered eu leaders various plans preventing migrant refugee boat arrivals amp send heres helpful overview ',\n",
       "       'waking today like ',\n",
       "       'theyre sending best lot rapists scumbags lowest form dna fake family separatedxxf#stoptheinvasion#deportthemall #noamnesty#buildthewall ',\n",
       "       'rt #chelseahandler #kimkardashian #kanye #kanyewest #cent #comedy #lol #lmao #memes #bruh #petty #funnyshit #truth ',\n",
       "       'douglas todd trudeau government goes silent canadas syrian refugees via add alarming list governments fails ',\n",
       "       'redhead girls hot ginger girls usualy fugly'], dtype=object)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering - Word embeding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\n",
    "- CountVectorizer, Tfidftransformer & Tfidfvectorizer are Frequency based Word Embedding technique\n",
    "- Tfidftransformer acts on sparse matrix and Tfidfvectorizer acts on raw text data\n",
    "- Tfidfvectorizer = countVectorizater + Tfidftransformer\n",
    "\n",
    "- https://www.analyticsvidhya.com/blog/2018/07/hands-on-sentiment-analysis-dataset-python/\n",
    "- vectorizer = word embedding process of converting text data to numerical vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://spotintelligence.com/2023/02/15/word2vec-for-text-classification/#:~:text=Word2Vec%20is%20a%20popular%20algorithm,a%20large%20corpus%20of%20text\n",
    "- Word2vec is not a single algorithm but a combination of two techniques â€“ CBOW(Continuous bag of words) and Skip-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class w2vVectorizer():\n",
    "    def __init__(self) -> None:\n",
    "        self.w2v_model = None\n",
    "    \n",
    "    def w2v_vectorizer(self,sentence):\n",
    "        # vectorize the text data\n",
    "        words = sentence.split()\n",
    "        words_vec = [self.w2v_model.wv[word] for word in words if word in self.w2v_model.wv]\n",
    "        if len(words_vec) == 0:\n",
    "            return np.zeros(100)\n",
    "        words_vec = np.array(words_vec)\n",
    "        return words_vec.mean(axis=0)\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        # train the model when fit the pipeline\n",
    "        sentences = [sentence.split() for sentence in x]\n",
    "        self.w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=2, workers=4)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x, y=None):\n",
    "        # when use fit or transform on the pipeline \n",
    "        return np.array([self.w2v_vectorizer(sentence) for sentence in x])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "def save_model(model, model_name):\n",
    "    filename = f\"models/{model_name}.pickle\"\n",
    "    pickle.dump(model, open(filename,\"wb\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert given text to a vector base\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([('vect', CountVectorizer()),\n",
    "               ('clf', DecisionTreeClassifier()),\n",
    "              ])\n",
    "model_name = \"dtc\"\n",
    "model.fit(x_train, y_train)\n",
    "save_model(model,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([('vect', TfidfVectorizer()),\n",
    "               ('clf', DecisionTreeClassifier()),\n",
    "              ])\n",
    "model_name = \"dtc-tfid\"\n",
    "model.fit(x_train, y_train)\n",
    "save_model(model,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([('vect', w2vVectorizer()),\n",
    "               ('clf', DecisionTreeClassifier()),\n",
    "              ])\n",
    "model_name = \"dtc-w2v\"\n",
    "model.fit(x_train, y_train)\n",
    "save_model(model,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- supervisied learning algorithm\n",
    "- Unlike neural networks, SVMs can work with very small datasets and are not prone to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([('vect', CountVectorizer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "               ])\n",
    "model_name = \"svm\"\n",
    "model.fit(x_train, y_train)\n",
    "save_model(model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([('vect', TfidfVectorizer()),\n",
    "               ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "              ])\n",
    "model_name = \"svm-tfid\"\n",
    "model.fit(x_train, y_train)\n",
    "save_model(model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([('vect', w2vVectorizer()),\n",
    "               ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "              ])\n",
    "model_name = \"svm-w2v\"\n",
    "model.fit(x_train, y_train)\n",
    "save_model(model, model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([('vect', CountVectorizer()),\n",
    "        ('clf', LogisticRegression(n_jobs=1, C=1e5,max_iter=6300)),\n",
    "        ])\n",
    "model_name = \"lr\"\n",
    "model.fit(x_train, y_train)\n",
    "save_model(model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([('vect', TfidfVectorizer()),\n",
    "        ('clf', LogisticRegression(n_jobs=1, C=1e5,max_iter=6300)),\n",
    "        ])\n",
    "model_name = \"lr-tfid\"\n",
    "model.fit(x_train, y_train)\n",
    "save_model(model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([('vect', w2vVectorizer()),\n",
    "        ('clf', LogisticRegression(n_jobs=1, C=1e5,max_iter=6300)),\n",
    "        ])\n",
    "model_name = \"lr-w2v\"\n",
    "model.fit(x_train, y_train)\n",
    "save_model(model, model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import one_hot, Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.utils.data_utils import pad_sequences\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, GlobalMaxPooling1D, Embedding, Conv1D, LSTM\n",
    "\n",
    "from keras.metrics import BinaryAccuracy,Precision,Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_no = np.array(list(map(lambda x:1 if x==\"Hate\" else 0, y_train)))\n",
    "y_test_no = np.array(list(map(lambda x:1 if x==\"Hate\" else 0, y_test)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeding layer convert text to numeric form which is used as the first layer for the deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "x_train_token = word_tokenizer.texts_to_sequences(x_train)\n",
    "x_test_token = word_tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 5201 6283\n"
     ]
    }
   ],
   "source": [
    "# max word in a sentences\n",
    "maxx = 0\n",
    "index = 0\n",
    "count = 0\n",
    "for i in x_train_token:\n",
    "    le = len(i)\n",
    "    if le > maxx: \n",
    "        maxx=le\n",
    "        index = count\n",
    "    count+=1\n",
    "\n",
    "print(maxx, index, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14084\n"
     ]
    }
   ],
   "source": [
    "vocab_length = len(word_tokenizer.word_index) + 1\n",
    "print(vocab_length)\n",
    "# can set higher depend, find from above, max is 38 word so roughly use 50 word\n",
    "maxlen = 50 \n",
    "\n",
    "# pad so all text is 50 length\n",
    "x_train_pad = pad_sequences(x_train_token, padding = 'post', maxlen=maxlen)\n",
    "x_test_pad = pad_sequences(x_test_token, padding = 'post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6283"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove embedding \n",
    "embeddings_dic = dict()\n",
    "glove_file = open(\"Dataset/glove_embedding.txt\", encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "    embeddings_dic[word] = vector_dimensions\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_dic['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding matrix having 100 col\n",
    "# for all vocab word in tokenizer we give it a vector from glove\n",
    "# for those not found in glove will be empty 0\n",
    "embbedding_matrix = np.zeros((vocab_length, 100))\n",
    "for word, index in word_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dic.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embbedding_matrix[index] = embedding_vector\n",
    "        \n",
    "embbedding_matrix.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "embedding_layer = Embedding(vocab_length, 100, weights = [embbedding_matrix], input_length=maxlen, trainable=False)\n",
    "\n",
    "model.add(embedding_layer)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "    BinaryAccuracy(name=\"accuracy\"),\n",
    "    Precision(name=\"precision\"),\n",
    "    Recall(name=\"recall\")\n",
    "]\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 50, 100)           1408400   \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 5000)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 5001      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,413,401\n",
      "Trainable params: 5,001\n",
      "Non-trainable params: 1,408,400\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4935 - accuracy: 0.7582 - precision: 0.7465 - recall: 0.6472\n",
      "Epoch 2/20\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.4890 - accuracy: 0.7614 - precision: 0.7481 - recall: 0.6559\n",
      "Epoch 3/20\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.4840 - accuracy: 0.7665 - precision: 0.7407 - recall: 0.6879\n",
      "Epoch 4/20\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.4804 - accuracy: 0.7689 - precision: 0.7628 - recall: 0.6570\n",
      "Epoch 5/20\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.4747 - accuracy: 0.7751 - precision: 0.7647 - recall: 0.6751\n",
      "Epoch 6/20\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.4714 - accuracy: 0.7735 - precision: 0.7573 - recall: 0.6822\n",
      "Epoch 7/20\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.4686 - accuracy: 0.7769 - precision: 0.7640 - recall: 0.6822\n",
      "Epoch 8/20\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.4647 - accuracy: 0.7770 - precision: 0.7600 - recall: 0.6898\n",
      "Epoch 9/20\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.4617 - accuracy: 0.7799 - precision: 0.7675 - recall: 0.6868\n",
      "Epoch 10/20\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.4592 - accuracy: 0.7807 - precision: 0.7693 - recall: 0.6864\n",
      "Epoch 11/20\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.4557 - accuracy: 0.7821 - precision: 0.7709 - recall: 0.6887\n",
      "Epoch 12/20\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.4544 - accuracy: 0.7837 - precision: 0.7680 - recall: 0.6988\n",
      "Epoch 13/20\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.4516 - accuracy: 0.7832 - precision: 0.7680 - recall: 0.6973\n",
      "Epoch 14/20\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.4495 - accuracy: 0.7874 - precision: 0.7766 - recall: 0.6969\n",
      "Epoch 15/20\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.4469 - accuracy: 0.7872 - precision: 0.7751 - recall: 0.6988\n",
      "Epoch 16/20\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.4446 - accuracy: 0.7904 - precision: 0.7689 - recall: 0.7199\n",
      "Epoch 17/20\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.4429 - accuracy: 0.7896 - precision: 0.7797 - recall: 0.6992\n",
      "Epoch 18/20\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.4408 - accuracy: 0.7933 - precision: 0.7750 - recall: 0.7192\n",
      "Epoch 19/20\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.4384 - accuracy: 0.7934 - precision: 0.7822 - recall: 0.7079\n",
      "Epoch 20/20\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.4376 - accuracy: 0.7915 - precision: 0.7799 - recall: 0.7052\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f9086d0210>"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_pad, y_train_no, batch_size=128, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 0s 1ms/step - loss: 0.6589 - accuracy: 0.6554 - precision: 0.5896 - recall: 0.5833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.658920407295227, 0.6554028987884521, 0.5896057486534119, 0.5833333134651184]"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test_pad, y_test_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = model.predict(x_test_pad)\n",
    "y_test_pred.flatten()\n",
    "\n",
    "y_test_pred = np.where(y_test_pred > 0.5, 1, 0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.71      0.70      1565\n",
      "           1       0.59      0.58      0.59      1128\n",
      "\n",
      "    accuracy                           0.66      2693\n",
      "   macro avg       0.65      0.65      0.65      2693\n",
      "weighted avg       0.65      0.66      0.66      2693\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_no, y_test_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN/RNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270\n",
    "- https://www.youtube.com/watch?v=hOCDJyZ6quA\n",
    "- tensorflow hub bert https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\n",
    "- bert will convert sentence into embeding vector which will feed to neural network for training \n",
    "- consist of preprocess and embeding \n",
    "- (4)BERT-RNN: The corresponding representational word vectors were trained by BERT model for the input text, which were then classified by RNN neural network. (5)word2vec-RNN: This model is a traditional text classification model. 4.3.\n",
    "- BERT is a neural-network-based technique for language processing pre-training\n",
    "- it is not a classification algorithm \n",
    "- BERT generates <b>contextual embeddings</b>, the input to the model is a sentence rather than a single word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict data\n",
    "print(\"Test Data Accuracy  :\\t\", model.score(x_test, y_test))\n",
    "y_test_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Hate       0.69      0.67      0.68      1115\n",
      "    Non-Hate       0.77      0.78      0.78      1583\n",
      "\n",
      "    accuracy                           0.74      2698\n",
      "   macro avg       0.73      0.73      0.73      2698\n",
      "weighted avg       0.74      0.74      0.74      2698\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report\n",
    "print(classification_report(y_test, y_test_pred, labels=[\"Hate\",\"Non-Hate\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap\n",
    "ax = plt.subplot()\n",
    "\n",
    "# Plot the two-way Confusion Matrix\n",
    "sb.heatmap(confusion_matrix(y_test, y_test_pred, labels=[\"Hate\",\"Non-Hate\"]), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax=ax)\n",
    "\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual')\n",
    "ax.xaxis.set_ticklabels([\"Hate\",\"Non-Hate\"])\n",
    "ax.yaxis.set_ticklabels([\"Hate\",\"Non-Hate\"])\n",
    "\n",
    "# Count\n",
    "df1 = pd.DataFrame({'Actual':y_test, 'Predict':y_test_pred})\n",
    "# print(df1.describe())\n",
    "print(f\"Count: {df1['Actual'].value_counts()}\")\n",
    "print()\n",
    "print(f\"Count: {df1['Predict'].value_counts()}\")\n",
    "print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def get_score(y_test, y_test_pred):\n",
    "    # print(f\"Accuracy: {accuracy_score(y_test, y_test_pred):.2f}\")\n",
    "    # score = precision_recall_fscore_support(y_test, y_test_pred, average=\"weighted\")\n",
    "    # print(f\"Precision: {score[0]:.2f}\")\n",
    "    # print(f\"Recall: {score[1]:.2f}\")\n",
    "    # print(f\"F1-score: {score[2]:.2f}\")\n",
    "    # print()\n",
    "    a = accuracy_score(y_test, y_test_pred)\n",
    "    prf = precision_recall_fscore_support(y_test, y_test_pred, average=\"weighted\")\n",
    "    return a, prf[0], prf[1], prf[2]\n",
    "    # print(precision_recall_fscore_support(y_test, y_test_pred, average=\"micro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "def get_result():\n",
    "    model_to_load = [\"dtc\", \"dtc-tfid\", \"dtc-w2v\",\"svm\", \"svm-tfid\", \"svm-w2v\", \"lr\", \"lr-tfid\",\"lr-w2v\"]\n",
    "    c = [\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"]\n",
    "    result_table = pd.DataFrame(columns=c)\n",
    "\n",
    "    for i in model_to_load:\n",
    "        filename = f\"models/{i}.pickle\"\n",
    "        old_model = pickle.load(open(filename,\"rb\"))\n",
    "        \n",
    "        y_test_pred = old_model.predict(x_test)\n",
    "\n",
    "        a = accuracy_score(y_test, y_test_pred)\n",
    "        prf = precision_recall_fscore_support(y_test, y_test_pred, average=\"weighted\")\n",
    "\n",
    "        result_table.loc[len(result_table)] = [i, a, prf[0], prf[1], prf[2]]\n",
    "\n",
    "    return result_table.style.highlight_max(color = 'red', axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_52f0a_row3_col1, #T_52f0a_row3_col2, #T_52f0a_row3_col3, #T_52f0a_row3_col4, #T_52f0a_row5_col0 {\n",
       "  background-color: red;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_52f0a\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_52f0a_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_52f0a_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_52f0a_level0_col2\" class=\"col_heading level0 col2\" >Precision</th>\n",
       "      <th id=\"T_52f0a_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_52f0a_level0_col4\" class=\"col_heading level0 col4\" >F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_52f0a_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_52f0a_row0_col0\" class=\"data row0 col0\" >dtc</td>\n",
       "      <td id=\"T_52f0a_row0_col1\" class=\"data row0 col1\" >0.749350</td>\n",
       "      <td id=\"T_52f0a_row0_col2\" class=\"data row0 col2\" >0.747932</td>\n",
       "      <td id=\"T_52f0a_row0_col3\" class=\"data row0 col3\" >0.749350</td>\n",
       "      <td id=\"T_52f0a_row0_col4\" class=\"data row0 col4\" >0.748249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_52f0a_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_52f0a_row1_col0\" class=\"data row1 col0\" >dtc-tfid</td>\n",
       "      <td id=\"T_52f0a_row1_col1\" class=\"data row1 col1\" >0.743780</td>\n",
       "      <td id=\"T_52f0a_row1_col2\" class=\"data row1 col2\" >0.743531</td>\n",
       "      <td id=\"T_52f0a_row1_col3\" class=\"data row1 col3\" >0.743780</td>\n",
       "      <td id=\"T_52f0a_row1_col4\" class=\"data row1 col4\" >0.743649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_52f0a_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_52f0a_row2_col0\" class=\"data row2 col0\" >dtc-w2v</td>\n",
       "      <td id=\"T_52f0a_row2_col1\" class=\"data row2 col1\" >0.586706</td>\n",
       "      <td id=\"T_52f0a_row2_col2\" class=\"data row2 col2\" >0.589898</td>\n",
       "      <td id=\"T_52f0a_row2_col3\" class=\"data row2 col3\" >0.586706</td>\n",
       "      <td id=\"T_52f0a_row2_col4\" class=\"data row2 col4\" >0.588024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_52f0a_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_52f0a_row3_col0\" class=\"data row3 col0\" >svm</td>\n",
       "      <td id=\"T_52f0a_row3_col1\" class=\"data row3 col1\" >0.768659</td>\n",
       "      <td id=\"T_52f0a_row3_col2\" class=\"data row3 col2\" >0.767394</td>\n",
       "      <td id=\"T_52f0a_row3_col3\" class=\"data row3 col3\" >0.768659</td>\n",
       "      <td id=\"T_52f0a_row3_col4\" class=\"data row3 col4\" >0.766195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_52f0a_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_52f0a_row4_col0\" class=\"data row4 col0\" >svm-tfid</td>\n",
       "      <td id=\"T_52f0a_row4_col1\" class=\"data row4 col1\" >0.746008</td>\n",
       "      <td id=\"T_52f0a_row4_col2\" class=\"data row4 col2\" >0.760112</td>\n",
       "      <td id=\"T_52f0a_row4_col3\" class=\"data row4 col3\" >0.746008</td>\n",
       "      <td id=\"T_52f0a_row4_col4\" class=\"data row4 col4\" >0.731861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_52f0a_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_52f0a_row5_col0\" class=\"data row5 col0\" >svm-w2v</td>\n",
       "      <td id=\"T_52f0a_row5_col1\" class=\"data row5 col1\" >0.590048</td>\n",
       "      <td id=\"T_52f0a_row5_col2\" class=\"data row5 col2\" >0.611645</td>\n",
       "      <td id=\"T_52f0a_row5_col3\" class=\"data row5 col3\" >0.590048</td>\n",
       "      <td id=\"T_52f0a_row5_col4\" class=\"data row5 col4\" >0.461541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_52f0a_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_52f0a_row6_col0\" class=\"data row6 col0\" >lr</td>\n",
       "      <td id=\"T_52f0a_row6_col1\" class=\"data row6 col1\" >0.684738</td>\n",
       "      <td id=\"T_52f0a_row6_col2\" class=\"data row6 col2\" >0.690064</td>\n",
       "      <td id=\"T_52f0a_row6_col3\" class=\"data row6 col3\" >0.684738</td>\n",
       "      <td id=\"T_52f0a_row6_col4\" class=\"data row6 col4\" >0.686326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_52f0a_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_52f0a_row7_col0\" class=\"data row7 col0\" >lr-tfid</td>\n",
       "      <td id=\"T_52f0a_row7_col1\" class=\"data row7 col1\" >0.694393</td>\n",
       "      <td id=\"T_52f0a_row7_col2\" class=\"data row7 col2\" >0.697312</td>\n",
       "      <td id=\"T_52f0a_row7_col3\" class=\"data row7 col3\" >0.694393</td>\n",
       "      <td id=\"T_52f0a_row7_col4\" class=\"data row7 col4\" >0.695449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_52f0a_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_52f0a_row8_col0\" class=\"data row8 col0\" >lr-w2v</td>\n",
       "      <td id=\"T_52f0a_row8_col1\" class=\"data row8 col1\" >0.708504</td>\n",
       "      <td id=\"T_52f0a_row8_col2\" class=\"data row8 col2\" >0.705758</td>\n",
       "      <td id=\"T_52f0a_row8_col3\" class=\"data row8 col3\" >0.708504</td>\n",
       "      <td id=\"T_52f0a_row8_col4\" class=\"data row8 col4\" >0.705809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1f954ed6450>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_2fd06_row3_col1, #T_2fd06_row3_col2, #T_2fd06_row3_col3, #T_2fd06_row3_col4, #T_2fd06_row5_col0 {\n",
       "  background-color: red;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_2fd06\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_2fd06_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_2fd06_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_2fd06_level0_col2\" class=\"col_heading level0 col2\" >Precision</th>\n",
       "      <th id=\"T_2fd06_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_2fd06_level0_col4\" class=\"col_heading level0 col4\" >F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_2fd06_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_2fd06_row0_col0\" class=\"data row0 col0\" >dtc</td>\n",
       "      <td id=\"T_2fd06_row0_col1\" class=\"data row0 col1\" >0.762347</td>\n",
       "      <td id=\"T_2fd06_row0_col2\" class=\"data row0 col2\" >0.762231</td>\n",
       "      <td id=\"T_2fd06_row0_col3\" class=\"data row0 col3\" >0.762347</td>\n",
       "      <td id=\"T_2fd06_row0_col4\" class=\"data row0 col4\" >0.762287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2fd06_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_2fd06_row1_col0\" class=\"data row1 col0\" >dtc-tfid</td>\n",
       "      <td id=\"T_2fd06_row1_col1\" class=\"data row1 col1\" >0.744523</td>\n",
       "      <td id=\"T_2fd06_row1_col2\" class=\"data row1 col2\" >0.744717</td>\n",
       "      <td id=\"T_2fd06_row1_col3\" class=\"data row1 col3\" >0.744523</td>\n",
       "      <td id=\"T_2fd06_row1_col4\" class=\"data row1 col4\" >0.744616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2fd06_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_2fd06_row2_col0\" class=\"data row2 col0\" >dtc-w2v</td>\n",
       "      <td id=\"T_2fd06_row2_col1\" class=\"data row2 col1\" >0.583364</td>\n",
       "      <td id=\"T_2fd06_row2_col2\" class=\"data row2 col2\" >0.585593</td>\n",
       "      <td id=\"T_2fd06_row2_col3\" class=\"data row2 col3\" >0.583364</td>\n",
       "      <td id=\"T_2fd06_row2_col4\" class=\"data row2 col4\" >0.584338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2fd06_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_2fd06_row3_col0\" class=\"data row3 col0\" >svm</td>\n",
       "      <td id=\"T_2fd06_row3_col1\" class=\"data row3 col1\" >0.773115</td>\n",
       "      <td id=\"T_2fd06_row3_col2\" class=\"data row3 col2\" >0.774587</td>\n",
       "      <td id=\"T_2fd06_row3_col3\" class=\"data row3 col3\" >0.773115</td>\n",
       "      <td id=\"T_2fd06_row3_col4\" class=\"data row3 col4\" >0.768277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2fd06_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_2fd06_row4_col0\" class=\"data row4 col0\" >svm-tfid</td>\n",
       "      <td id=\"T_2fd06_row4_col1\" class=\"data row4 col1\" >0.743780</td>\n",
       "      <td id=\"T_2fd06_row4_col2\" class=\"data row4 col2\" >0.767172</td>\n",
       "      <td id=\"T_2fd06_row4_col3\" class=\"data row4 col3\" >0.743780</td>\n",
       "      <td id=\"T_2fd06_row4_col4\" class=\"data row4 col4\" >0.725699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2fd06_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_2fd06_row5_col0\" class=\"data row5 col0\" >svm-w2v</td>\n",
       "      <td id=\"T_2fd06_row5_col1\" class=\"data row5 col1\" >0.597104</td>\n",
       "      <td id=\"T_2fd06_row5_col2\" class=\"data row5 col2\" >0.605277</td>\n",
       "      <td id=\"T_2fd06_row5_col3\" class=\"data row5 col3\" >0.597104</td>\n",
       "      <td id=\"T_2fd06_row5_col4\" class=\"data row5 col4\" >0.498157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2fd06_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_2fd06_row6_col0\" class=\"data row6 col0\" >lr</td>\n",
       "      <td id=\"T_2fd06_row6_col1\" class=\"data row6 col1\" >0.726328</td>\n",
       "      <td id=\"T_2fd06_row6_col2\" class=\"data row6 col2\" >0.729749</td>\n",
       "      <td id=\"T_2fd06_row6_col3\" class=\"data row6 col3\" >0.726328</td>\n",
       "      <td id=\"T_2fd06_row6_col4\" class=\"data row6 col4\" >0.727414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2fd06_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_2fd06_row7_col0\" class=\"data row7 col0\" >lr-tfid</td>\n",
       "      <td id=\"T_2fd06_row7_col1\" class=\"data row7 col1\" >0.725956</td>\n",
       "      <td id=\"T_2fd06_row7_col2\" class=\"data row7 col2\" >0.728042</td>\n",
       "      <td id=\"T_2fd06_row7_col3\" class=\"data row7 col3\" >0.725956</td>\n",
       "      <td id=\"T_2fd06_row7_col4\" class=\"data row7 col4\" >0.726728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2fd06_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_2fd06_row8_col0\" class=\"data row8 col0\" >lr-w2v</td>\n",
       "      <td id=\"T_2fd06_row8_col1\" class=\"data row8 col1\" >0.705904</td>\n",
       "      <td id=\"T_2fd06_row8_col2\" class=\"data row8 col2\" >0.703003</td>\n",
       "      <td id=\"T_2fd06_row8_col3\" class=\"data row8 col3\" >0.705904</td>\n",
       "      <td id=\"T_2fd06_row8_col4\" class=\"data row8 col4\" >0.700796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1f954c14490>"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_result()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
