{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle \n",
    "\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum text\n",
    "# sb.set()\n",
    "# pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GabHateCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "Non-Hate    0.876664\n",
       "Hate        0.123336\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = \"GabHateCorpus\"\n",
    "filepath = \"Dataset/GabHateCorpus/\"\n",
    "df = pd.read_csv(filepath+\"data_processed.csv\")\n",
    "df['class'].value_counts(normalize=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SE2019 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "Non-Hate    0.578542\n",
       "Hate        0.421458\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = \"SE2019\"\n",
    "filepath = \"Dataset/SE2019/\"\n",
    "df = pd.read_csv(filepath+\"data_processed.csv\")\n",
    "df['class'].value_counts(normalize=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implicit_hate_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "Non-Hate    0.618726\n",
       "Hate        0.381274\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = \"Implicit_hate_corpus\"\n",
    "filepath = \"Dataset/Implicit_hate_corpus/\"\n",
    "df = pd.read_csv(filepath+\"data_processed.csv\")\n",
    "df['class'].value_counts(normalize=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21478 entries, 0 to 21477\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   class   21478 non-null  object\n",
      " 1   text    21478 non-null  object\n",
      " 2   hate    21478 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 503.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "      <th>hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hate</td>\n",
       "      <td>jewish harvard professor noel ignatiev wants ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Non-Hate</td>\n",
       "      <td>bhigher education part european culture impor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Non-Hate</td>\n",
       "      <td>problem whites christians go ahead free say</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Non-Hate</td>\n",
       "      <td>yasir qadhi hate preacher calling christians ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Non-Hate</td>\n",
       "      <td>rt three million germans mass murdered destru...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      class                                               text  hate\n",
       "0      Hate   jewish harvard professor noel ignatiev wants ...     1\n",
       "1  Non-Hate   bhigher education part european culture impor...     0\n",
       "2  Non-Hate       problem whites christians go ahead free say      0\n",
       "3  Non-Hate   yasir qadhi hate preacher calling christians ...     0\n",
       "4  Non-Hate   rt three million germans mass murdered destru...     0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "      <th>hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [class, text, hate]\n",
       "Index: []"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['text'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13000 12974\n"
     ]
    }
   ],
   "source": [
    "filepath = \"Dataset/SE2019/\"\n",
    "df_need = pd.read_csv(filepath+\"data_needed.csv\")\n",
    "df_processed = pd.read_csv(filepath+\"data_processed.csv\")\n",
    "print(len(df_need), len(df_processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_need = np.array(df_need[\"text\"])\n",
    "text_processed = np.array(df_processed[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    counter = 0\n",
    "    for i in text:\n",
    "        counter += len(i.split())\n",
    "    return counter\n",
    "\n",
    "def count_token(text):\n",
    "    s = set()\n",
    "    for i in text:\n",
    "        tokenize = i.split()\n",
    "        for j in tokenize:\n",
    "            s.add(j)    \n",
    "    return len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word\n",
      "need:  281058\n",
      "pro:  156009\n",
      "token\n",
      "need:  53046\n",
      "pro:  22855\n"
     ]
    }
   ],
   "source": [
    "print(\"word\")\n",
    "print(\"need: \", count_words(text_need))\n",
    "print(\"pro: \", count_words(text_processed))\n",
    "\n",
    "print(\"token\")\n",
    "print(\"need: \", count_token(text_need))\n",
    "print(\"pro: \", count_token(text_processed))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change test size to 10 fold cross validation    \n",
    "so test size is 0.10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set : (24401,) (24401,)\n",
      "Test Set  : (2712,) (2712,)\n"
     ]
    }
   ],
   "source": [
    "test_size = 0.10\n",
    "x = np.array(df[\"text\"])\n",
    "y = np.array(df[\"class\"])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = test_size, random_state=42) #random state ensure same sample\n",
    "print(\"Train Set :\", x_train.shape, y_train.shape) \n",
    "print(\"Test Set  :\", x_test.shape, y_test.shape) \n",
    "\n",
    "# y in digit form\n",
    "y_train_binary = np.array(list(map(lambda x:1 if x==\"Hate\" else 0, y_train)))\n",
    "y_test_binary = np.array(list(map(lambda x:1 if x==\"Hate\" else 0, y_test)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.30\n",
    "- se2019 = orange colored scumpig cunt whore god soon go history worst potus brainwas ', <br>\n",
    "- gab = mainstream muslims murder gays ', <br>\n",
    "- implicit = 'would think sweden cops able use snipers shoot flaming bombs hands till crush muslims never stop muslims understand force',\n",
    "\n",
    "10 fold\n",
    "- gab = 'anyone help older samsung tablet get white screen trying log gab download says parsing error bookmarked still wont open even online browser help'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'anyone help older samsung tablet get white screen trying log gab download says parsing error bookmarked still wont open even online browser help'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering - Word embeding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Word2Vec and GloVe are based on static word embeddings while Transformers are based on dynamic word embeddings.\n",
    "\n",
    "- Many different models for creating pre-trained word embeddings such as Word2Vec, GloVe, fastText, and BioWordVec "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\n",
    "- CountVectorizer, Tfidftransformer & Tfidfvectorizer are Frequency based Word Embedding technique\n",
    "- Tfidftransformer acts on sparse matrix and Tfidfvectorizer acts on raw text data\n",
    "- Tfidfvectorizer = countVectorizater + Tfidftransformer\n",
    "\n",
    "- https://www.analyticsvidhya.com/blog/2018/07/hands-on-sentiment-analysis-dataset-python/\n",
    "- vectorizer = word embedding process of converting text data to numerical vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://spotintelligence.com/2023/02/15/word2vec-for-text-classification/#:~:text=Word2Vec%20is%20a%20popular%20algorithm,a%20large%20corpus%20of%20text\n",
    "- Word2vec is not a single algorithm but a combination of two techniques – CBOW(Continuous bag of words) and Skip-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class w2vVectorizer():\n",
    "    def __init__(self) -> None:\n",
    "        self.w2v_model = None\n",
    "    \n",
    "    def w2v_vectorizer(self,sentence):\n",
    "        # vectorize the text data\n",
    "        words = sentence.split()\n",
    "        words_vec = [self.w2v_model.wv[word] for word in words if word in self.w2v_model.wv]\n",
    "        if len(words_vec) == 0:\n",
    "            return np.zeros(100)\n",
    "        words_vec = np.array(words_vec)\n",
    "        return words_vec.mean(axis=0)\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        # train the model when fit the pipeline\n",
    "        sentences = [sentence.split() for sentence in x]\n",
    "        self.w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=2, workers=4)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x, y=None):\n",
    "        # when use fit or transform on the pipeline \n",
    "        return np.array([self.w2v_vectorizer(sentence) for sentence in x])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "def save_model(model, model_name):\n",
    "    filename = f\"models/{model_name}.pickle\"\n",
    "    pickle.dump(model, open(filename,\"wb\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert given text to a vector base\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([('vect', CountVectorizer()),\n",
    "               ('clf', DecisionTreeClassifier()),\n",
    "              ])\n",
    "model_name = \"dtc\"\n",
    "model.fit(x_train, y_train)\n",
    "save_model(model,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([('vect', TfidfVectorizer()),\n",
    "               ('clf', DecisionTreeClassifier()),\n",
    "              ])\n",
    "model_name = \"dtc-tfid\"\n",
    "model.fit(x_train, y_train)\n",
    "save_model(model,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([('vect', w2vVectorizer()),\n",
    "               ('clf', DecisionTreeClassifier()),\n",
    "              ])\n",
    "model_name = \"dtc-w2v\"\n",
    "model.fit(x_train, y_train)\n",
    "save_model(model,model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- supervisied learning algorithm\n",
    "- Unlike neural networks, SVMs can work with very small datasets and are not prone to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([('vect', CountVectorizer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "               ])\n",
    "model_name = \"svm\"\n",
    "model.fit(x_train, y_train)\n",
    "save_model(model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([('vect', TfidfVectorizer()),\n",
    "               ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "              ])\n",
    "model_name = \"svm-tfid\"\n",
    "model.fit(x_train, y_train)\n",
    "save_model(model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([('vect', w2vVectorizer()),\n",
    "               ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "              ])\n",
    "model_name = \"svm-w2v\"\n",
    "model.fit(x_train, y_train)\n",
    "save_model(model, model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([('vect', CountVectorizer()),\n",
    "        ('clf', LogisticRegression(n_jobs=1, C=1e5,max_iter=6300)),\n",
    "        ])\n",
    "model_name = \"lr\"\n",
    "model.fit(x_train, y_train)\n",
    "save_model(model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([('vect', TfidfVectorizer()),\n",
    "        ('clf', LogisticRegression(n_jobs=1, C=1e5,max_iter=6300)),\n",
    "        ])\n",
    "model_name = \"lr-tfid\"\n",
    "model.fit(x_train, y_train)\n",
    "save_model(model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([('vect', w2vVectorizer()),\n",
    "        ('clf', LogisticRegression(n_jobs=1, C=1e5,max_iter=6300)),\n",
    "        ])\n",
    "model_name = \"lr-w2v\"\n",
    "model.fit(x_train, y_train)\n",
    "save_model(model, model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict data\n",
    "print(\"Test Data Accuracy  :\\t\", model.score(x_test, y_test))\n",
    "y_test_pred = model.predict(x_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorflow gpu \n",
    "- https://www.tensorflow.org/install/pip#windows-native\n",
    "- https://lifewithdata.com/2022/01/16/how-to-install-tensorflow-and-keras-with-gpu-support-on-windows/ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.youtube.com/watch?v=oWo9SNcyxlI\n",
    "- good read for = https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-022-01665-y\n",
    "\n",
    "- cnn\n",
    "- transformer encoder\n",
    "- pretrained bert\n",
    "- typical neural network = rnn, gru, lstm, bi-lstm\n",
    "\n",
    "- other than bert, the rest need word embedding so do glove, word2vec and fasttext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.utils.data_utils import pad_sequences\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, GlobalMaxPooling1D, Embedding\n",
    "from keras.layers import Conv1D, LSTM, SpatialDropout1D, Bidirectional, GRU, SimpleRNN, TextVectorization\n",
    "\n",
    "from keras.metrics import BinaryAccuracy,Precision,Recall\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "\n",
    "from gensim.models import FastText, Word2Vec\n",
    "\n",
    "METRICS = [\n",
    "    BinaryAccuracy(name=\"accuracy\"),\n",
    "    Precision(name=\"precision\"),\n",
    "    Recall(name=\"recall\")\n",
    "]\n",
    "\n",
    "def save_model_nn(model, model_name, embedding_name, dataset_name):\n",
    "    filename = f\"models/{dataset_name}_{embedding_name}_{model_name}\"\n",
    "    model.save(filename)\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.12.0\n",
      "Keras Version: 2.12.0\n",
      "GPU is NOT AVAILABLE\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tensorflow version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {tf.keras.__version__}\")\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")\n",
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFID Vectorization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embeding layer convert text to numeric form which is used as the first layer for the deep learning model\n",
    "- https://speakai.co/the-best-pretrained-word-embeddings/#:~:text=The%20most%20popular%20pretrained%20word,GloVe%2C%20Word2Vec%2C%20and%20FastText."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_em(x_train):\n",
    "    embedding_name = \"glove\"\n",
    "    text_length = 50 #pad/truncate text to this long, such that each text after token will be this long\n",
    "\n",
    "    custom_encoder = TextVectorization(\n",
    "        standardize = None,\n",
    "        output_sequence_length=text_length, \n",
    "    )\n",
    "    custom_encoder.adapt(x_train)\n",
    "    vocab = custom_encoder.get_vocabulary()\n",
    "    print(f\"total vocab {len(vocab)}\")\n",
    "    vocab_dict = dict(zip(vocab, range(len(vocab))))\n",
    "\n",
    "    # load glove to dictionay\n",
    "    embeddings_dic = dict()\n",
    "    glove_file = open(\"Dataset/glove_embedding.txt\", encoding=\"utf8\")\n",
    "\n",
    "    for line in glove_file:\n",
    "        records = line.split()\n",
    "        word = records[0]\n",
    "        vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "        embeddings_dic[word] = vector_dimensions\n",
    "    glove_file.close()\n",
    "    print(\"Total words \", len(embeddings_dic))\n",
    "\n",
    "    # create vocab length is the size of token in dictionary\n",
    "    # Size of the vocabulary\n",
    "    vocab_length = len(vocab) + 1\n",
    "    embedding_dim = 100 #each glove word is 100 long\n",
    "\n",
    "    hits = 0\n",
    "    miss = 0\n",
    "\n",
    "    # create embedding matrix having 100 col\n",
    "    # for all vocab word we give it a vector value from glove\n",
    "    # for those not found in glove will be empty 0\n",
    "    # size of embedding_matriz = size of word_tokenizer.word_index.items()\n",
    "    # embedding_matrix is the weight \n",
    "    embedding_matrix = np.zeros((vocab_length, embedding_dim))\n",
    "    for word, index in vocab_dict.items():\n",
    "        embedding_vector = embeddings_dic.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            miss +=1\n",
    "    print(\"Converted %d words (%d misses)\" % (hits, miss))\n",
    "\n",
    "    custom_embedding = Embedding(vocab_length, embedding_dim, \n",
    "                embeddings_initializer=keras.initializers.Constant(embedding_matrix), \n",
    "                trainable = False,\n",
    "                input_length=text_length,\n",
    "                mask_zero=True)\n",
    "    \n",
    "    return custom_encoder, custom_embedding, embedding_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://medium.com/@93Kryptonian/word-embedding-using-fasttext-62beb0209db9\n",
    "- It treats each word as composed of n-grams. In word2vec each word is represented as a bag of words but in FastText each word is represented as a bag of character n-gram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasttext_em(x_train):\n",
    "    embedding_name = \"fasttext\"\n",
    "    text_length = 50 #pad/truncate text to this long, such that each text after token will be this long\n",
    "    vector_size=100\n",
    "\n",
    "    sentences = [sentence.split() for sentence in x_train]\n",
    "    ft_model = FastText(sentences, vector_size=vector_size, window=5, min_count=2, workers=4, seed=42, sg=1, epochs=10) # skip gram or cbow=0\n",
    "    ft = ft_model.wv\n",
    "    ft_vocab = ft.index_to_key\n",
    "\n",
    "    custom_encoder = TextVectorization(\n",
    "        standardize = None,\n",
    "        output_sequence_length=text_length, \n",
    "        vocabulary = ft_vocab\n",
    "    )\n",
    "\n",
    "    vocab = custom_encoder.get_vocabulary()\n",
    "    print(f\"total vocab {len(vocab)}\")\n",
    "    vocab_dict = dict(zip(vocab, range(len(vocab))))\n",
    "\n",
    "    vocab_length = len(vocab) + 1\n",
    "    embedding_dim = vector_size\n",
    "\n",
    "    hits = 0\n",
    "    miss = 0\n",
    "\n",
    "    embedding_matrix = np.zeros((vocab_length, embedding_dim))\n",
    "    for word, index in vocab_dict.items():\n",
    "        embedding_vector = ft[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            miss +=1\n",
    "    print(\"Converted %d words (%d misses)\" % (hits, miss))\n",
    "\n",
    "    custom_embedding = Embedding(vocab_length, embedding_dim, \n",
    "                embeddings_initializer=keras.initializers.Constant(embedding_matrix), \n",
    "                trainable = False,\n",
    "                input_length=text_length,\n",
    "                mask_zero=True)\n",
    "    return custom_encoder, custom_embedding, embedding_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- using pretrained??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_em(x_train):\n",
    "    embedding_name = \"word2vec\"\n",
    "    text_length = 50 #pad/truncate text to this long, such that each text after token will be this long\n",
    "    vector_size=100\n",
    "\n",
    "    sentences = [sentence.split() for sentence in x_train]\n",
    "    w_model = Word2Vec(sentences, vector_size=100, window=5, min_count=2, workers=4, sg=1, epochs=10)\n",
    "    w = w_model.wv\n",
    "    w_vocab = w.index_to_key\n",
    "\n",
    "    custom_encoder = TextVectorization(\n",
    "        standardize = None,\n",
    "        output_sequence_length=text_length, \n",
    "        vocabulary = w_vocab\n",
    "    )\n",
    "\n",
    "    vocab = custom_encoder.get_vocabulary()\n",
    "    print(f\"total vocab {len(vocab)}\")\n",
    "    vocab_dict = dict(zip(vocab, range(len(vocab))))\n",
    "\n",
    "    vocab_length = len(vocab) + 1\n",
    "    embedding_dim = vector_size\n",
    "\n",
    "    hits = 0\n",
    "    miss = 0\n",
    "\n",
    "    embedding_matrix = np.zeros((vocab_length, embedding_dim))\n",
    "    for word, index in vocab_dict.items():\n",
    "        if word in w.key_to_index:\n",
    "            embedding_vector = w[word]\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[index] = embedding_vector\n",
    "                hits += 1\n",
    "            else:\n",
    "                print(word)\n",
    "                miss +=1\n",
    "        else:\n",
    "            print(word)\n",
    "            miss +=1\n",
    "    print(\"Converted %d words (%d misses)\" % (hits, miss))\n",
    "\n",
    "    custom_embedding = Embedding(vocab_length, embedding_dim, \n",
    "                embeddings_initializer=keras.initializers.Constant(embedding_matrix), \n",
    "                trainable = False,\n",
    "                input_length=text_length,\n",
    "                mask_zero=True)\n",
    "    return custom_encoder, custom_embedding, embedding_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test without pre-trained\n",
    "- https://www.tensorflow.org/text/guide/word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noTrained_em(x_train):\n",
    "    embedding_name = \"no_train\"\n",
    "    text_length = 50 #pad/truncate text to this long, such that each text after token will be this long\n",
    "    vector_size=100\n",
    "\n",
    "    custom_encoder = TextVectorization(\n",
    "        standardize = None,\n",
    "        output_sequence_length=text_length, \n",
    "    )\n",
    "    custom_encoder.adapt(x_train)\n",
    "    vocab = custom_encoder.get_vocabulary()\n",
    "    print(f\"total vocab {len(vocab)}\")\n",
    "    vocab_dict = dict(zip(vocab, range(len(vocab))))\n",
    "\n",
    "    vocab_length = len(vocab) + 1\n",
    "    embedding_dim = vector_size\n",
    "\n",
    "    custom_embedding = Embedding(vocab_length, embedding_dim,\n",
    "                input_length=text_length,\n",
    "                mask_zero=True)\n",
    "    return custom_encoder, custom_embedding, embedding_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snn(custom_encoder, custom_embedding, embedding_name):\n",
    "    model = Sequential()\n",
    "    model.add(custom_encoder)\n",
    "    model.add(custom_embedding)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=METRICS)\n",
    "\n",
    "    history = model.fit(x_train, y_train_binary, epochs=10,\n",
    "                        validation_data=(x_test,y_test_binary))\n",
    "\n",
    "    model_name = \"snn-\"+embedding_name\n",
    "    save_model_nn(model, model_name)\n",
    "    print(history.history['val_accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- usually for 2d image recognition \n",
    "- use 1d cnn for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(custom_encoder, custom_embedding, embedding_name):\n",
    "    print(\"cnn\")\n",
    "    model = Sequential()\n",
    "    model.add(custom_encoder)\n",
    "    model.add(custom_embedding)\n",
    "    model.add(Conv1D(128, 5, activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=METRICS)\n",
    "\n",
    "    history = model.fit(x_train, y_train_binary, epochs=10,\n",
    "                        validation_data=(x_test,y_test_binary))\n",
    "\n",
    "    save_model_nn(model, \"cnn\", embedding_name, dataset_name)\n",
    "    print(f\"acc {history.history['val_accuracy'][0]}\")\n",
    "    return history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN - LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LSTM is a variant of RNN\n",
    "- input -> textvectorization (encoder/token) -> emedding -> nn layer\n",
    "- Now that all samples have a uniform length, the model must be informed that some part of the data is actually padding and should be ignored. That mechanism is masking.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sigmoid - 6s 32ms/step - loss: 0.1901 - accuracy: 0.9281 - precision: 0.9117 - recall: 0.9186 - val_loss: 0.7441 - val_accuracy: 0.7256 - val_precision: 0.6628 - val_recall: 0.7021\n",
    "- relu - 7s 34ms/step - loss: 0.5181 - accuracy: 0.7423 - precision: 0.7250 - recall: 0.6280 - val_loss: 0.5530 - val_accuracy: 0.7338 - val_precision: 0.7074 - val_recall: 0.6215\n",
    "- sigmoid is better for binary text classification\n",
    "\n",
    "- dropout - 19s 98ms/step - loss: 0.3879 - accuracy: 0.8246 - precision: 0.7934 - recall: 0.7904 - val_loss: 0.5436 - val_accuracy: 0.7323 - val_precision: 0.6497 - val_recall: 0.7828"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GOOD = https://medium.com/mlearning-ai/the-classification-of-text-messages-using-lstm-bi-lstm-and-gru-f79b207f90ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is better with the dropout\n",
    "def lstm(custom_encoder, custom_embedding, embedding_name):\n",
    "    print(\"lstm\")\n",
    "    model = Sequential()\n",
    "    model.add(custom_encoder)\n",
    "    model.add(custom_embedding)\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=METRICS)\n",
    "\n",
    "    history = model.fit(x_train, y_train_binary, epochs=10,\n",
    "                        validation_data=(x_test,y_test_binary))\n",
    "\n",
    "    save_model_nn(model, \"lstm\", embedding_name, dataset_name)\n",
    "    print(f\"acc {history.history['val_accuracy'][0]}\")\n",
    "    return history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN - BILSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bilstm(custom_encoder, custom_embedding, embedding_name):\n",
    "    print(\"bilstm\")\n",
    "    model = Sequential()\n",
    "    model.add(custom_encoder)\n",
    "    model.add(custom_embedding)\n",
    "    model.add(Bidirectional(LSTM(128)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=METRICS)\n",
    "\n",
    "    history = model.fit(x_train, y_train_binary, epochs=10,\n",
    "                        validation_data=(x_test,y_test_binary))\n",
    "\n",
    "    save_model_nn(model, \"bilstm\", embedding_name, dataset_name)\n",
    "    print(f\"acc {history.history['val_accuracy'][0]}\")\n",
    "    return history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN - GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(custom_encoder, custom_embedding, embedding_name):\n",
    "    print(\"gru\")\n",
    "    model = Sequential()\n",
    "    model.add(custom_encoder)\n",
    "    model.add(custom_embedding)\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(GRU(128))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=METRICS)\n",
    "\n",
    "    history = model.fit(x_train, y_train_binary, epochs=10,\n",
    "                        validation_data=(x_test,y_test_binary))\n",
    "\n",
    "    save_model_nn(model, \"gru\", embedding_name, dataset_name)\n",
    "    print(f\"acc {history.history['val_accuracy'][0]}\")\n",
    "    return history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.tensorflow.org/text/tutorials/text_classification_rnn\n",
    "- 'input_dim' = the vocab size that we will choose. In other words it is the number of unique words in the vocab.\n",
    "- 'output_dim' = the number of dimensions we wish to embed into. Each word will be represented by a vector of this much dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(custom_encoder, custom_embedding, embedding_name):\n",
    "    print(\"rnn\")\n",
    "    model = Sequential()\n",
    "    model.add(custom_encoder)\n",
    "    model.add(custom_embedding)\n",
    "    model.add(SimpleRNN(128))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=METRICS)\n",
    "\n",
    "    history = model.fit(x_train, y_train_binary, epochs=10,\n",
    "                        validation_data=(x_test,y_test_binary))\n",
    "\n",
    "    save_model_nn(model, \"rnn\", embedding_name, dataset_name)\n",
    "    print(f\"acc {history.history['val_accuracy'][0]}\")\n",
    "    return history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre trained transformer\n",
    "- gpt word embedding and bert\n",
    "- based on transformer architecture\n",
    "- uses deep learning for word embedding \n",
    "- Yes, transformer-based word embeddings are a form of deep learning. The transformer model architecture, which is the foundation of transformer-based word embeddings, is a deep learning architecture widely used in natural language processing (NLP) tasks.\n",
    "- transformer-based word embeddings are a type of deep learning technique that utilizes the power of deep neural networks to learn contextually rich representations of words or tokens in natural language text."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GPT-2 belongs to a family of deep learning models called “Transformers”. Transformers are the building block of the current state-of-the-art NLP architecture\n",
    "- A typical transformers design contains two parts, encoder and decoders, both working as vectorized representation of word relationships.\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/Fine-tuned_classification.ipynb\n",
    "- can do through fine tunning or word embedding \n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuining "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://platform.openai.com/docs/api-reference/fine-tunes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset for gpt\n",
    "df_gpt = pd.DataFrame(zip(x_train,y_train_binary), columns = ['prompt', 'completion'])\n",
    "df_gpt.to_json(\"Dataset/SE2019/gpt_data_train.jsonl\", orient='records', lines=True)\n",
    "\n",
    "df_gpt = pd.DataFrame(zip(x_test,y_test_binary), columns = ['prompt', 'completion'])\n",
    "df_gpt.to_json(\"Dataset/SE2019/gpt_data_test.jsonl\", orient='records', lines=True)\n",
    "\n",
    "df_gpt[\"completion\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# prepare dataset for fine tune do in cmd\n",
    "# !openai tools fine_tunes.prepare_data -f Dataset/SE2019/gpt_data_test.jsonl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<File file id=file-XCtTOLLxI6jM59TlrDoT4LGL at 0x1d8e3f835f0> JSON: {\n",
       "  \"object\": \"file\",\n",
       "  \"id\": \"file-XCtTOLLxI6jM59TlrDoT4LGL\",\n",
       "  \"purpose\": \"fine-tune\",\n",
       "  \"filename\": \"file\",\n",
       "  \"bytes\": 656131,\n",
       "  \"created_at\": 1687786717,\n",
       "  \"status\": \"uploaded\",\n",
       "  \"status_details\": null\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# upload file to openai and create fine tune model\n",
    "# openai.File.create(\n",
    "#   file=open(\"Dataset/SE2019/gpt_data_train_prepared_train.jsonl\", \"rb\"),\n",
    "#   purpose='fine-tune'\n",
    "# )\n",
    "\n",
    "# openai.File.create(\n",
    "#   file=open(\"Dataset/SE2019/gpt_data_train_prepared_valid.jsonl\", \"rb\"),\n",
    "#   purpose='fine-tune'\n",
    "# )\n",
    "\n",
    "# openai.FineTune.create(training_file =\"file-XCtTOLLxI6jM59TlrDoT4LGL\",\n",
    "#                        validation_file=\"file-Do46v5kndAoPqdaR2ZVwbPT5\",\n",
    "#                        model = \"ada\",\n",
    "#                        compute_classification_metrics = True,\n",
    "#                        classification_positive_class = \" Hate\"\n",
    "#                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject list at 0x27457b19310> JSON: {\n",
       "  \"object\": \"list\",\n",
       "  \"data\": [\n",
       "    {\n",
       "      \"object\": \"fine-tune\",\n",
       "      \"id\": \"ft-VwKO2vWcNtBJrPoZorUtFTVV\",\n",
       "      \"hyperparams\": {\n",
       "        \"n_epochs\": 4,\n",
       "        \"batch_size\": 8,\n",
       "        \"prompt_loss_weight\": 0.01,\n",
       "        \"learning_rate_multiplier\": 0.1,\n",
       "        \"classification_positive_class\": \" Hate\",\n",
       "        \"compute_classification_metrics\": true\n",
       "      },\n",
       "      \"organization_id\": \"org-57m2RCBaIU5pd9nsTDikjeLg\",\n",
       "      \"model\": \"ada\",\n",
       "      \"training_files\": [\n",
       "        {\n",
       "          \"object\": \"file\",\n",
       "          \"id\": \"file-XCtTOLLxI6jM59TlrDoT4LGL\",\n",
       "          \"purpose\": \"fine-tune\",\n",
       "          \"filename\": \"file\",\n",
       "          \"bytes\": 656131,\n",
       "          \"created_at\": 1687786717,\n",
       "          \"status\": \"processed\",\n",
       "          \"status_details\": null\n",
       "        }\n",
       "      ],\n",
       "      \"validation_files\": [\n",
       "        {\n",
       "          \"object\": \"file\",\n",
       "          \"id\": \"file-Do46v5kndAoPqdaR2ZVwbPT5\",\n",
       "          \"purpose\": \"fine-tune\",\n",
       "          \"filename\": \"file\",\n",
       "          \"bytes\": 122361,\n",
       "          \"created_at\": 1687786754,\n",
       "          \"status\": \"processed\",\n",
       "          \"status_details\": null\n",
       "        }\n",
       "      ],\n",
       "      \"result_files\": [\n",
       "        {\n",
       "          \"object\": \"file\",\n",
       "          \"id\": \"file-xfJGasLV4EMODTNBLNd6zxWU\",\n",
       "          \"purpose\": \"fine-tune-results\",\n",
       "          \"filename\": \"compiled_results.csv\",\n",
       "          \"bytes\": 174768,\n",
       "          \"created_at\": 1687790623,\n",
       "          \"status\": \"processed\",\n",
       "          \"status_details\": null\n",
       "        }\n",
       "      ],\n",
       "      \"created_at\": 1687786769,\n",
       "      \"updated_at\": 1687790623,\n",
       "      \"status\": \"succeeded\",\n",
       "      \"fine_tuned_model\": \"ada:ft-personal-2023-06-26-14-43-42\"\n",
       "    },\n",
       "    {\n",
       "      \"object\": \"fine-tune\",\n",
       "      \"id\": \"ft-LAcA6GtvgJ0lksqRT8jwtzSx\",\n",
       "      \"hyperparams\": {\n",
       "        \"n_epochs\": 4,\n",
       "        \"batch_size\": 8,\n",
       "        \"prompt_loss_weight\": 0.01,\n",
       "        \"learning_rate_multiplier\": 0.1,\n",
       "        \"classification_positive_class\": \" 0\",\n",
       "        \"compute_classification_metrics\": true\n",
       "      },\n",
       "      \"organization_id\": \"org-57m2RCBaIU5pd9nsTDikjeLg\",\n",
       "      \"model\": \"ada\",\n",
       "      \"training_files\": [\n",
       "        {\n",
       "          \"object\": \"file\",\n",
       "          \"id\": \"file-m1vI4bNTIIUnUZJwUM4eo2S3\",\n",
       "          \"purpose\": \"fine-tune\",\n",
       "          \"filename\": \"Dataset/SE2019/gpt_data_train_prepared.jsonl\",\n",
       "          \"bytes\": 745178,\n",
       "          \"created_at\": 1687797102,\n",
       "          \"status\": \"processed\",\n",
       "          \"status_details\": null\n",
       "        }\n",
       "      ],\n",
       "      \"validation_files\": [\n",
       "        {\n",
       "          \"object\": \"file\",\n",
       "          \"id\": \"file-GR7BA1XaebEeGZxz9QLB5sg8\",\n",
       "          \"purpose\": \"fine-tune\",\n",
       "          \"filename\": \"Dataset/SE2019/gpt_data_test_prepared.jsonl\",\n",
       "          \"bytes\": 321131,\n",
       "          \"created_at\": 1687797106,\n",
       "          \"status\": \"processed\",\n",
       "          \"status_details\": null\n",
       "        }\n",
       "      ],\n",
       "      \"result_files\": [\n",
       "        {\n",
       "          \"object\": \"file\",\n",
       "          \"id\": \"file-jLyo55YpRZ6D0diBaQedaJ81\",\n",
       "          \"purpose\": \"fine-tune-results\",\n",
       "          \"filename\": \"compiled_results.csv\",\n",
       "          \"bytes\": 188239,\n",
       "          \"created_at\": 1687800449,\n",
       "          \"status\": \"processed\",\n",
       "          \"status_details\": null\n",
       "        }\n",
       "      ],\n",
       "      \"created_at\": 1687797106,\n",
       "      \"updated_at\": 1687800449,\n",
       "      \"status\": \"succeeded\",\n",
       "      \"fine_tuned_model\": \"ada:ft-personal-2023-06-26-17-27-28\"\n",
       "    },\n",
       "    {\n",
       "      \"object\": \"fine-tune\",\n",
       "      \"id\": \"ft-XGDLqYtbnvM9gy4Mrm8AWGay\",\n",
       "      \"hyperparams\": {\n",
       "        \"n_epochs\": 4,\n",
       "        \"batch_size\": null,\n",
       "        \"prompt_loss_weight\": 0.01,\n",
       "        \"learning_rate_multiplier\": null,\n",
       "        \"classification_positive_class\": \" Hate\",\n",
       "        \"compute_classification_metrics\": true\n",
       "      },\n",
       "      \"organization_id\": \"org-57m2RCBaIU5pd9nsTDikjeLg\",\n",
       "      \"model\": \"ada\",\n",
       "      \"training_files\": [\n",
       "        {\n",
       "          \"object\": \"file\",\n",
       "          \"id\": \"file-XCtTOLLxI6jM59TlrDoT4LGL\",\n",
       "          \"purpose\": \"fine-tune\",\n",
       "          \"filename\": \"file\",\n",
       "          \"bytes\": 656131,\n",
       "          \"created_at\": 1687786717,\n",
       "          \"status\": \"processed\",\n",
       "          \"status_details\": null\n",
       "        }\n",
       "      ],\n",
       "      \"validation_files\": [\n",
       "        {\n",
       "          \"object\": \"file\",\n",
       "          \"id\": \"file-Do46v5kndAoPqdaR2ZVwbPT5\",\n",
       "          \"purpose\": \"fine-tune\",\n",
       "          \"filename\": \"file\",\n",
       "          \"bytes\": 122361,\n",
       "          \"created_at\": 1687786754,\n",
       "          \"status\": \"processed\",\n",
       "          \"status_details\": null\n",
       "        }\n",
       "      ],\n",
       "      \"result_files\": [],\n",
       "      \"created_at\": 1687844635,\n",
       "      \"updated_at\": 1687844807,\n",
       "      \"status\": \"cancelled\",\n",
       "      \"fine_tuned_model\": null\n",
       "    }\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.FineTune.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# create fine tune model\n",
    "# !openai api fine_tunes.create -m ada -t \"Dataset/SE2019/gpt_data_train_prepared.jsonl\" -v \"Dataset/SE2019/gpt_data_test_prepared.jsonl\" --compute_classification_metrics --classification_positive_class \" 0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FineTune fine-tune id=ft-XGDLqYtbnvM9gy4Mrm8AWGay at 0x27457b18ef0> JSON: {\n",
       "  \"object\": \"fine-tune\",\n",
       "  \"id\": \"ft-XGDLqYtbnvM9gy4Mrm8AWGay\",\n",
       "  \"hyperparams\": {\n",
       "    \"n_epochs\": 4,\n",
       "    \"batch_size\": null,\n",
       "    \"prompt_loss_weight\": 0.01,\n",
       "    \"learning_rate_multiplier\": null,\n",
       "    \"classification_positive_class\": \" Hate\",\n",
       "    \"compute_classification_metrics\": true\n",
       "  },\n",
       "  \"organization_id\": \"org-57m2RCBaIU5pd9nsTDikjeLg\",\n",
       "  \"model\": \"ada\",\n",
       "  \"training_files\": [\n",
       "    {\n",
       "      \"object\": \"file\",\n",
       "      \"id\": \"file-XCtTOLLxI6jM59TlrDoT4LGL\",\n",
       "      \"purpose\": \"fine-tune\",\n",
       "      \"filename\": \"file\",\n",
       "      \"bytes\": 656131,\n",
       "      \"created_at\": 1687786717,\n",
       "      \"status\": \"processed\",\n",
       "      \"status_details\": null\n",
       "    }\n",
       "  ],\n",
       "  \"validation_files\": [\n",
       "    {\n",
       "      \"object\": \"file\",\n",
       "      \"id\": \"file-Do46v5kndAoPqdaR2ZVwbPT5\",\n",
       "      \"purpose\": \"fine-tune\",\n",
       "      \"filename\": \"file\",\n",
       "      \"bytes\": 122361,\n",
       "      \"created_at\": 1687786754,\n",
       "      \"status\": \"processed\",\n",
       "      \"status_details\": null\n",
       "    }\n",
       "  ],\n",
       "  \"result_files\": [],\n",
       "  \"created_at\": 1687844635,\n",
       "  \"updated_at\": 1687844807,\n",
       "  \"status\": \"cancelled\",\n",
       "  \"fine_tuned_model\": null,\n",
       "  \"events\": [\n",
       "    {\n",
       "      \"object\": \"fine-tune-event\",\n",
       "      \"level\": \"info\",\n",
       "      \"message\": \"Created fine-tune: ft-XGDLqYtbnvM9gy4Mrm8AWGay\",\n",
       "      \"created_at\": 1687844635\n",
       "    },\n",
       "    {\n",
       "      \"object\": \"fine-tune-event\",\n",
       "      \"level\": \"info\",\n",
       "      \"message\": \"Fine-tune cancelled\",\n",
       "      \"created_at\": 1687844807\n",
       "    }\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.FineTune.retrieve(id=\"ft-XGDLqYtbnvM9gy4Mrm8AWGay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>elapsed_tokens</th>\n",
       "      <th>elapsed_examples</th>\n",
       "      <th>training_loss</th>\n",
       "      <th>training_sequence_accuracy</th>\n",
       "      <th>training_token_accuracy</th>\n",
       "      <th>validation_loss</th>\n",
       "      <th>validation_sequence_accuracy</th>\n",
       "      <th>validation_token_accuracy</th>\n",
       "      <th>classification/accuracy</th>\n",
       "      <th>classification/precision</th>\n",
       "      <th>classification/recall</th>\n",
       "      <th>classification/auroc</th>\n",
       "      <th>classification/auprc</th>\n",
       "      <th>classification/f1.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3137</th>\n",
       "      <td>3138</td>\n",
       "      <td>1017808</td>\n",
       "      <td>25104</td>\n",
       "      <td>0.030316</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.814333</td>\n",
       "      <td>0.837239</td>\n",
       "      <td>0.844728</td>\n",
       "      <td>0.890362</td>\n",
       "      <td>0.914601</td>\n",
       "      <td>0.840967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      step  elapsed_tokens  elapsed_examples  training_loss   \n",
       "3137  3138         1017808             25104       0.030316  \\\n",
       "\n",
       "      training_sequence_accuracy  training_token_accuracy  validation_loss   \n",
       "3137                         1.0                      1.0              NaN  \\\n",
       "\n",
       "      validation_sequence_accuracy  validation_token_accuracy   \n",
       "3137                           NaN                        NaN  \\\n",
       "\n",
       "      classification/accuracy  classification/precision   \n",
       "3137                 0.814333                  0.837239  \\\n",
       "\n",
       "      classification/recall  classification/auroc  classification/auprc   \n",
       "3137               0.844728              0.890362              0.914601  \\\n",
       "\n",
       "      classification/f1.0  \n",
       "3137             0.840967  "
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get result of model\n",
    "#!openai api fine_tunes.results -i ft-LAcA6GtvgJ0lksqRT8jwtzSx > result.csv\n",
    "\n",
    "results = pd.read_csv('result.csv')\n",
    "results[results['classification/accuracy'].notnull()].tail(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_complete_create(prompt_text):\n",
    "    model_id = \"ada:ft-personal-2023-06-26-17-27-28\" \n",
    "    result_gpt = openai.Completion.create(model=model_id, prompt=prompt_text, max_tokens=1, temperature=0)\n",
    "    return result_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hey people already long islandanother chopped...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mandy manning one world reliefs friends partne...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tell us u rape people pussy  -&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>three streets berlin shut fighting migrants tu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>maybe visits school lebron built build one imm...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  completion\n",
       "0   hey people already long islandanother chopped...           1\n",
       "1  mandy manning one world reliefs friends partne...           0\n",
       "2                    tell us u rape people pussy  ->           0\n",
       "3  three streets berlin shut fighting migrants tu...           1\n",
       "4  maybe visits school lebron built build one imm...           0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = \"Dataset/SE2019/gpt_data_test_prepared.jsonl\"\n",
    "df_gpt = pd.read_json(filepath, lines= True)\n",
    "df_gpt['predicted'] = -1\n",
    "df_gpt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_predict(start = 0, step = 10):\n",
    "    max_s = len(df_gpt)\n",
    "    prompts_tosend = []\n",
    "    for i in range(start, start+step):\n",
    "        if i == max_s: break\n",
    "        p = df_gpt.loc[i]['prompt']\n",
    "        prompts_tosend.append(p)\n",
    "    \n",
    "    predict_result = gpt_complete_create(prompts_tosend)\n",
    "    choices_gpt = predict_result.to_dict()['choices']\n",
    "\n",
    "    for i in range(len(choices_gpt)):\n",
    "        j = choices_gpt[i].to_dict()\n",
    "        df_gpt.loc[start+j[\"index\"], 'predicted'] = int(j['text'])\n",
    "\n",
    "    print(f\"predicted {start} to {start+len(prompts_tosend)-1}\")\n",
    "\n",
    "    # for i in range(start, start+step):\n",
    "    #     r = df_gpt.loc[i]\n",
    "    #     print(r['prompt'], \"\\n\", r['completion'], r['predicted'])\n",
    "\n",
    "    return prompts_tosend, predict_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2693"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next start 2060\n",
      "next start 2110\n",
      "next start 2160\n",
      "next start 2210\n",
      "next start 2260\n",
      "next start 2310\n",
      "next start 2360\n",
      "next start 2410\n",
      "next start 2460\n",
      "next start 2510\n",
      "next start 2560\n",
      "next start 2610\n",
      "next start 2660\n",
      "next start 2693\n"
     ]
    }
   ],
   "source": [
    "def loop_gpt(start, end, step):\n",
    "    for i in range(start, end, step):\n",
    "        bb, cc = gpt_predict(i, step)\n",
    "loop_gpt(0, len(df_gpt), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [prompt, completion, predicted]\n",
       "Index: []"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gpt.loc[df_gpt['predicted'] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = df_gpt['completion'].to_numpy()\n",
    "y_test_pred = df_gpt['predicted'].to_numpy()\n",
    "\n",
    "y_test = np.where(y_test == 1, \"Hate\", \"Non-Hate\") \n",
    "y_test_pred = np.where(y_test_pred == 1, \"Hate\", \"Non-Hate\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpt.to_csv(\"Dataset/SE2019/gpt_data_test_result\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Hate       0.78      0.78      0.78      1128\n",
      "    Non-Hate       0.84      0.84      0.84      1565\n",
      "\n",
      "    accuracy                           0.81      2693\n",
      "   macro avg       0.81      0.81      0.81      2693\n",
      "weighted avg       0.81      0.81      0.81      2693\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_test_pred, labels=[\"Hate\",\"Non-Hate\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270\n",
    "- https://www.youtube.com/watch?v=hOCDJyZ6quA\n",
    "- tensorflow hub bert https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\n",
    "- bert will convert sentence into embeding vector which will feed to neural network for training \n",
    "- consist of preprocess and embeding \n",
    "- (4)BERT-RNN: The corresponding representational word vectors were trained by BERT model for the input text, which were then classified by RNN neural network. (5)word2vec-RNN: This model is a traditional text classification model. 4.3.\n",
    "- BERT is a neural-network-based technique for language processing pre-training\n",
    "- it is not a classification algorithm \n",
    "- BERT generates <b>contextual embeddings</b>, the input to the model is a sentence rather than a single word.\n",
    "- BERT learns contextualized word representations, often referred to as contextual word embeddings or contextualized embeddings. Unlike traditional word embeddings, which assign a fixed vector representation to each word, BERT's word representations are sensitive to the context in which the word appears."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_1 (TextV  (None, 50)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 50, 100)           3200300   \n",
      "                                                                 \n",
      " spatial_dropout1d_2 (Spatia  (None, 50, 100)          0         \n",
      " lDropout1D)                                                     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 128)               117248    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,317,677\n",
      "Trainable params: 117,377\n",
      "Non-trainable params: 3,200,300\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_predict():\n",
    "    score = model.evaluate(x_test, y_test_binary, verbose=0)\n",
    "    print(\"Score: \", score[0])\n",
    "    print(\"Accuracy: \", score[1])\n",
    "\n",
    "    y_test_pred_percent = model.predict(x_test, verbose=0)\n",
    "    y_test_pred = np.where(y_test_pred_percent > 0.5, \"Hate\", \"Non-Hate\") \n",
    "    y_test_pred = y_test_pred.flatten()\n",
    "\n",
    "    return y_test_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(model_history):\n",
    "    # Model performance charts\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(1, 2, 1)\n",
    "\n",
    "    plt.plot(model_history.history['accuracy'])\n",
    "    plt.plot(model_history.history['val_accuracy'])\n",
    "\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.ylim(None, 1)\n",
    "    plt.subplot(1, 2, 2)\n",
    "\n",
    "    plt.plot(model_history.history['loss'])\n",
    "    plt.plot(model_history.history['val_loss'])\n",
    "\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.ylim(0, None)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(y_test, y_test_pred):\n",
    "    # Heatmap\n",
    "    ax = plt.subplot()\n",
    "\n",
    "    # Plot the two-way Confusion Matrix\n",
    "    sb.heatmap(confusion_matrix(y_test, y_test_pred, labels=[\"Hate\",\"Non-Hate\"]), \n",
    "            annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax=ax)\n",
    "\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.xaxis.set_ticklabels([\"Hate\",\"Non-Hate\"])\n",
    "    ax.yaxis.set_ticklabels([\"Hate\",\"Non-Hate\"])\n",
    "\n",
    "    # Count\n",
    "    df1 = pd.DataFrame({'Actual':y_test, 'Predict':y_test_pred})\n",
    "    # print(df1.describe())\n",
    "    print(f\"Count: {df1['Actual'].value_counts()}\")\n",
    "    print()\n",
    "    print(f\"Count: {df1['Predict'].value_counts()}\")\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in model_to_load:\n",
    "#     filename = f\"models/{i}.pickle\"\n",
    "#     old_model = pickle.load(open(filename,\"rb\"))\n",
    "\n",
    "def get_classification_report(i, cr):\n",
    "    return [i, cr['accuracy'], cr['macro avg']['precision'], \n",
    "            cr['macro avg']['recall'], cr['macro avg']['f1-score'],\n",
    "            cr['Hate']['f1-score'],cr['Non-Hate']['f1-score'], \n",
    "            cr['Hate']['support'],cr['Non-Hate']['support']]\n",
    "\n",
    "\n",
    "def get_result_nn(model_to_load):\n",
    "    c = ['Model', 'Accuracy', 'precision', 'recall', 'f1-score', 'hate f1', \"non-hate f1\", 'hate support', 'non-hate support']\n",
    "    result_table = pd.DataFrame(columns=c)\n",
    "    for i in model_to_load:\n",
    "        filename = f\"models/{i}\"\n",
    "        print(filename)\n",
    "        old_model = load_model(filename)\n",
    "\n",
    "        y_test_pred = old_model.predict(x_test, verbose=0)\n",
    "        y_test_pred = np.where(y_test_pred > 0.5, \"Hate\", \"Non-Hate\") \n",
    "        y_test_pred = y_test_pred.flatten()\n",
    "\n",
    "        cr = classification_report(y_test, y_test_pred, labels=[\"Hate\",\"Non-Hate\"], output_dict=True)\n",
    "        result_table.loc[len(result_table)] = get_classification_report(i, cr)\n",
    "    return result_table.style.highlight_max(color = 'red', axis = 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'anyone help older samsung tablet get white screen trying log gab download says parsing error bookmarked still wont open even online browser help'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18979 8134\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train), len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total vocab 37041\n",
      "no_train\n",
      "GabHateCorpus\n"
     ]
    }
   ],
   "source": [
    "custom_encoder, custom_embedding, embedding_name = noTrained_em(x_train)\n",
    "print(embedding_name)\n",
    "print(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.30 fold\n",
    "- se2019 = Converted 7613 words (2 misses)\n",
    "- gab = Converted 13974 words (2 misses)\n",
    "\n",
    "10 fold\n",
    "- gab, word2vec = Converted 16230 words (2 misses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn\n",
      "Epoch 1/10\n",
      "763/763 [==============================] - 35s 44ms/step - loss: 0.3068 - accuracy: 0.8880 - precision: 0.6301 - recall: 0.2231 - val_loss: 0.2695 - val_accuracy: 0.8931 - val_precision: 0.6594 - val_recall: 0.2725\n",
      "Epoch 2/10\n",
      "763/763 [==============================] - 31s 41ms/step - loss: 0.1754 - accuracy: 0.9336 - precision: 0.8073 - recall: 0.6066 - val_loss: 0.3079 - val_accuracy: 0.8879 - val_precision: 0.5798 - val_recall: 0.3263\n",
      "Epoch 3/10\n",
      "763/763 [==============================] - 33s 43ms/step - loss: 0.0583 - accuracy: 0.9812 - precision: 0.9525 - recall: 0.8920 - val_loss: 0.4039 - val_accuracy: 0.8831 - val_precision: 0.5392 - val_recall: 0.3503\n",
      "Epoch 4/10\n",
      "763/763 [==============================] - 31s 41ms/step - loss: 0.0180 - accuracy: 0.9947 - precision: 0.9885 - recall: 0.9684 - val_loss: 0.4873 - val_accuracy: 0.8779 - val_precision: 0.5059 - val_recall: 0.3862\n",
      "Epoch 5/10\n",
      "763/763 [==============================] - 31s 41ms/step - loss: 0.0082 - accuracy: 0.9976 - precision: 0.9943 - recall: 0.9864 - val_loss: 0.5623 - val_accuracy: 0.8802 - val_precision: 0.5204 - val_recall: 0.3443\n",
      "Epoch 6/10\n",
      "763/763 [==============================] - 31s 40ms/step - loss: 0.0050 - accuracy: 0.9986 - precision: 0.9963 - recall: 0.9924 - val_loss: 0.6261 - val_accuracy: 0.8772 - val_precision: 0.5024 - val_recall: 0.3084\n",
      "Epoch 7/10\n",
      "763/763 [==============================] - 35s 46ms/step - loss: 0.0041 - accuracy: 0.9990 - precision: 0.9970 - recall: 0.9950 - val_loss: 0.6674 - val_accuracy: 0.8791 - val_precision: 0.5146 - val_recall: 0.3174\n",
      "Epoch 8/10\n",
      "763/763 [==============================] - 33s 44ms/step - loss: 0.0033 - accuracy: 0.9992 - precision: 0.9973 - recall: 0.9960 - val_loss: 0.6752 - val_accuracy: 0.8721 - val_precision: 0.4780 - val_recall: 0.4222\n",
      "Epoch 9/10\n",
      "763/763 [==============================] - 30s 39ms/step - loss: 0.0033 - accuracy: 0.9991 - precision: 0.9967 - recall: 0.9960 - val_loss: 0.7314 - val_accuracy: 0.8768 - val_precision: 0.5000 - val_recall: 0.3234\n",
      "Epoch 10/10\n",
      "763/763 [==============================] - 35s 46ms/step - loss: 0.0032 - accuracy: 0.9993 - precision: 0.9973 - recall: 0.9967 - val_loss: 0.7651 - val_accuracy: 0.8798 - val_precision: 0.5185 - val_recall: 0.3353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/GabHateCorpus_no_train_cnn\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/GabHateCorpus_no_train_cnn\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 0.8930678367614746\n",
      "rnn\n",
      "Epoch 1/10\n",
      "763/763 [==============================] - 115s 148ms/step - loss: 0.1489 - accuracy: 0.9422 - precision: 0.8159 - recall: 0.6863 - val_loss: 0.4125 - val_accuracy: 0.8698 - val_precision: 0.4570 - val_recall: 0.3024\n",
      "Epoch 2/10\n",
      "763/763 [==============================] - 121s 158ms/step - loss: 0.0597 - accuracy: 0.9793 - precision: 0.9230 - recall: 0.9083 - val_loss: 0.4803 - val_accuracy: 0.8551 - val_precision: 0.4013 - val_recall: 0.3593\n",
      "Epoch 3/10\n",
      "763/763 [==============================] - 119s 156ms/step - loss: 0.0302 - accuracy: 0.9889 - precision: 0.9576 - recall: 0.9525 - val_loss: 0.6529 - val_accuracy: 0.8654 - val_precision: 0.4317 - val_recall: 0.2934\n",
      "Epoch 4/10\n",
      "763/763 [==============================] - 117s 154ms/step - loss: 0.0173 - accuracy: 0.9945 - precision: 0.9813 - recall: 0.9744 - val_loss: 0.7736 - val_accuracy: 0.8595 - val_precision: 0.4041 - val_recall: 0.2964\n",
      "Epoch 5/10\n",
      "763/763 [==============================] - 115s 150ms/step - loss: 0.0107 - accuracy: 0.9967 - precision: 0.9870 - recall: 0.9864 - val_loss: 0.6590 - val_accuracy: 0.8448 - val_precision: 0.2995 - val_recall: 0.1946\n",
      "Epoch 6/10\n",
      "763/763 [==============================] - 109s 144ms/step - loss: 0.0118 - accuracy: 0.9961 - precision: 0.9870 - recall: 0.9814 - val_loss: 0.8400 - val_accuracy: 0.8422 - val_precision: 0.3601 - val_recall: 0.3623\n",
      "Epoch 7/10\n",
      "763/763 [==============================] - 103s 135ms/step - loss: 0.0109 - accuracy: 0.9966 - precision: 0.9887 - recall: 0.9841 - val_loss: 0.7830 - val_accuracy: 0.8459 - val_precision: 0.3779 - val_recall: 0.3892\n",
      "Epoch 8/10\n",
      "763/763 [==============================] - 94s 124ms/step - loss: 0.0120 - accuracy: 0.9961 - precision: 0.9850 - recall: 0.9834 - val_loss: 0.8460 - val_accuracy: 0.8322 - val_precision: 0.3315 - val_recall: 0.3563\n",
      "Epoch 9/10\n",
      "763/763 [==============================] - 112s 147ms/step - loss: 0.0079 - accuracy: 0.9972 - precision: 0.9890 - recall: 0.9880 - val_loss: 1.0179 - val_accuracy: 0.8348 - val_precision: 0.3304 - val_recall: 0.3323\n",
      "Epoch 10/10\n",
      "763/763 [==============================] - 102s 133ms/step - loss: 0.0064 - accuracy: 0.9979 - precision: 0.9930 - recall: 0.9897 - val_loss: 0.9482 - val_accuracy: 0.8326 - val_precision: 0.3137 - val_recall: 0.3024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/GabHateCorpus_no_train_rnn\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/GabHateCorpus_no_train_rnn\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 0.869837760925293\n",
      "lstm\n",
      "Epoch 1/10\n",
      "763/763 [==============================] - 358s 464ms/step - loss: 0.0824 - accuracy: 0.9579 - precision: 0.8641 - recall: 0.7814 - val_loss: 0.5779 - val_accuracy: 0.8426 - val_precision: 0.3786 - val_recall: 0.4341\n",
      "Epoch 2/10\n",
      "763/763 [==============================] - 296s 388ms/step - loss: 0.0180 - accuracy: 0.9944 - precision: 0.9809 - recall: 0.9734 - val_loss: 0.7569 - val_accuracy: 0.8510 - val_precision: 0.3776 - val_recall: 0.3234\n",
      "Epoch 3/10\n",
      "763/763 [==============================] - 301s 394ms/step - loss: 0.0100 - accuracy: 0.9971 - precision: 0.9897 - recall: 0.9867 - val_loss: 0.7764 - val_accuracy: 0.8459 - val_precision: 0.3679 - val_recall: 0.3503\n",
      "Epoch 4/10\n",
      "763/763 [==============================] - 307s 402ms/step - loss: 0.0070 - accuracy: 0.9979 - precision: 0.9927 - recall: 0.9900 - val_loss: 0.9544 - val_accuracy: 0.8462 - val_precision: 0.3754 - val_recall: 0.3743\n",
      "Epoch 5/10\n",
      "763/763 [==============================] - 311s 408ms/step - loss: 0.0056 - accuracy: 0.9981 - precision: 0.9933 - recall: 0.9914 - val_loss: 0.8721 - val_accuracy: 0.8319 - val_precision: 0.3342 - val_recall: 0.3683\n",
      "Epoch 6/10\n",
      "763/763 [==============================] - 305s 400ms/step - loss: 0.0062 - accuracy: 0.9982 - precision: 0.9927 - recall: 0.9930 - val_loss: 0.9005 - val_accuracy: 0.8566 - val_precision: 0.3799 - val_recall: 0.2605\n",
      "Epoch 7/10\n",
      "763/763 [==============================] - 316s 415ms/step - loss: 0.0033 - accuracy: 0.9990 - precision: 0.9970 - recall: 0.9947 - val_loss: 1.0540 - val_accuracy: 0.8647 - val_precision: 0.4225 - val_recall: 0.2695\n",
      "Epoch 8/10\n",
      "763/763 [==============================] - 245s 322ms/step - loss: 0.0053 - accuracy: 0.9981 - precision: 0.9930 - recall: 0.9917 - val_loss: 0.9419 - val_accuracy: 0.8485 - val_precision: 0.3695 - val_recall: 0.3263\n",
      "Epoch 9/10\n",
      "763/763 [==============================] - 159s 208ms/step - loss: 0.0037 - accuracy: 0.9985 - precision: 0.9953 - recall: 0.9924 - val_loss: 0.9320 - val_accuracy: 0.8555 - val_precision: 0.3811 - val_recall: 0.2784\n",
      "Epoch 10/10\n",
      "763/763 [==============================] - 159s 208ms/step - loss: 0.0035 - accuracy: 0.9989 - precision: 0.9967 - recall: 0.9947 - val_loss: 1.0141 - val_accuracy: 0.8426 - val_precision: 0.3514 - val_recall: 0.3293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, lstm_cell_24_layer_call_fn, lstm_cell_24_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/GabHateCorpus_no_train_lstm\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/GabHateCorpus_no_train_lstm\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 0.8425516486167908\n",
      "bilstm\n",
      "Epoch 1/10\n",
      "763/763 [==============================] - 306s 392ms/step - loss: 0.0464 - accuracy: 0.9696 - precision: 0.9083 - recall: 0.8382 - val_loss: 0.7997 - val_accuracy: 0.8448 - val_precision: 0.3724 - val_recall: 0.3802\n",
      "Epoch 2/10\n",
      "763/763 [==============================] - 310s 407ms/step - loss: 0.0054 - accuracy: 0.9984 - precision: 0.9957 - recall: 0.9910 - val_loss: 0.9759 - val_accuracy: 0.8414 - val_precision: 0.3605 - val_recall: 0.3713\n",
      "Epoch 3/10\n",
      "763/763 [==============================] - 314s 411ms/step - loss: 0.0057 - accuracy: 0.9983 - precision: 0.9937 - recall: 0.9924 - val_loss: 0.8995 - val_accuracy: 0.8392 - val_precision: 0.3500 - val_recall: 0.3563\n",
      "Epoch 4/10\n",
      "763/763 [==============================] - 309s 405ms/step - loss: 0.0032 - accuracy: 0.9988 - precision: 0.9963 - recall: 0.9940 - val_loss: 1.0786 - val_accuracy: 0.8367 - val_precision: 0.3507 - val_recall: 0.3832\n",
      "Epoch 5/10\n",
      "763/763 [==============================] - 316s 414ms/step - loss: 0.0029 - accuracy: 0.9991 - precision: 0.9973 - recall: 0.9957 - val_loss: 0.9066 - val_accuracy: 0.8426 - val_precision: 0.3690 - val_recall: 0.3922\n",
      "Epoch 6/10\n",
      "763/763 [==============================] - 325s 426ms/step - loss: 0.0029 - accuracy: 0.9989 - precision: 0.9963 - recall: 0.9944 - val_loss: 1.1307 - val_accuracy: 0.8389 - val_precision: 0.3648 - val_recall: 0.4162\n",
      "Epoch 7/10\n",
      "763/763 [==============================] - 326s 428ms/step - loss: 0.0024 - accuracy: 0.9989 - precision: 0.9967 - recall: 0.9940 - val_loss: 1.0283 - val_accuracy: 0.8285 - val_precision: 0.3235 - val_recall: 0.3593\n",
      "Epoch 8/10\n",
      "763/763 [==============================] - 314s 411ms/step - loss: 0.0020 - accuracy: 0.9993 - precision: 0.9983 - recall: 0.9957 - val_loss: 1.1044 - val_accuracy: 0.8407 - val_precision: 0.3497 - val_recall: 0.3413\n",
      "Epoch 9/10\n",
      "763/763 [==============================] - 307s 402ms/step - loss: 0.0018 - accuracy: 0.9993 - precision: 0.9980 - recall: 0.9960 - val_loss: 1.1312 - val_accuracy: 0.8352 - val_precision: 0.3408 - val_recall: 0.3623\n",
      "Epoch 10/10\n",
      "763/763 [==============================] - 303s 397ms/step - loss: 0.0022 - accuracy: 0.9991 - precision: 0.9970 - recall: 0.9960 - val_loss: 1.0695 - val_accuracy: 0.8027 - val_precision: 0.2994 - val_recall: 0.4491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, lstm_cell_26_layer_call_fn, lstm_cell_26_layer_call_and_return_conditional_losses, lstm_cell_27_layer_call_fn, lstm_cell_27_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/GabHateCorpus_no_train_bilstm\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/GabHateCorpus_no_train_bilstm\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 0.844763994216919\n",
      "gru\n",
      "Epoch 1/10\n",
      "763/763 [==============================] - 216s 278ms/step - loss: 0.0371 - accuracy: 0.9689 - precision: 0.8759 - recall: 0.8717 - val_loss: 0.8563 - val_accuracy: 0.8378 - val_precision: 0.3209 - val_recall: 0.2844\n",
      "Epoch 2/10\n",
      "763/763 [==============================] - 199s 261ms/step - loss: 0.0039 - accuracy: 0.9990 - precision: 0.9983 - recall: 0.9937 - val_loss: 1.0127 - val_accuracy: 0.8355 - val_precision: 0.3427 - val_recall: 0.3653\n",
      "Epoch 3/10\n",
      "763/763 [==============================] - 201s 264ms/step - loss: 0.0038 - accuracy: 0.9990 - precision: 0.9970 - recall: 0.9947 - val_loss: 0.9929 - val_accuracy: 0.8348 - val_precision: 0.3443 - val_recall: 0.3772\n",
      "Epoch 4/10\n",
      "763/763 [==============================] - 204s 268ms/step - loss: 0.0040 - accuracy: 0.9988 - precision: 0.9963 - recall: 0.9937 - val_loss: 1.0385 - val_accuracy: 0.8274 - val_precision: 0.3255 - val_recall: 0.3743\n",
      "Epoch 5/10\n",
      "763/763 [==============================] - 205s 269ms/step - loss: 0.0024 - accuracy: 0.9991 - precision: 0.9960 - recall: 0.9963 - val_loss: 1.2110 - val_accuracy: 0.8378 - val_precision: 0.3520 - val_recall: 0.3772\n",
      "Epoch 6/10\n",
      "763/763 [==============================] - 211s 276ms/step - loss: 0.0022 - accuracy: 0.9992 - precision: 0.9980 - recall: 0.9957 - val_loss: 1.1862 - val_accuracy: 0.8385 - val_precision: 0.3514 - val_recall: 0.3683\n",
      "Epoch 7/10\n",
      "763/763 [==============================] - 207s 272ms/step - loss: 0.0018 - accuracy: 0.9993 - precision: 0.9970 - recall: 0.9970 - val_loss: 1.2740 - val_accuracy: 0.8274 - val_precision: 0.3299 - val_recall: 0.3892\n",
      "Epoch 8/10\n",
      "763/763 [==============================] - 198s 259ms/step - loss: 0.0015 - accuracy: 0.9994 - precision: 0.9987 - recall: 0.9963 - val_loss: 1.4557 - val_accuracy: 0.8355 - val_precision: 0.3519 - val_recall: 0.3982\n",
      "Epoch 9/10\n",
      "763/763 [==============================] - 212s 277ms/step - loss: 0.0045 - accuracy: 0.9984 - precision: 0.9940 - recall: 0.9927 - val_loss: 1.0820 - val_accuracy: 0.8348 - val_precision: 0.3476 - val_recall: 0.3892\n",
      "Epoch 10/10\n",
      "763/763 [==============================] - 200s 262ms/step - loss: 0.0020 - accuracy: 0.9993 - precision: 0.9980 - recall: 0.9960 - val_loss: 1.1602 - val_accuracy: 0.8359 - val_precision: 0.3419 - val_recall: 0.3593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, gru_cell_6_layer_call_fn, gru_cell_6_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/GabHateCorpus_no_train_gru\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/GabHateCorpus_no_train_gru\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 0.8377581238746643\n"
     ]
    }
   ],
   "source": [
    "h = cnn(custom_encoder,custom_embedding,embedding_name)\n",
    "h = rnn(custom_encoder,custom_embedding,embedding_name)\n",
    "h = lstm(custom_encoder,custom_embedding,embedding_name)\n",
    "# h = bilstm(custom_encoder,custom_embedding,embedding_name)\n",
    "h = gru(custom_encoder,custom_embedding,embedding_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/Implicit_hate_corpus_fasttext_cnn\n",
      "models/Implicit_hate_corpus_fasttext_rnn\n",
      "models/Implicit_hate_corpus_fasttext_lstm\n",
      "models/Implicit_hate_corpus_fasttext_bilstm\n",
      "models/Implicit_hate_corpus_fasttext_gru\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_e754e_row0_col5, #T_e754e_row0_col7, #T_e754e_row0_col8, #T_e754e_row1_col0, #T_e754e_row1_col7, #T_e754e_row1_col8, #T_e754e_row2_col7, #T_e754e_row2_col8, #T_e754e_row3_col1, #T_e754e_row3_col3, #T_e754e_row3_col4, #T_e754e_row3_col7, #T_e754e_row3_col8, #T_e754e_row4_col2, #T_e754e_row4_col6, #T_e754e_row4_col7, #T_e754e_row4_col8 {\n",
       "  background-color: red;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_e754e\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e754e_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_e754e_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_e754e_level0_col2\" class=\"col_heading level0 col2\" >precision</th>\n",
       "      <th id=\"T_e754e_level0_col3\" class=\"col_heading level0 col3\" >recall</th>\n",
       "      <th id=\"T_e754e_level0_col4\" class=\"col_heading level0 col4\" >f1-score</th>\n",
       "      <th id=\"T_e754e_level0_col5\" class=\"col_heading level0 col5\" >hate f1</th>\n",
       "      <th id=\"T_e754e_level0_col6\" class=\"col_heading level0 col6\" >non-hate f1</th>\n",
       "      <th id=\"T_e754e_level0_col7\" class=\"col_heading level0 col7\" >hate support</th>\n",
       "      <th id=\"T_e754e_level0_col8\" class=\"col_heading level0 col8\" >non-hate support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e754e_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_e754e_row0_col0\" class=\"data row0 col0\" >Implicit_hate_corpus_fasttext_cnn</td>\n",
       "      <td id=\"T_e754e_row0_col1\" class=\"data row0 col1\" >0.691962</td>\n",
       "      <td id=\"T_e754e_row0_col2\" class=\"data row0 col2\" >0.672204</td>\n",
       "      <td id=\"T_e754e_row0_col3\" class=\"data row0 col3\" >0.673499</td>\n",
       "      <td id=\"T_e754e_row0_col4\" class=\"data row0 col4\" >0.672811</td>\n",
       "      <td id=\"T_e754e_row0_col5\" class=\"data row0 col5\" >0.593654</td>\n",
       "      <td id=\"T_e754e_row0_col6\" class=\"data row0 col6\" >0.751968</td>\n",
       "      <td id=\"T_e754e_row0_col7\" class=\"data row0 col7\" >2419</td>\n",
       "      <td id=\"T_e754e_row0_col8\" class=\"data row0 col8\" >4025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e754e_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_e754e_row1_col0\" class=\"data row1 col0\" >Implicit_hate_corpus_fasttext_rnn</td>\n",
       "      <td id=\"T_e754e_row1_col1\" class=\"data row1 col1\" >0.699876</td>\n",
       "      <td id=\"T_e754e_row1_col2\" class=\"data row1 col2\" >0.682861</td>\n",
       "      <td id=\"T_e754e_row1_col3\" class=\"data row1 col3\" >0.647670</td>\n",
       "      <td id=\"T_e754e_row1_col4\" class=\"data row1 col4\" >0.652007</td>\n",
       "      <td id=\"T_e754e_row1_col5\" class=\"data row1 col5\" >0.522940</td>\n",
       "      <td id=\"T_e754e_row1_col6\" class=\"data row1 col6\" >0.781073</td>\n",
       "      <td id=\"T_e754e_row1_col7\" class=\"data row1 col7\" >2419</td>\n",
       "      <td id=\"T_e754e_row1_col8\" class=\"data row1 col8\" >4025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e754e_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_e754e_row2_col0\" class=\"data row2 col0\" >Implicit_hate_corpus_fasttext_lstm</td>\n",
       "      <td id=\"T_e754e_row2_col1\" class=\"data row2 col1\" >0.714618</td>\n",
       "      <td id=\"T_e754e_row2_col2\" class=\"data row2 col2\" >0.698594</td>\n",
       "      <td id=\"T_e754e_row2_col3\" class=\"data row2 col3\" >0.669451</td>\n",
       "      <td id=\"T_e754e_row2_col4\" class=\"data row2 col4\" >0.675276</td>\n",
       "      <td id=\"T_e754e_row2_col5\" class=\"data row2 col5\" >0.562247</td>\n",
       "      <td id=\"T_e754e_row2_col6\" class=\"data row2 col6\" >0.788304</td>\n",
       "      <td id=\"T_e754e_row2_col7\" class=\"data row2 col7\" >2419</td>\n",
       "      <td id=\"T_e754e_row2_col8\" class=\"data row2 col8\" >4025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e754e_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_e754e_row3_col0\" class=\"data row3 col0\" >Implicit_hate_corpus_fasttext_bilstm</td>\n",
       "      <td id=\"T_e754e_row3_col1\" class=\"data row3 col1\" >0.717722</td>\n",
       "      <td id=\"T_e754e_row3_col2\" class=\"data row3 col2\" >0.700199</td>\n",
       "      <td id=\"T_e754e_row3_col3\" class=\"data row3 col3\" >0.677956</td>\n",
       "      <td id=\"T_e754e_row3_col4\" class=\"data row3 col4\" >0.683571</td>\n",
       "      <td id=\"T_e754e_row3_col5\" class=\"data row3 col5\" >0.579616</td>\n",
       "      <td id=\"T_e754e_row3_col6\" class=\"data row3 col6\" >0.787525</td>\n",
       "      <td id=\"T_e754e_row3_col7\" class=\"data row3 col7\" >2419</td>\n",
       "      <td id=\"T_e754e_row3_col8\" class=\"data row3 col8\" >4025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e754e_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_e754e_row4_col0\" class=\"data row4 col0\" >Implicit_hate_corpus_fasttext_gru</td>\n",
       "      <td id=\"T_e754e_row4_col1\" class=\"data row4 col1\" >0.714618</td>\n",
       "      <td id=\"T_e754e_row4_col2\" class=\"data row4 col2\" >0.703246</td>\n",
       "      <td id=\"T_e754e_row4_col3\" class=\"data row4 col3\" >0.662193</td>\n",
       "      <td id=\"T_e754e_row4_col4\" class=\"data row4 col4\" >0.667807</td>\n",
       "      <td id=\"T_e754e_row4_col5\" class=\"data row4 col5\" >0.543106</td>\n",
       "      <td id=\"T_e754e_row4_col6\" class=\"data row4 col6\" >0.792508</td>\n",
       "      <td id=\"T_e754e_row4_col7\" class=\"data row4 col7\" >2419</td>\n",
       "      <td id=\"T_e754e_row4_col8\" class=\"data row4 col8\" >4025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x19db4958210>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds =\"Implicit_hate_corpus\"\n",
    "em = 'fasttext'\n",
    "get_result_nn([ds+\"_\"+em+\"_cnn\", ds+\"_\"+em+\"_rnn\", ds+\"_\"+em+\"_lstm\",ds+\"_\"+em+\"_bilstm\",ds+\"_\"+em+\"_gru\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/GabHateCorpus_no_train_cnn\n",
      "models/GabHateCorpus_no_train_rnn\n",
      "models/GabHateCorpus_no_train_lstm\n",
      "models/GabHateCorpus_no_train_bilstm\n",
      "models/GabHateCorpus_no_train_gru\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_994a8_row0_col1, #T_994a8_row0_col2, #T_994a8_row0_col4, #T_994a8_row0_col5, #T_994a8_row0_col6, #T_994a8_row0_col7, #T_994a8_row0_col8, #T_994a8_row1_col0, #T_994a8_row1_col7, #T_994a8_row1_col8, #T_994a8_row2_col7, #T_994a8_row2_col8, #T_994a8_row3_col3, #T_994a8_row3_col7, #T_994a8_row3_col8, #T_994a8_row4_col7, #T_994a8_row4_col8 {\n",
       "  background-color: red;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_994a8\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_994a8_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_994a8_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_994a8_level0_col2\" class=\"col_heading level0 col2\" >precision</th>\n",
       "      <th id=\"T_994a8_level0_col3\" class=\"col_heading level0 col3\" >recall</th>\n",
       "      <th id=\"T_994a8_level0_col4\" class=\"col_heading level0 col4\" >f1-score</th>\n",
       "      <th id=\"T_994a8_level0_col5\" class=\"col_heading level0 col5\" >hate f1</th>\n",
       "      <th id=\"T_994a8_level0_col6\" class=\"col_heading level0 col6\" >non-hate f1</th>\n",
       "      <th id=\"T_994a8_level0_col7\" class=\"col_heading level0 col7\" >hate support</th>\n",
       "      <th id=\"T_994a8_level0_col8\" class=\"col_heading level0 col8\" >non-hate support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_994a8_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_994a8_row0_col0\" class=\"data row0 col0\" >GabHateCorpus_no_train_cnn</td>\n",
       "      <td id=\"T_994a8_row0_col1\" class=\"data row0 col1\" >0.879794</td>\n",
       "      <td id=\"T_994a8_row0_col2\" class=\"data row0 col2\" >0.714788</td>\n",
       "      <td id=\"T_994a8_row0_col3\" class=\"data row0 col3\" >0.645798</td>\n",
       "      <td id=\"T_994a8_row0_col4\" class=\"data row0 col4\" >0.670194</td>\n",
       "      <td id=\"T_994a8_row0_col5\" class=\"data row0 col5\" >0.407273</td>\n",
       "      <td id=\"T_994a8_row0_col6\" class=\"data row0 col6\" >0.933114</td>\n",
       "      <td id=\"T_994a8_row0_col7\" class=\"data row0 col7\" >334</td>\n",
       "      <td id=\"T_994a8_row0_col8\" class=\"data row0 col8\" >2378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_994a8_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_994a8_row1_col0\" class=\"data row1 col0\" >GabHateCorpus_no_train_rnn</td>\n",
       "      <td id=\"T_994a8_row1_col1\" class=\"data row1 col1\" >0.832596</td>\n",
       "      <td id=\"T_994a8_row1_col2\" class=\"data row1 col2\" >0.608088</td>\n",
       "      <td id=\"T_994a8_row1_col3\" class=\"data row1 col3\" >0.604730</td>\n",
       "      <td id=\"T_994a8_row1_col4\" class=\"data row1 col4\" >0.606354</td>\n",
       "      <td id=\"T_994a8_row1_col5\" class=\"data row1 col5\" >0.307927</td>\n",
       "      <td id=\"T_994a8_row1_col6\" class=\"data row1 col6\" >0.904782</td>\n",
       "      <td id=\"T_994a8_row1_col7\" class=\"data row1 col7\" >334</td>\n",
       "      <td id=\"T_994a8_row1_col8\" class=\"data row1 col8\" >2378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_994a8_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_994a8_row2_col0\" class=\"data row2 col0\" >GabHateCorpus_no_train_lstm</td>\n",
       "      <td id=\"T_994a8_row2_col1\" class=\"data row2 col1\" >0.842552</td>\n",
       "      <td id=\"T_994a8_row2_col2\" class=\"data row2 col2\" >0.629033</td>\n",
       "      <td id=\"T_994a8_row2_col3\" class=\"data row2 col3\" >0.621988</td>\n",
       "      <td id=\"T_994a8_row2_col4\" class=\"data row2 col4\" >0.625322</td>\n",
       "      <td id=\"T_994a8_row2_col5\" class=\"data row2 col5\" >0.340031</td>\n",
       "      <td id=\"T_994a8_row2_col6\" class=\"data row2 col6\" >0.910613</td>\n",
       "      <td id=\"T_994a8_row2_col7\" class=\"data row2 col7\" >334</td>\n",
       "      <td id=\"T_994a8_row2_col8\" class=\"data row2 col8\" >2378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_994a8_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_994a8_row3_col0\" class=\"data row3 col0\" >GabHateCorpus_no_train_bilstm</td>\n",
       "      <td id=\"T_994a8_row3_col1\" class=\"data row3 col1\" >0.802729</td>\n",
       "      <td id=\"T_994a8_row3_col2\" class=\"data row3 col2\" >0.608090</td>\n",
       "      <td id=\"T_994a8_row3_col3\" class=\"data row3 col3\" >0.650749</td>\n",
       "      <td id=\"T_994a8_row3_col4\" class=\"data row3 col4\" >0.621349</td>\n",
       "      <td id=\"T_994a8_row3_col5\" class=\"data row3 col5\" >0.359281</td>\n",
       "      <td id=\"T_994a8_row3_col6\" class=\"data row3 col6\" >0.883417</td>\n",
       "      <td id=\"T_994a8_row3_col7\" class=\"data row3 col7\" >334</td>\n",
       "      <td id=\"T_994a8_row3_col8\" class=\"data row3 col8\" >2378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_994a8_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_994a8_row4_col0\" class=\"data row4 col0\" >GabHateCorpus_no_train_gru</td>\n",
       "      <td id=\"T_994a8_row4_col1\" class=\"data row4 col1\" >0.835914</td>\n",
       "      <td id=\"T_994a8_row4_col2\" class=\"data row4 col2\" >0.625620</td>\n",
       "      <td id=\"T_994a8_row4_col3\" class=\"data row4 col3\" >0.631070</td>\n",
       "      <td id=\"T_994a8_row4_col4\" class=\"data row4 col4\" >0.628232</td>\n",
       "      <td id=\"T_994a8_row4_col5\" class=\"data row4 col5\" >0.350365</td>\n",
       "      <td id=\"T_994a8_row4_col6\" class=\"data row4 col6\" >0.906098</td>\n",
       "      <td id=\"T_994a8_row4_col7\" class=\"data row4 col7\" >334</td>\n",
       "      <td id=\"T_994a8_row4_col8\" class=\"data row4 col8\" >2378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1fae94e13d0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds =\"GabHateCorpus\"\n",
    "em = 'no_train'\n",
    "get_result_nn([ds+\"_\"+em+\"_cnn\", ds+\"_\"+em+\"_rnn\", ds+\"_\"+em+\"_lstm\",ds+\"_\"+em+\"_bilstm\",ds+\"_\"+em+\"_gru\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/GabHateCorpus_word2vec_cnn\n",
      "models/GabHateCorpus_word2vec_rnn\n",
      "models/GabHateCorpus_word2vec_lstm\n",
      "models/GabHateCorpus_word2vec_bilstm\n",
      "models/GabHateCorpus_word2vec_gru\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_043a0_row0_col3, #T_043a0_row0_col4, #T_043a0_row0_col5, #T_043a0_row0_col7, #T_043a0_row0_col8, #T_043a0_row1_col0, #T_043a0_row1_col7, #T_043a0_row1_col8, #T_043a0_row2_col7, #T_043a0_row2_col8, #T_043a0_row3_col7, #T_043a0_row3_col8, #T_043a0_row4_col1, #T_043a0_row4_col2, #T_043a0_row4_col6, #T_043a0_row4_col7, #T_043a0_row4_col8 {\n",
       "  background-color: red;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_043a0\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_043a0_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_043a0_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_043a0_level0_col2\" class=\"col_heading level0 col2\" >precision</th>\n",
       "      <th id=\"T_043a0_level0_col3\" class=\"col_heading level0 col3\" >recall</th>\n",
       "      <th id=\"T_043a0_level0_col4\" class=\"col_heading level0 col4\" >f1-score</th>\n",
       "      <th id=\"T_043a0_level0_col5\" class=\"col_heading level0 col5\" >hate f1</th>\n",
       "      <th id=\"T_043a0_level0_col6\" class=\"col_heading level0 col6\" >non-hate f1</th>\n",
       "      <th id=\"T_043a0_level0_col7\" class=\"col_heading level0 col7\" >hate support</th>\n",
       "      <th id=\"T_043a0_level0_col8\" class=\"col_heading level0 col8\" >non-hate support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_043a0_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_043a0_row0_col0\" class=\"data row0 col0\" >GabHateCorpus_word2vec_cnn</td>\n",
       "      <td id=\"T_043a0_row0_col1\" class=\"data row0 col1\" >0.887906</td>\n",
       "      <td id=\"T_043a0_row0_col2\" class=\"data row0 col2\" >0.746150</td>\n",
       "      <td id=\"T_043a0_row0_col3\" class=\"data row0 col3\" >0.642703</td>\n",
       "      <td id=\"T_043a0_row0_col4\" class=\"data row0 col4\" >0.674457</td>\n",
       "      <td id=\"T_043a0_row0_col5\" class=\"data row0 col5\" >0.410853</td>\n",
       "      <td id=\"T_043a0_row0_col6\" class=\"data row0 col6\" >0.938060</td>\n",
       "      <td id=\"T_043a0_row0_col7\" class=\"data row0 col7\" >334</td>\n",
       "      <td id=\"T_043a0_row0_col8\" class=\"data row0 col8\" >2378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_043a0_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_043a0_row1_col0\" class=\"data row1 col0\" >GabHateCorpus_word2vec_rnn</td>\n",
       "      <td id=\"T_043a0_row1_col1\" class=\"data row1 col1\" >0.873894</td>\n",
       "      <td id=\"T_043a0_row1_col2\" class=\"data row1 col2\" >0.694916</td>\n",
       "      <td id=\"T_043a0_row1_col3\" class=\"data row1 col3\" >0.633426</td>\n",
       "      <td id=\"T_043a0_row1_col4\" class=\"data row1 col4\" >0.655119</td>\n",
       "      <td id=\"T_043a0_row1_col5\" class=\"data row1 col5\" >0.380435</td>\n",
       "      <td id=\"T_043a0_row1_col6\" class=\"data row1 col6\" >0.929803</td>\n",
       "      <td id=\"T_043a0_row1_col7\" class=\"data row1 col7\" >334</td>\n",
       "      <td id=\"T_043a0_row1_col8\" class=\"data row1 col8\" >2378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_043a0_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_043a0_row2_col0\" class=\"data row2 col0\" >GabHateCorpus_word2vec_lstm</td>\n",
       "      <td id=\"T_043a0_row2_col1\" class=\"data row2 col1\" >0.888274</td>\n",
       "      <td id=\"T_043a0_row2_col2\" class=\"data row2 col2\" >0.775411</td>\n",
       "      <td id=\"T_043a0_row2_col3\" class=\"data row2 col3\" >0.591443</td>\n",
       "      <td id=\"T_043a0_row2_col4\" class=\"data row2 col4\" >0.621357</td>\n",
       "      <td id=\"T_043a0_row2_col5\" class=\"data row2 col5\" >0.303448</td>\n",
       "      <td id=\"T_043a0_row2_col6\" class=\"data row2 col6\" >0.939266</td>\n",
       "      <td id=\"T_043a0_row2_col7\" class=\"data row2 col7\" >334</td>\n",
       "      <td id=\"T_043a0_row2_col8\" class=\"data row2 col8\" >2378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_043a0_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_043a0_row3_col0\" class=\"data row3 col0\" >GabHateCorpus_word2vec_bilstm</td>\n",
       "      <td id=\"T_043a0_row3_col1\" class=\"data row3 col1\" >0.884956</td>\n",
       "      <td id=\"T_043a0_row3_col2\" class=\"data row3 col2\" >0.734727</td>\n",
       "      <td id=\"T_043a0_row3_col3\" class=\"data row3 col3\" >0.633300</td>\n",
       "      <td id=\"T_043a0_row3_col4\" class=\"data row3 col4\" >0.663554</td>\n",
       "      <td id=\"T_043a0_row3_col5\" class=\"data row3 col5\" >0.390625</td>\n",
       "      <td id=\"T_043a0_row3_col6\" class=\"data row3 col6\" >0.936482</td>\n",
       "      <td id=\"T_043a0_row3_col7\" class=\"data row3 col7\" >334</td>\n",
       "      <td id=\"T_043a0_row3_col8\" class=\"data row3 col8\" >2378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_043a0_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_043a0_row4_col0\" class=\"data row4 col0\" >GabHateCorpus_word2vec_gru</td>\n",
       "      <td id=\"T_043a0_row4_col1\" class=\"data row4 col1\" >0.893437</td>\n",
       "      <td id=\"T_043a0_row4_col2\" class=\"data row4 col2\" >0.807469</td>\n",
       "      <td id=\"T_043a0_row4_col3\" class=\"data row4 col3\" >0.605968</td>\n",
       "      <td id=\"T_043a0_row4_col4\" class=\"data row4 col4\" >0.641856</td>\n",
       "      <td id=\"T_043a0_row4_col5\" class=\"data row4 col5\" >0.341686</td>\n",
       "      <td id=\"T_043a0_row4_col6\" class=\"data row4 col6\" >0.942026</td>\n",
       "      <td id=\"T_043a0_row4_col7\" class=\"data row4 col7\" >334</td>\n",
       "      <td id=\"T_043a0_row4_col8\" class=\"data row4 col8\" >2378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1fb282f6510>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds =\"GabHateCorpus\"\n",
    "em = 'word2vec'\n",
    "get_result_nn([ds+\"_\"+em+\"_cnn\", ds+\"_\"+em+\"_rnn\", ds+\"_\"+em+\"_lstm\",ds+\"_\"+em+\"_bilstm\",ds+\"_\"+em+\"_gru\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f5372_row0_col6, #T_f5372_row0_col8, #T_f5372_row1_col0, #T_f5372_row1_col6, #T_f5372_row1_col8, #T_f5372_row2_col1, #T_f5372_row2_col2, #T_f5372_row2_col6, #T_f5372_row2_col7, #T_f5372_row2_col8, #T_f5372_row3_col6, #T_f5372_row3_col8, #T_f5372_row4_col3, #T_f5372_row4_col4, #T_f5372_row4_col5, #T_f5372_row4_col6, #T_f5372_row4_col8 {\n",
       "  background-color: red;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f5372\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f5372_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_f5372_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_f5372_level0_col2\" class=\"col_heading level0 col2\" >precision</th>\n",
       "      <th id=\"T_f5372_level0_col3\" class=\"col_heading level0 col3\" >recall</th>\n",
       "      <th id=\"T_f5372_level0_col4\" class=\"col_heading level0 col4\" >f1-score</th>\n",
       "      <th id=\"T_f5372_level0_col5\" class=\"col_heading level0 col5\" >hate f1</th>\n",
       "      <th id=\"T_f5372_level0_col6\" class=\"col_heading level0 col6\" >hate support</th>\n",
       "      <th id=\"T_f5372_level0_col7\" class=\"col_heading level0 col7\" >non-hate f1</th>\n",
       "      <th id=\"T_f5372_level0_col8\" class=\"col_heading level0 col8\" >non-hate support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f5372_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_f5372_row0_col0\" class=\"data row0 col0\" >GabHateCorpus_fasttext_cnn</td>\n",
       "      <td id=\"T_f5372_row0_col1\" class=\"data row0 col1\" >0.883944</td>\n",
       "      <td id=\"T_f5372_row0_col2\" class=\"data row0 col2\" >0.724345</td>\n",
       "      <td id=\"T_f5372_row0_col3\" class=\"data row0 col3\" >0.603634</td>\n",
       "      <td id=\"T_f5372_row0_col4\" class=\"data row0 col4\" >0.632063</td>\n",
       "      <td id=\"T_f5372_row0_col5\" class=\"data row0 col5\" >0.327635</td>\n",
       "      <td id=\"T_f5372_row0_col6\" class=\"data row0 col6\" >983</td>\n",
       "      <td id=\"T_f5372_row0_col7\" class=\"data row0 col7\" >0.936491</td>\n",
       "      <td id=\"T_f5372_row0_col8\" class=\"data row0 col8\" >7151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5372_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_f5372_row1_col0\" class=\"data row1 col0\" >GabHateCorpus_fasttext_rnn</td>\n",
       "      <td id=\"T_f5372_row1_col1\" class=\"data row1 col1\" >0.879149</td>\n",
       "      <td id=\"T_f5372_row1_col2\" class=\"data row1 col2\" >0.689995</td>\n",
       "      <td id=\"T_f5372_row1_col3\" class=\"data row1 col3\" >0.503949</td>\n",
       "      <td id=\"T_f5372_row1_col4\" class=\"data row1 col4\" >0.476797</td>\n",
       "      <td id=\"T_f5372_row1_col5\" class=\"data row1 col5\" >0.017982</td>\n",
       "      <td id=\"T_f5372_row1_col6\" class=\"data row1 col6\" >983</td>\n",
       "      <td id=\"T_f5372_row1_col7\" class=\"data row1 col7\" >0.935613</td>\n",
       "      <td id=\"T_f5372_row1_col8\" class=\"data row1 col8\" >7151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5372_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_f5372_row2_col0\" class=\"data row2 col0\" >GabHateCorpus_fasttext_lstm</td>\n",
       "      <td id=\"T_f5372_row2_col1\" class=\"data row2 col1\" >0.891320</td>\n",
       "      <td id=\"T_f5372_row2_col2\" class=\"data row2 col2\" >0.795910</td>\n",
       "      <td id=\"T_f5372_row2_col3\" class=\"data row2 col3\" >0.584577</td>\n",
       "      <td id=\"T_f5372_row2_col4\" class=\"data row2 col4\" >0.613565</td>\n",
       "      <td id=\"T_f5372_row2_col5\" class=\"data row2 col5\" >0.285945</td>\n",
       "      <td id=\"T_f5372_row2_col6\" class=\"data row2 col6\" >983</td>\n",
       "      <td id=\"T_f5372_row2_col7\" class=\"data row2 col7\" >0.941184</td>\n",
       "      <td id=\"T_f5372_row2_col8\" class=\"data row2 col8\" >7151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5372_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_f5372_row3_col0\" class=\"data row3 col0\" >GabHateCorpus_fasttext_bilstm</td>\n",
       "      <td id=\"T_f5372_row3_col1\" class=\"data row3 col1\" >0.890214</td>\n",
       "      <td id=\"T_f5372_row3_col2\" class=\"data row3 col2\" >0.755402</td>\n",
       "      <td id=\"T_f5372_row3_col3\" class=\"data row3 col3\" >0.620362</td>\n",
       "      <td id=\"T_f5372_row3_col4\" class=\"data row3 col4\" >0.653948</td>\n",
       "      <td id=\"T_f5372_row3_col5\" class=\"data row3 col5\" >0.368011</td>\n",
       "      <td id=\"T_f5372_row3_col6\" class=\"data row3 col6\" >983</td>\n",
       "      <td id=\"T_f5372_row3_col7\" class=\"data row3 col7\" >0.939886</td>\n",
       "      <td id=\"T_f5372_row3_col8\" class=\"data row3 col8\" >7151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5372_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_f5372_row4_col0\" class=\"data row4 col0\" >GabHateCorpus_fasttext_gru</td>\n",
       "      <td id=\"T_f5372_row4_col1\" class=\"data row4 col1\" >0.889107</td>\n",
       "      <td id=\"T_f5372_row4_col2\" class=\"data row4 col2\" >0.740086</td>\n",
       "      <td id=\"T_f5372_row4_col3\" class=\"data row4 col3\" >0.664044</td>\n",
       "      <td id=\"T_f5372_row4_col4\" class=\"data row4 col4\" >0.691493</td>\n",
       "      <td id=\"T_f5372_row4_col5\" class=\"data row4 col5\" >0.444581</td>\n",
       "      <td id=\"T_f5372_row4_col6\" class=\"data row4 col6\" >983</td>\n",
       "      <td id=\"T_f5372_row4_col7\" class=\"data row4 col7\" >0.938405</td>\n",
       "      <td id=\"T_f5372_row4_col8\" class=\"data row4 col8\" >7151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x22adb39c2d0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds =\"GabHateCorpus\"\n",
    "em = 'fasttext'\n",
    "get_result_nn([ds+\"_\"+em+\"_cnn\", ds+\"_\"+em+\"_rnn\", ds+\"_\"+em+\"_lstm\",ds+\"_\"+em+\"_bilstm\",ds+\"_\"+em+\"_gru\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_26f23_row1_col0, #T_26f23_row2_col1, #T_26f23_row2_col2, #T_26f23_row2_col3, #T_26f23_row2_col4 {\n",
       "  background-color: red;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_26f23\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_26f23_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_26f23_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_26f23_level0_col2\" class=\"col_heading level0 col2\" >Precision</th>\n",
       "      <th id=\"T_26f23_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_26f23_level0_col4\" class=\"col_heading level0 col4\" >F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_26f23_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_26f23_row0_col0\" class=\"data row0 col0\" >GabHateCorpus_glove_cnn</td>\n",
       "      <td id=\"T_26f23_row0_col1\" class=\"data row0 col1\" >0.887755</td>\n",
       "      <td id=\"T_26f23_row0_col2\" class=\"data row0 col2\" >0.864472</td>\n",
       "      <td id=\"T_26f23_row0_col3\" class=\"data row0 col3\" >0.887755</td>\n",
       "      <td id=\"T_26f23_row0_col4\" class=\"data row0 col4\" >0.864063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_26f23_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_26f23_row1_col0\" class=\"data row1 col0\" >GabHateCorpus_glove_rnn</td>\n",
       "      <td id=\"T_26f23_row1_col1\" class=\"data row1 col1\" >0.885050</td>\n",
       "      <td id=\"T_26f23_row1_col2\" class=\"data row1 col2\" >0.858332</td>\n",
       "      <td id=\"T_26f23_row1_col3\" class=\"data row1 col3\" >0.885050</td>\n",
       "      <td id=\"T_26f23_row1_col4\" class=\"data row1 col4\" >0.852100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_26f23_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_26f23_row2_col0\" class=\"data row2 col0\" >GabHateCorpus_glove_lstm</td>\n",
       "      <td id=\"T_26f23_row2_col1\" class=\"data row2 col1\" >0.892304</td>\n",
       "      <td id=\"T_26f23_row2_col2\" class=\"data row2 col2\" >0.880023</td>\n",
       "      <td id=\"T_26f23_row2_col3\" class=\"data row2 col3\" >0.892304</td>\n",
       "      <td id=\"T_26f23_row2_col4\" class=\"data row2 col4\" >0.884057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_26f23_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_26f23_row3_col0\" class=\"data row3 col0\" >GabHateCorpus_glove_bilstm</td>\n",
       "      <td id=\"T_26f23_row3_col1\" class=\"data row3 col1\" >0.880747</td>\n",
       "      <td id=\"T_26f23_row3_col2\" class=\"data row3 col2\" >0.872886</td>\n",
       "      <td id=\"T_26f23_row3_col3\" class=\"data row3 col3\" >0.880747</td>\n",
       "      <td id=\"T_26f23_row3_col4\" class=\"data row3 col4\" >0.876315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_26f23_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_26f23_row4_col0\" class=\"data row4 col0\" >GabHateCorpus_glove_gru</td>\n",
       "      <td id=\"T_26f23_row4_col1\" class=\"data row4 col1\" >0.882837</td>\n",
       "      <td id=\"T_26f23_row4_col2\" class=\"data row4 col2\" >0.876953</td>\n",
       "      <td id=\"T_26f23_row4_col3\" class=\"data row4 col3\" >0.882837</td>\n",
       "      <td id=\"T_26f23_row4_col4\" class=\"data row4 col4\" >0.879606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x23b9d55ce10>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = 'GabHateCorpus'\n",
    "em = 'glove'\n",
    "get_result_nn([ds+\"_\"+em+\"_cnn\", ds+\"_\"+em+\"_rnn\", ds+\"_\"+em+\"_lstm\",ds+\"_\"+em+\"_bilstm\",ds+\"_\"+em+\"_gru\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/SE2019_word2vec_cnn\n",
      "models/SE2019_word2vec_rnn\n",
      "models/SE2019_word2vec_lstm\n",
      "models/SE2019_word2vec_bilstm\n",
      "models/SE2019_word2vec_gru\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_aeaa9_row0_col5, #T_aeaa9_row0_col7, #T_aeaa9_row0_col8, #T_aeaa9_row1_col0, #T_aeaa9_row1_col7, #T_aeaa9_row1_col8, #T_aeaa9_row2_col2, #T_aeaa9_row2_col6, #T_aeaa9_row2_col7, #T_aeaa9_row2_col8, #T_aeaa9_row3_col7, #T_aeaa9_row3_col8, #T_aeaa9_row4_col1, #T_aeaa9_row4_col3, #T_aeaa9_row4_col4, #T_aeaa9_row4_col7, #T_aeaa9_row4_col8 {\n",
       "  background-color: red;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_aeaa9\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_aeaa9_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_aeaa9_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_aeaa9_level0_col2\" class=\"col_heading level0 col2\" >precision</th>\n",
       "      <th id=\"T_aeaa9_level0_col3\" class=\"col_heading level0 col3\" >recall</th>\n",
       "      <th id=\"T_aeaa9_level0_col4\" class=\"col_heading level0 col4\" >f1-score</th>\n",
       "      <th id=\"T_aeaa9_level0_col5\" class=\"col_heading level0 col5\" >hate f1</th>\n",
       "      <th id=\"T_aeaa9_level0_col6\" class=\"col_heading level0 col6\" >non-hate f1</th>\n",
       "      <th id=\"T_aeaa9_level0_col7\" class=\"col_heading level0 col7\" >hate support</th>\n",
       "      <th id=\"T_aeaa9_level0_col8\" class=\"col_heading level0 col8\" >non-hate support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_aeaa9_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_aeaa9_row0_col0\" class=\"data row0 col0\" >SE2019_word2vec_cnn</td>\n",
       "      <td id=\"T_aeaa9_row0_col1\" class=\"data row0 col1\" >0.700488</td>\n",
       "      <td id=\"T_aeaa9_row0_col2\" class=\"data row0 col2\" >0.696548</td>\n",
       "      <td id=\"T_aeaa9_row0_col3\" class=\"data row0 col3\" >0.700337</td>\n",
       "      <td id=\"T_aeaa9_row0_col4\" class=\"data row0 col4\" >0.697045</td>\n",
       "      <td id=\"T_aeaa9_row0_col5\" class=\"data row0 col5\" >0.664750</td>\n",
       "      <td id=\"T_aeaa9_row0_col6\" class=\"data row0 col6\" >0.729341</td>\n",
       "      <td id=\"T_aeaa9_row0_col7\" class=\"data row0 col7\" >1653</td>\n",
       "      <td id=\"T_aeaa9_row0_col8\" class=\"data row0 col8\" >2240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_aeaa9_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_aeaa9_row1_col0\" class=\"data row1 col0\" >SE2019_word2vec_rnn</td>\n",
       "      <td id=\"T_aeaa9_row1_col1\" class=\"data row1 col1\" >0.669407</td>\n",
       "      <td id=\"T_aeaa9_row1_col2\" class=\"data row1 col2\" >0.660556</td>\n",
       "      <td id=\"T_aeaa9_row1_col3\" class=\"data row1 col3\" >0.655255</td>\n",
       "      <td id=\"T_aeaa9_row1_col4\" class=\"data row1 col4\" >0.656664</td>\n",
       "      <td id=\"T_aeaa9_row1_col5\" class=\"data row1 col5\" >0.590519</td>\n",
       "      <td id=\"T_aeaa9_row1_col6\" class=\"data row1 col6\" >0.722809</td>\n",
       "      <td id=\"T_aeaa9_row1_col7\" class=\"data row1 col7\" >1653</td>\n",
       "      <td id=\"T_aeaa9_row1_col8\" class=\"data row1 col8\" >2240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_aeaa9_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_aeaa9_row2_col0\" class=\"data row2 col0\" >SE2019_word2vec_lstm</td>\n",
       "      <td id=\"T_aeaa9_row2_col1\" class=\"data row2 col1\" >0.713332</td>\n",
       "      <td id=\"T_aeaa9_row2_col2\" class=\"data row2 col2\" >0.711090</td>\n",
       "      <td id=\"T_aeaa9_row2_col3\" class=\"data row2 col3\" >0.693266</td>\n",
       "      <td id=\"T_aeaa9_row2_col4\" class=\"data row2 col4\" >0.696179</td>\n",
       "      <td id=\"T_aeaa9_row2_col5\" class=\"data row2 col5\" >0.623989</td>\n",
       "      <td id=\"T_aeaa9_row2_col6\" class=\"data row2 col6\" >0.768369</td>\n",
       "      <td id=\"T_aeaa9_row2_col7\" class=\"data row2 col7\" >1653</td>\n",
       "      <td id=\"T_aeaa9_row2_col8\" class=\"data row2 col8\" >2240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_aeaa9_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_aeaa9_row3_col0\" class=\"data row3 col0\" >SE2019_word2vec_bilstm</td>\n",
       "      <td id=\"T_aeaa9_row3_col1\" class=\"data row3 col1\" >0.713075</td>\n",
       "      <td id=\"T_aeaa9_row3_col2\" class=\"data row3 col2\" >0.708568</td>\n",
       "      <td id=\"T_aeaa9_row3_col3\" class=\"data row3 col3\" >0.695976</td>\n",
       "      <td id=\"T_aeaa9_row3_col4\" class=\"data row3 col4\" >0.698712</td>\n",
       "      <td id=\"T_aeaa9_row3_col5\" class=\"data row3 col5\" >0.632928</td>\n",
       "      <td id=\"T_aeaa9_row3_col6\" class=\"data row3 col6\" >0.764495</td>\n",
       "      <td id=\"T_aeaa9_row3_col7\" class=\"data row3 col7\" >1653</td>\n",
       "      <td id=\"T_aeaa9_row3_col8\" class=\"data row3 col8\" >2240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_aeaa9_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_aeaa9_row4_col0\" class=\"data row4 col0\" >SE2019_word2vec_gru</td>\n",
       "      <td id=\"T_aeaa9_row4_col1\" class=\"data row4 col1\" >0.717185</td>\n",
       "      <td id=\"T_aeaa9_row4_col2\" class=\"data row4 col2\" >0.710524</td>\n",
       "      <td id=\"T_aeaa9_row4_col3\" class=\"data row4 col3\" >0.706840</td>\n",
       "      <td id=\"T_aeaa9_row4_col4\" class=\"data row4 col4\" >0.708230</td>\n",
       "      <td id=\"T_aeaa9_row4_col5\" class=\"data row4 col5\" >0.657116</td>\n",
       "      <td id=\"T_aeaa9_row4_col6\" class=\"data row4 col6\" >0.759344</td>\n",
       "      <td id=\"T_aeaa9_row4_col7\" class=\"data row4 col7\" >1653</td>\n",
       "      <td id=\"T_aeaa9_row4_col8\" class=\"data row4 col8\" >2240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1faff4f8510>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = 'SE2019'\n",
    "em = 'word2vec'\n",
    "get_result_nn([ds+\"_\"+em+\"_cnn\", ds+\"_\"+em+\"_rnn\", ds+\"_\"+em+\"_lstm\",ds+\"_\"+em+\"_bilstm\",ds+\"_\"+em+\"_gru\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/SE2019_fasttext_cnn\n",
      "models/SE2019_fasttext_rnn\n",
      "models/SE2019_fasttext_lstm\n",
      "models/SE2019_fasttext_bilstm\n",
      "models/SE2019_fasttext_gru\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_bd23c_row0_col7, #T_bd23c_row0_col8, #T_bd23c_row1_col0, #T_bd23c_row1_col7, #T_bd23c_row1_col8, #T_bd23c_row2_col6, #T_bd23c_row2_col7, #T_bd23c_row2_col8, #T_bd23c_row3_col1, #T_bd23c_row3_col2, #T_bd23c_row3_col3, #T_bd23c_row3_col4, #T_bd23c_row3_col5, #T_bd23c_row3_col7, #T_bd23c_row3_col8, #T_bd23c_row4_col7, #T_bd23c_row4_col8 {\n",
       "  background-color: red;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_bd23c\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_bd23c_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_bd23c_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_bd23c_level0_col2\" class=\"col_heading level0 col2\" >precision</th>\n",
       "      <th id=\"T_bd23c_level0_col3\" class=\"col_heading level0 col3\" >recall</th>\n",
       "      <th id=\"T_bd23c_level0_col4\" class=\"col_heading level0 col4\" >f1-score</th>\n",
       "      <th id=\"T_bd23c_level0_col5\" class=\"col_heading level0 col5\" >hate f1</th>\n",
       "      <th id=\"T_bd23c_level0_col6\" class=\"col_heading level0 col6\" >non-hate f1</th>\n",
       "      <th id=\"T_bd23c_level0_col7\" class=\"col_heading level0 col7\" >hate support</th>\n",
       "      <th id=\"T_bd23c_level0_col8\" class=\"col_heading level0 col8\" >non-hate support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_bd23c_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_bd23c_row0_col0\" class=\"data row0 col0\" >SE2019_fasttext_cnn</td>\n",
       "      <td id=\"T_bd23c_row0_col1\" class=\"data row0 col1\" >0.701259</td>\n",
       "      <td id=\"T_bd23c_row0_col2\" class=\"data row0 col2\" >0.696076</td>\n",
       "      <td id=\"T_bd23c_row0_col3\" class=\"data row0 col3\" >0.699025</td>\n",
       "      <td id=\"T_bd23c_row0_col4\" class=\"data row0 col4\" >0.696878</td>\n",
       "      <td id=\"T_bd23c_row0_col5\" class=\"data row0 col5\" >0.660438</td>\n",
       "      <td id=\"T_bd23c_row0_col6\" class=\"data row0 col6\" >0.733318</td>\n",
       "      <td id=\"T_bd23c_row0_col7\" class=\"data row0 col7\" >1653</td>\n",
       "      <td id=\"T_bd23c_row0_col8\" class=\"data row0 col8\" >2240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd23c_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_bd23c_row1_col0\" class=\"data row1 col0\" >SE2019_fasttext_rnn</td>\n",
       "      <td id=\"T_bd23c_row1_col1\" class=\"data row1 col1\" >0.682764</td>\n",
       "      <td id=\"T_bd23c_row1_col2\" class=\"data row1 col2\" >0.679793</td>\n",
       "      <td id=\"T_bd23c_row1_col3\" class=\"data row1 col3\" >0.658064</td>\n",
       "      <td id=\"T_bd23c_row1_col4\" class=\"data row1 col4\" >0.659184</td>\n",
       "      <td id=\"T_bd23c_row1_col5\" class=\"data row1 col5\" >0.569536</td>\n",
       "      <td id=\"T_bd23c_row1_col6\" class=\"data row1 col6\" >0.748831</td>\n",
       "      <td id=\"T_bd23c_row1_col7\" class=\"data row1 col7\" >1653</td>\n",
       "      <td id=\"T_bd23c_row1_col8\" class=\"data row1 col8\" >2240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd23c_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_bd23c_row2_col0\" class=\"data row2 col0\" >SE2019_fasttext_lstm</td>\n",
       "      <td id=\"T_bd23c_row2_col1\" class=\"data row2 col1\" >0.704598</td>\n",
       "      <td id=\"T_bd23c_row2_col2\" class=\"data row2 col2\" >0.697699</td>\n",
       "      <td id=\"T_bd23c_row2_col3\" class=\"data row2 col3\" >0.691701</td>\n",
       "      <td id=\"T_bd23c_row2_col4\" class=\"data row2 col4\" >0.693555</td>\n",
       "      <td id=\"T_bd23c_row2_col5\" class=\"data row2 col5\" >0.635384</td>\n",
       "      <td id=\"T_bd23c_row2_col6\" class=\"data row2 col6\" >0.751727</td>\n",
       "      <td id=\"T_bd23c_row2_col7\" class=\"data row2 col7\" >1653</td>\n",
       "      <td id=\"T_bd23c_row2_col8\" class=\"data row2 col8\" >2240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd23c_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_bd23c_row3_col0\" class=\"data row3 col0\" >SE2019_fasttext_bilstm</td>\n",
       "      <td id=\"T_bd23c_row3_col1\" class=\"data row3 col1\" >0.711790</td>\n",
       "      <td id=\"T_bd23c_row3_col2\" class=\"data row3 col2\" >0.705390</td>\n",
       "      <td id=\"T_bd23c_row3_col3\" class=\"data row3 col3\" >0.706433</td>\n",
       "      <td id=\"T_bd23c_row3_col4\" class=\"data row3 col4\" >0.705855</td>\n",
       "      <td id=\"T_bd23c_row3_col5\" class=\"data row3 col5\" >0.664072</td>\n",
       "      <td id=\"T_bd23c_row3_col6\" class=\"data row3 col6\" >0.747638</td>\n",
       "      <td id=\"T_bd23c_row3_col7\" class=\"data row3 col7\" >1653</td>\n",
       "      <td id=\"T_bd23c_row3_col8\" class=\"data row3 col8\" >2240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd23c_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_bd23c_row4_col0\" class=\"data row4 col0\" >SE2019_fasttext_gru</td>\n",
       "      <td id=\"T_bd23c_row4_col1\" class=\"data row4 col1\" >0.708965</td>\n",
       "      <td id=\"T_bd23c_row4_col2\" class=\"data row4 col2\" >0.702250</td>\n",
       "      <td id=\"T_bd23c_row4_col3\" class=\"data row4 col3\" >0.702471</td>\n",
       "      <td id=\"T_bd23c_row4_col4\" class=\"data row4 col4\" >0.702358</td>\n",
       "      <td id=\"T_bd23c_row4_col5\" class=\"data row4 col5\" >0.658014</td>\n",
       "      <td id=\"T_bd23c_row4_col6\" class=\"data row4 col6\" >0.746702</td>\n",
       "      <td id=\"T_bd23c_row4_col7\" class=\"data row4 col7\" >1653</td>\n",
       "      <td id=\"T_bd23c_row4_col8\" class=\"data row4 col8\" >2240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x19dc079f490>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = 'SE2019'\n",
    "em = 'fasttext'\n",
    "get_result_nn([ds+\"_\"+em+\"_cnn\", ds+\"_\"+em+\"_rnn\", ds+\"_\"+em+\"_lstm\",ds+\"_\"+em+\"_bilstm\",ds+\"_\"+em+\"_gru\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_3f5ec_row1_col0, #T_3f5ec_row4_col1, #T_3f5ec_row4_col2, #T_3f5ec_row4_col3, #T_3f5ec_row4_col4 {\n",
       "  background-color: red;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_3f5ec\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_3f5ec_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_3f5ec_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_3f5ec_level0_col2\" class=\"col_heading level0 col2\" >Precision</th>\n",
       "      <th id=\"T_3f5ec_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_3f5ec_level0_col4\" class=\"col_heading level0 col4\" >F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3f5ec_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_3f5ec_row0_col0\" class=\"data row0 col0\" >SE2019_glove_cnn</td>\n",
       "      <td id=\"T_3f5ec_row0_col1\" class=\"data row0 col1\" >0.709989</td>\n",
       "      <td id=\"T_3f5ec_row0_col2\" class=\"data row0 col2\" >0.707252</td>\n",
       "      <td id=\"T_3f5ec_row0_col3\" class=\"data row0 col3\" >0.709989</td>\n",
       "      <td id=\"T_3f5ec_row0_col4\" class=\"data row0 col4\" >0.707052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f5ec_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_3f5ec_row1_col0\" class=\"data row1 col0\" >SE2019_glove_rnn</td>\n",
       "      <td id=\"T_3f5ec_row1_col1\" class=\"data row1 col1\" >0.661344</td>\n",
       "      <td id=\"T_3f5ec_row1_col2\" class=\"data row1 col2\" >0.661685</td>\n",
       "      <td id=\"T_3f5ec_row1_col3\" class=\"data row1 col3\" >0.661344</td>\n",
       "      <td id=\"T_3f5ec_row1_col4\" class=\"data row1 col4\" >0.661509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f5ec_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_3f5ec_row2_col0\" class=\"data row2 col0\" >SE2019_glove_lstm</td>\n",
       "      <td id=\"T_3f5ec_row2_col1\" class=\"data row2 col1\" >0.732269</td>\n",
       "      <td id=\"T_3f5ec_row2_col2\" class=\"data row2 col2\" >0.746559</td>\n",
       "      <td id=\"T_3f5ec_row2_col3\" class=\"data row2 col3\" >0.732269</td>\n",
       "      <td id=\"T_3f5ec_row2_col4\" class=\"data row2 col4\" >0.734036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f5ec_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_3f5ec_row3_col0\" class=\"data row3 col0\" >SE2019_glove_bilstm</td>\n",
       "      <td id=\"T_3f5ec_row3_col1\" class=\"data row3 col1\" >0.726328</td>\n",
       "      <td id=\"T_3f5ec_row3_col2\" class=\"data row3 col2\" >0.730623</td>\n",
       "      <td id=\"T_3f5ec_row3_col3\" class=\"data row3 col3\" >0.726328</td>\n",
       "      <td id=\"T_3f5ec_row3_col4\" class=\"data row3 col4\" >0.727574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f5ec_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_3f5ec_row4_col0\" class=\"data row4 col0\" >SE2019_glove_gru</td>\n",
       "      <td id=\"T_3f5ec_row4_col1\" class=\"data row4 col1\" >0.756034</td>\n",
       "      <td id=\"T_3f5ec_row4_col2\" class=\"data row4 col2\" >0.754855</td>\n",
       "      <td id=\"T_3f5ec_row4_col3\" class=\"data row4 col3\" >0.756034</td>\n",
       "      <td id=\"T_3f5ec_row4_col4\" class=\"data row4 col4\" >0.755169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x23b5dc99bd0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = 'SE2019'\n",
    "em = 'glove'\n",
    "get_result_nn([ds+\"_\"+em+\"_cnn\", ds+\"_\"+em+\"_rnn\", ds+\"_\"+em+\"_lstm\",ds+\"_\"+em+\"_bilstm\",ds+\"_\"+em+\"_gru\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_52f0a_row3_col1, #T_52f0a_row3_col2, #T_52f0a_row3_col3, #T_52f0a_row3_col4, #T_52f0a_row5_col0 {\n",
       "  background-color: red;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_52f0a\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_52f0a_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_52f0a_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_52f0a_level0_col2\" class=\"col_heading level0 col2\" >Precision</th>\n",
       "      <th id=\"T_52f0a_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_52f0a_level0_col4\" class=\"col_heading level0 col4\" >F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_52f0a_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_52f0a_row0_col0\" class=\"data row0 col0\" >dtc</td>\n",
       "      <td id=\"T_52f0a_row0_col1\" class=\"data row0 col1\" >0.749350</td>\n",
       "      <td id=\"T_52f0a_row0_col2\" class=\"data row0 col2\" >0.747932</td>\n",
       "      <td id=\"T_52f0a_row0_col3\" class=\"data row0 col3\" >0.749350</td>\n",
       "      <td id=\"T_52f0a_row0_col4\" class=\"data row0 col4\" >0.748249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_52f0a_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_52f0a_row1_col0\" class=\"data row1 col0\" >dtc-tfid</td>\n",
       "      <td id=\"T_52f0a_row1_col1\" class=\"data row1 col1\" >0.743780</td>\n",
       "      <td id=\"T_52f0a_row1_col2\" class=\"data row1 col2\" >0.743531</td>\n",
       "      <td id=\"T_52f0a_row1_col3\" class=\"data row1 col3\" >0.743780</td>\n",
       "      <td id=\"T_52f0a_row1_col4\" class=\"data row1 col4\" >0.743649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_52f0a_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_52f0a_row2_col0\" class=\"data row2 col0\" >dtc-w2v</td>\n",
       "      <td id=\"T_52f0a_row2_col1\" class=\"data row2 col1\" >0.586706</td>\n",
       "      <td id=\"T_52f0a_row2_col2\" class=\"data row2 col2\" >0.589898</td>\n",
       "      <td id=\"T_52f0a_row2_col3\" class=\"data row2 col3\" >0.586706</td>\n",
       "      <td id=\"T_52f0a_row2_col4\" class=\"data row2 col4\" >0.588024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_52f0a_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_52f0a_row3_col0\" class=\"data row3 col0\" >svm</td>\n",
       "      <td id=\"T_52f0a_row3_col1\" class=\"data row3 col1\" >0.768659</td>\n",
       "      <td id=\"T_52f0a_row3_col2\" class=\"data row3 col2\" >0.767394</td>\n",
       "      <td id=\"T_52f0a_row3_col3\" class=\"data row3 col3\" >0.768659</td>\n",
       "      <td id=\"T_52f0a_row3_col4\" class=\"data row3 col4\" >0.766195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_52f0a_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_52f0a_row4_col0\" class=\"data row4 col0\" >svm-tfid</td>\n",
       "      <td id=\"T_52f0a_row4_col1\" class=\"data row4 col1\" >0.746008</td>\n",
       "      <td id=\"T_52f0a_row4_col2\" class=\"data row4 col2\" >0.760112</td>\n",
       "      <td id=\"T_52f0a_row4_col3\" class=\"data row4 col3\" >0.746008</td>\n",
       "      <td id=\"T_52f0a_row4_col4\" class=\"data row4 col4\" >0.731861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_52f0a_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_52f0a_row5_col0\" class=\"data row5 col0\" >svm-w2v</td>\n",
       "      <td id=\"T_52f0a_row5_col1\" class=\"data row5 col1\" >0.590048</td>\n",
       "      <td id=\"T_52f0a_row5_col2\" class=\"data row5 col2\" >0.611645</td>\n",
       "      <td id=\"T_52f0a_row5_col3\" class=\"data row5 col3\" >0.590048</td>\n",
       "      <td id=\"T_52f0a_row5_col4\" class=\"data row5 col4\" >0.461541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_52f0a_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_52f0a_row6_col0\" class=\"data row6 col0\" >lr</td>\n",
       "      <td id=\"T_52f0a_row6_col1\" class=\"data row6 col1\" >0.684738</td>\n",
       "      <td id=\"T_52f0a_row6_col2\" class=\"data row6 col2\" >0.690064</td>\n",
       "      <td id=\"T_52f0a_row6_col3\" class=\"data row6 col3\" >0.684738</td>\n",
       "      <td id=\"T_52f0a_row6_col4\" class=\"data row6 col4\" >0.686326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_52f0a_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_52f0a_row7_col0\" class=\"data row7 col0\" >lr-tfid</td>\n",
       "      <td id=\"T_52f0a_row7_col1\" class=\"data row7 col1\" >0.694393</td>\n",
       "      <td id=\"T_52f0a_row7_col2\" class=\"data row7 col2\" >0.697312</td>\n",
       "      <td id=\"T_52f0a_row7_col3\" class=\"data row7 col3\" >0.694393</td>\n",
       "      <td id=\"T_52f0a_row7_col4\" class=\"data row7 col4\" >0.695449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_52f0a_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_52f0a_row8_col0\" class=\"data row8 col0\" >lr-w2v</td>\n",
       "      <td id=\"T_52f0a_row8_col1\" class=\"data row8 col1\" >0.708504</td>\n",
       "      <td id=\"T_52f0a_row8_col2\" class=\"data row8 col2\" >0.705758</td>\n",
       "      <td id=\"T_52f0a_row8_col3\" class=\"data row8 col3\" >0.708504</td>\n",
       "      <td id=\"T_52f0a_row8_col4\" class=\"data row8 col4\" >0.705809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1f954ed6450>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_result()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
