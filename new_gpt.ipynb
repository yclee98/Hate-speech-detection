{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result_table():\n",
    "    c = ['Model', 'Accuracy', 'precision', 'recall', 'f1-score', 'hate f1', \"non-hate f1\", 'hate support', 'non-hate support']\n",
    "    result_table = pd.DataFrame(columns=c)\n",
    "    return result_table\n",
    "\n",
    "def get_classification_report(i, cr):\n",
    "    return [i, cr['accuracy'], cr['macro avg']['precision'], \n",
    "            cr['macro avg']['recall'], cr['macro avg']['f1-score'],\n",
    "            cr['Hate']['f1-score'],cr['Non-Hate']['f1-score'], \n",
    "            cr['Hate']['support'],cr['Non-Hate']['support']]\n",
    "\n",
    "def get_result_single(y_test, y_test_pred, model_name, result_table):\n",
    "    cr = classification_report(y_test, y_test_pred, labels=[\"Hate\",\"Non-Hate\"], output_dict=True)\n",
    "    result_table.loc[len(result_table)] = get_classification_report(model_name, cr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://platform.openai.com/docs/api-reference/fine-tunes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset for gpt\n",
    "df_gpt = pd.DataFrame(zip(x_train,y_train_binary), columns = ['prompt', 'completion'])\n",
    "df_gpt.to_json(f\"Dataset/{dataset_name}/gpt_data_train.jsonl\", orient='records', lines=True)\n",
    "print(len(df_gpt))\n",
    "\n",
    "df_gpt = pd.DataFrame(zip(x_test,y_test_binary), columns = ['prompt', 'completion'])\n",
    "df_gpt.to_json(f\"Dataset/{dataset_name}/gpt_data_test.jsonl\", orient='records', lines=True)\n",
    "print(len(df_gpt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset for fine tune do in cmd\n",
    "print(f\"openai tools fine_tunes.prepare_data -f Dataset/{dataset_name}/gpt_data_train.jsonl\")\n",
    "print(f\"openai tools fine_tunes.prepare_data -f Dataset/{dataset_name}/gpt_data_test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload file to openai and create fine tune model\n",
    "train_create_output = openai.File.create(\n",
    "  file=open(f\"Dataset/{dataset_name}/gpt_data_train_prepared.jsonl\", \"rb\"),\n",
    "  purpose='fine-tune'\n",
    ")\n",
    "file_train_id = train_create_output.get('id')\n",
    "print(file_train_id, train_create_output.get('status'))\n",
    "\n",
    "test_create_output = openai.File.create(\n",
    "  file=open(f\"Dataset/{dataset_name}/gpt_data_test_prepared.jsonl\", \"rb\"),\n",
    "  purpose='fine-tune'\n",
    ")\n",
    "file_test_id = test_create_output.get('id')\n",
    "print(file_test_id, test_create_output.get('status'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the fine tune job\n",
    "fine_tune_create_output = openai.FineTune.create(training_file =file_train_id,\n",
    "                       validation_file=file_test_id,\n",
    "                       model = \"ada\",\n",
    "                       compute_classification_metrics = True,\n",
    "                       classification_positive_class = \" 0\"\n",
    "                       )\n",
    "fine_tune_id = fine_tune_create_output.get('id')\n",
    "print(fine_tune_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune list\n",
    "all_finetune = openai.FineTune.list()\n",
    "all_finetune_data = all_finetune.get('data')\n",
    "for i in range(len(all_finetune_data)):\n",
    "    print(all_finetune_data[i].get('id'), all_finetune_data[i].get('status'), all_finetune_data[i].get('fine_tuned_model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model list\n",
    "all_models = openai.Model.list()\n",
    "all_models_data = all_models.get('data')\n",
    "owned_by_list = ['openai','openai-dev', 'openai-internal']\n",
    "for i in range(len(all_models_data)):\n",
    "    if all_models_data[i].get('owned_by') not in owned_by_list:\n",
    "        print(all_models_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_id = \"ft-j51edHpwX7ZfLBe3GRrXKnDT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_output = openai.FineTune.retrieve(id=fine_tune_id)\n",
    "retrieve_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if retrieve_output.get(\"status\") == \"succeeded\":\n",
    "    model_id = retrieve_output.get('fine_tuned_model')\n",
    "    print(\"succeeded\", model_id)\n",
    "else:\n",
    "    print(retrieve_output.get(\"status\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.FineTune.list_events(id=fine_tune_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT Model result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get result of model in cmd\n",
    "# download result to result.csv\n",
    "print(f\"openai api fine_tunes.results -i {fine_tune_id} > result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result view\n",
    "results = pd.read_csv('result.csv')\n",
    "results[results['classification/accuracy'].notnull()].tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"Implicit_hate_corpus\"\n",
    "model_id = \"ada:ft-personal-2023-08-04-20-03-44\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_result = None\n",
    "\n",
    "def gpt_complete_create(prompt_text):\n",
    "    #model_id = \"ada:ft-personal-2023-06-26-17-27-28\" \n",
    "    result_gpt = openai.Completion.create(model=model_id, prompt=prompt_text, max_tokens=1, temperature=0)\n",
    "    return result_gpt\n",
    "\n",
    "def gpt_predict(start = 0, step = 10, max_s=10):\n",
    "    prompts_tosend = []\n",
    "    for i in range(start, start+step):\n",
    "        if i == max_s: break\n",
    "        p = df_gpt.loc[i]['prompt']\n",
    "        prompts_tosend.append(p)\n",
    "    \n",
    "    predict_result = gpt_complete_create(prompts_tosend)\n",
    "    choices_gpt = predict_result.get('choices')\n",
    "    \n",
    "    for i in range(len(choices_gpt)):\n",
    "        j = choices_gpt[i]['index']\n",
    "        df_gpt.loc[start+j, 'predicted'] = int(choices_gpt[i]['text']) \n",
    "\n",
    "    print(f\"predicted {start} to {start+len(prompts_tosend)-1}\")\n",
    "\n",
    "    return prompts_tosend, predict_result\n",
    "\n",
    "def loop_gpt(start, end, step, max_s):\n",
    "    for i in range(start, end, step):\n",
    "        bb, cc = gpt_predict(i, step, max_s)\n",
    "        time.sleep(50)\n",
    "    return bb, cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = f\"Dataset/{dataset_name}/gpt_data_test_prepared.jsonl\"\n",
    "df_gpt = pd.read_json(filepath, lines=True)\n",
    "df_gpt['predicted'] = -1\n",
    "df_gpt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_s = len(df_gpt)\n",
    "max_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    p, r = loop_gpt(600, max_s, 500, max_s)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View result for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gpt_result(dataset_name):\n",
    "    df_gpt = pd.read_json(f\"Dataset/{dataset_name}/gpt_data_test_result1.jsonl\", orient='records', lines=True)\n",
    "    return df_gpt\n",
    "\n",
    "def get_y_result(df_gpt):\n",
    "    y_test = df_gpt['completion'].to_numpy()\n",
    "    y_test_pred = df_gpt['predicted'].to_numpy()\n",
    "\n",
    "    y_test = np.where(y_test == 1, \"Hate\", \"Non-Hate\") \n",
    "    y_test_pred = np.where(y_test_pred == 1, \"Hate\", \"Non-Hate\") \n",
    "    return y_test, y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"Balanced\"\n",
    "df_gpt = pd.read_json(f\"Dataset/{dataset_name}/gpt_data_test_result1.jsonl\", orient='records', lines=True)\n",
    "df_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_result(df_gpt):\n",
    "    y_test = df_gpt['completion'].to_numpy()\n",
    "    y_test_pred = df_gpt['predicted'].to_numpy()\n",
    "\n",
    "    y_test = np.where(y_test == 1, \"Hate\", \"Non-Hate\") \n",
    "    y_test_pred = np.where(y_test_pred == 1, \"Hate\", \"Non-Hate\") \n",
    "    return y_test, y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = get_result_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpt = load_gpt_result(\"Balanced\")\n",
    "y_test, y_test_pred = get_y_result(df_gpt)\n",
    "get_result_single(y_test, y_test_pred, \"Balanced_GPT\", df_result)\n",
    "\n",
    "df_gpt = load_gpt_result(\"GabHateCorpus\")\n",
    "y_test, y_test_pred = get_y_result(df_gpt)\n",
    "get_result_single(y_test, y_test_pred, \"GabHateCorpus_GPT\", df_result)\n",
    "\n",
    "df_gpt = load_gpt_result(\"Implicit_hate_corpus\")\n",
    "y_test, y_test_pred = get_y_result(df_gpt)\n",
    "get_result_single(y_test, y_test_pred, \"Implicit_hate_corpus_GPT\", df_result)\n",
    "\n",
    "df_gpt = load_gpt_result(\"SE2019\")\n",
    "y_test, y_test_pred = get_y_result(df_gpt)\n",
    "get_result_single(y_test, y_test_pred, \"SE2019\", df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>hate f1</th>\n",
       "      <th>non-hate f1</th>\n",
       "      <th>hate support</th>\n",
       "      <th>non-hate support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Balanced_GPT</td>\n",
       "      <td>0.768033</td>\n",
       "      <td>0.768028</td>\n",
       "      <td>0.768025</td>\n",
       "      <td>0.768026</td>\n",
       "      <td>0.769253</td>\n",
       "      <td>0.766800</td>\n",
       "      <td>3420.0</td>\n",
       "      <td>3387.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GabHateCorpus_GPT</td>\n",
       "      <td>0.894815</td>\n",
       "      <td>0.748450</td>\n",
       "      <td>0.716789</td>\n",
       "      <td>0.730999</td>\n",
       "      <td>0.521079</td>\n",
       "      <td>0.940919</td>\n",
       "      <td>639.0</td>\n",
       "      <td>4761.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Implicit_hate_corpus_GPT</td>\n",
       "      <td>0.806331</td>\n",
       "      <td>0.797510</td>\n",
       "      <td>0.784396</td>\n",
       "      <td>0.789644</td>\n",
       "      <td>0.730395</td>\n",
       "      <td>0.848892</td>\n",
       "      <td>1622.0</td>\n",
       "      <td>2674.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SE2019</td>\n",
       "      <td>0.814333</td>\n",
       "      <td>0.809284</td>\n",
       "      <td>0.809436</td>\n",
       "      <td>0.809359</td>\n",
       "      <td>0.778565</td>\n",
       "      <td>0.840153</td>\n",
       "      <td>1128.0</td>\n",
       "      <td>1565.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Accuracy  precision    recall  f1-score  \\\n",
       "0              Balanced_GPT  0.768033   0.768028  0.768025  0.768026   \n",
       "1         GabHateCorpus_GPT  0.894815   0.748450  0.716789  0.730999   \n",
       "2  Implicit_hate_corpus_GPT  0.806331   0.797510  0.784396  0.789644   \n",
       "3                    SE2019  0.814333   0.809284  0.809436  0.809359   \n",
       "\n",
       "    hate f1  non-hate f1  hate support  non-hate support  \n",
       "0  0.769253     0.766800        3420.0            3387.0  \n",
       "1  0.521079     0.940919         639.0            4761.0  \n",
       "2  0.730395     0.848892        1622.0            2674.0  \n",
       "3  0.778565     0.840153        1128.0            1565.0  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
