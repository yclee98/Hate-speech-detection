{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle \n",
    "\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    counter = 0\n",
    "    for i in text:\n",
    "        counter += len(i.split())\n",
    "    return counter\n",
    "\n",
    "def count_token(text):\n",
    "    s = set()\n",
    "    for i in text:\n",
    "        tokenize = i.split()\n",
    "        for j in tokenize:\n",
    "            s.add(j)    \n",
    "    return len(s)\n",
    "\n",
    "\n",
    "def load_dataset(ds, opt=2):\n",
    "    if ds == 1:\n",
    "        dataset_name = \"GabHateCorpus\"\n",
    "    elif ds == 2:\n",
    "        dataset_name = \"Implicit_hate_corpus\"\n",
    "    elif ds == 3:\n",
    "        dataset_name = \"SE2019\"\n",
    "    else:\n",
    "        dataset_name = \"Balanced\"\n",
    "\n",
    "    filepath = \"Dataset/\"+dataset_name\n",
    "    if opt==1:\n",
    "        df = pd.read_csv(filepath+\"/data_processed.csv\")\n",
    "    elif opt==2:\n",
    "        df = pd.read_csv(filepath+\"/data_processed2.csv\")\n",
    "    else:\n",
    "        df = pd.read_csv(filepath+\"/data_needed.csv\")\n",
    "    print(df['class'].value_counts(normalize=True))\n",
    "    return df, dataset_name\n",
    "\n",
    "def split_data(df):\n",
    "    test_size = 0.20\n",
    "    x = np.array(df[\"text\"])\n",
    "    y = np.array(df[\"class\"])\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = test_size, random_state=42) #random state ensure same sample\n",
    "    print(\"Train Set :\", x_train.shape, y_train.shape) \n",
    "    print(\"Test Set  :\", x_test.shape, y_test.shape) \n",
    "    print(\"Total \", len(df))\n",
    "    # y in digit form\n",
    "    y_train_binary = np.array(list(map(lambda x:1 if x==\"Hate\" else 0, y_train)))\n",
    "    y_test_binary = np.array(list(map(lambda x:1 if x==\"Hate\" else 0, y_test)))\n",
    "    return x_train, y_train, y_train_binary, x_test, y_test, y_test_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.utils.data_utils import pad_sequences\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, GlobalMaxPooling1D, Embedding\n",
    "from keras.layers import Conv1D, LSTM, SpatialDropout1D, Bidirectional, GRU, SimpleRNN, TextVectorization\n",
    "\n",
    "from keras.metrics import BinaryAccuracy,Precision,Recall\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "\n",
    "from gensim.models import FastText, Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimingCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, logs={}):\n",
    "        super(TimingCallback, self).__init__()\n",
    "\n",
    "    def on_train_begin(self, epoch, logs={}):\n",
    "        self.starttime = time.time()\n",
    "    def on_train_end(self, epoch, logs={}):\n",
    "        self.stoptime = time.time()\n",
    "        print(f\"training time {self.stoptime - self.starttime}\")\n",
    "        \n",
    "METRICS = [\n",
    "    BinaryAccuracy(name=\"accuracy\"),\n",
    "    Precision(name=\"precision\"),\n",
    "    Recall(name=\"recall\")\n",
    "]\n",
    "\n",
    "def get_classification_report(i, cr):\n",
    "    return [i, cr['accuracy'], cr['macro avg']['precision'], \n",
    "            cr['macro avg']['recall'], cr['macro avg']['f1-score'],\n",
    "            cr['Hate']['f1-score'],cr['Non-Hate']['f1-score'], \n",
    "            cr['Hate']['support'],cr['Non-Hate']['support']]\n",
    "\n",
    "def get_result_table():\n",
    "    c = ['Model', 'Accuracy', 'precision', 'recall', 'f1-score', 'hate f1', \"non-hate f1\", 'hate support', 'non-hate support']\n",
    "    result_table = pd.DataFrame(columns=c)\n",
    "    return result_table\n",
    "\n",
    "def get_result_single(y_test, y_test_pred, model_name, result_table):\n",
    "    # c = ['Model', 'Accuracy', 'precision', 'recall', 'f1-score', 'hate f1', \"non-hate f1\", 'hate support', 'non-hate support']\n",
    "    # result_table = pd.DataFrame(columns=c)\n",
    "\n",
    "    cr = classification_report(y_test, y_test_pred, labels=[\"Hate\",\"Non-Hate\"], output_dict=True)\n",
    "    result_table.loc[len(result_table)] = get_classification_report(model_name, cr)\n",
    "    # return result_table\n",
    "\n",
    "def get_result_multiple(x_test, y_test, model_to_load):\n",
    "    c = ['Model', 'Accuracy', 'precision', 'recall', 'f1-score', 'hate f1', \"non-hate f1\", 'hate support', 'non-hate support']\n",
    "    result_table = pd.DataFrame(columns=c)\n",
    "    for i in model_to_load:\n",
    "        filename = f\"models/{i}\"\n",
    "        print(filename)\n",
    "        old_model = load_model(filename)\n",
    "\n",
    "        y_test_pred = old_model.predict(x_test, verbose=0)\n",
    "        y_test_pred = np.where(y_test_pred > 0.5, \"Hate\", \"Non-Hate\") \n",
    "        y_test_pred = y_test_pred.flatten()\n",
    "\n",
    "        cr = classification_report(y_test, y_test_pred, labels=[\"Hate\",\"Non-Hate\"], output_dict=True)\n",
    "        result_table.loc[len(result_table)] = get_classification_report(i, cr)\n",
    "    return result_table.style.highlight_max(color = 'red', axis = 0)\n",
    "\n",
    "def nn_predict(model,x_test, y_test_binary):\n",
    "    score = model.evaluate(x_test, y_test_binary, verbose=0)\n",
    "    print(\"Score: \", score[0])\n",
    "    print(\"Accuracy: \", score[1])\n",
    "\n",
    "    y_test_pred_percent = model.predict(x_test, verbose=0)\n",
    "    y_test_pred = np.where(y_test_pred_percent > 0.5, \"Hate\", \"Non-Hate\") \n",
    "    y_test_pred = y_test_pred.flatten()\n",
    "\n",
    "    return y_test_pred\n",
    "\n",
    "def save_model_nn(model, model_name, embedding_name, dataset_name):\n",
    "    filename = f\"models/{dataset_name}_{embedding_name}_{model_name}\"\n",
    "    model.save(filename)\n",
    "    return filename\n",
    "\n",
    "def load_model_nn(model_name):\n",
    "    filename = f\"models/{model_name}\"\n",
    "    print(filename)\n",
    "    return load_model(filename) \n",
    "\n",
    "def compile_fit_save(x_train, y_train_binary, x_test,y_test_binary, model, model_name, embedding_name, dataset_name, save, epoch=5, batch_size=32, lr=0.01):    \n",
    "    opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=opt,\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=METRICS)\n",
    "    \n",
    "    history = model.fit(x_train, y_train_binary, epochs=epoch,\n",
    "                        validation_data=(x_test,y_test_binary),\n",
    "                        batch_size = batch_size,\n",
    "                        callbacks=[TimingCallback()])\n",
    "\n",
    "    if save: \n",
    "        save_model_nn(model, model_name, embedding_name, dataset_name)        \n",
    "    print(f\"acc {history.history['val_accuracy'][0]}\")\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_em(x_train):\n",
    "    embedding_name = \"glove\"\n",
    "    text_length = 50 #pad/truncate text to this long, such that each text after token will be this long\n",
    "\n",
    "    custom_encoder = TextVectorization(\n",
    "        standardize = None,\n",
    "        output_sequence_length=text_length, \n",
    "    )\n",
    "    custom_encoder.adapt(x_train)\n",
    "    vocab = custom_encoder.get_vocabulary()\n",
    "    print(f\"total vocab {len(vocab)}\")\n",
    "    vocab_dict = dict(zip(vocab, range(len(vocab))))\n",
    "\n",
    "    # load glove to dictionay\n",
    "    embeddings_dic = dict()\n",
    "    glove_file = open(\"Dataset/trained/glove.42B.300d.txt\", encoding=\"utf8\")\n",
    "\n",
    "    for line in glove_file:\n",
    "        records = line.split()\n",
    "        word = records[0]\n",
    "        vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "        embeddings_dic[word] = vector_dimensions\n",
    "    glove_file.close()\n",
    "    print(\"Total words \", len(embeddings_dic))\n",
    "\n",
    "    # create vocab length is the size of token in dictionary\n",
    "    # Size of the vocabulary\n",
    "    vocab_length = len(vocab) + 1\n",
    "    embedding_dim = 300 #each glove word is 100 long\n",
    "\n",
    "    hits = 0\n",
    "    miss = 0\n",
    "    missWord = []\n",
    "\n",
    "    # create embedding matrix having 100 col\n",
    "    # for all vocab word we give it a vector value from glove\n",
    "    # for those not found in glove will be empty 0\n",
    "    # size of embedding_matriz = size of word_tokenizer.word_index.items()\n",
    "    # embedding_matrix is the weight \n",
    "    embedding_matrix = np.zeros((vocab_length, embedding_dim))\n",
    "    for word, index in vocab_dict.items():\n",
    "        embedding_vector = embeddings_dic.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            miss +=1\n",
    "            missWord.append(word)\n",
    "    print(\"Converted %d words (%d misses)\" % (hits, miss))\n",
    "\n",
    "    custom_embedding = Embedding(vocab_length, embedding_dim, \n",
    "                embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "                trainable = False,\n",
    "                input_length=text_length,\n",
    "                mask_zero=True)\n",
    "    \n",
    "    return custom_encoder, custom_embedding, embedding_name, missWord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastText, Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fasttext_model():\n",
    "    model_name=\"fasttext_trained\"\n",
    "    return KeyedVectors.load_word2vec_format(\"./Dataset/trained/wiki-news-300d-1M-subword.vec\", binary=False), model_name\n",
    "\n",
    "def get_word2vec_model():\n",
    "    model_name = \"word2vec_trained\"\n",
    "    return KeyedVectors.load_word2vec_format(\"./Dataset/trained/GoogleNews-vectors-negative300.bin\", binary=True), model_name\n",
    "\n",
    "def pre_trained_em(x_train, model_em, embedding_name):\n",
    "    text_length = 50 #pad/truncate text to this long, such that each text after token will be this long\n",
    "\n",
    "    custom_encoder = TextVectorization(\n",
    "        standardize = None,\n",
    "        output_sequence_length=text_length, \n",
    "    )\n",
    "    custom_encoder.adapt(x_train)\n",
    "    vocab = custom_encoder.get_vocabulary()\n",
    "    print(f\"total vocab {len(vocab)}\")\n",
    "    vocab_dict = dict(zip(vocab, range(len(vocab))))\n",
    "\n",
    "    vocab_length = len(vocab) + 1\n",
    "    embedding_dim = 300 \n",
    "\n",
    "    hits = 0\n",
    "    miss = 0\n",
    "    missWord = []\n",
    "\n",
    "    embedding_matrix = np.zeros((vocab_length, embedding_dim))\n",
    "    keyVector_key = model_em.index_to_key\n",
    "    print(f\"total vector {len(keyVector_key)}\")\n",
    "    for word, index in vocab_dict.items():\n",
    "        if word in keyVector_key:\n",
    "            embedding_vector = np.array(model_em[word])\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[index] = embedding_vector\n",
    "                hits += 1\n",
    "        else:\n",
    "            miss +=1\n",
    "            missWord.append(word)\n",
    "            \n",
    "    print(\"Converted %d words (%d misses)\" % (hits, miss))\n",
    "\n",
    "    custom_embedding = Embedding(vocab_length, embedding_dim, \n",
    "                embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "                trainable = False,\n",
    "                input_length=text_length,\n",
    "                mask_zero=True)\n",
    "    \n",
    "    return custom_encoder, custom_embedding, embedding_name, missWord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No pre-trained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noTrained_em(x_train):\n",
    "    embedding_name = \"no_train\"\n",
    "    text_length = 50 #pad/truncate text to this long, such that each text after token will be this long\n",
    "    vector_size= 300\n",
    "\n",
    "    custom_encoder = TextVectorization(\n",
    "        standardize = None,\n",
    "        output_sequence_length=text_length, \n",
    "    )\n",
    "    custom_encoder.adapt(x_train)\n",
    "    vocab = custom_encoder.get_vocabulary()\n",
    "    print(f\"total vocab {len(vocab)}\")\n",
    "    vocab_dict = dict(zip(vocab, range(len(vocab))))\n",
    "\n",
    "    vocab_length = len(vocab) + 1\n",
    "    embedding_dim = vector_size\n",
    "\n",
    "    custom_embedding = Embedding(vocab_length, embedding_dim,\n",
    "                input_length=text_length,\n",
    "                mask_zero=True)\n",
    "    return custom_encoder, custom_embedding, embedding_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization\n",
    "def add_connected_layer(model):\n",
    "    # model.add(Dropout(0.2))\n",
    "    # model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(x_train, y_train_binary, x_test,y_test_binary,custom_encoder, custom_embedding, embedding_name, dataset_name, save = True, epoch = 10, batch_size=32, lr=0.01):\n",
    "    model_name = \"cnn\"\n",
    "    print(model_name)\n",
    "    model = Sequential()\n",
    "    model.add(custom_encoder)\n",
    "    model.add(custom_embedding)\n",
    "    model.add(Conv1D(128, 3, activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    add_connected_layer(model)\n",
    "    return compile_fit_save(x_train, y_train_binary, x_test,y_test_binary,model, model_name, embedding_name, dataset_name, save, epoch, batch_size, lr)\n",
    "\n",
    "def rnn(x_train, y_train_binary, x_test,y_test_binary,custom_encoder, custom_embedding, embedding_name, dataset_name, save = True, epoch = 10, batch_size=32, lr=0.01):\n",
    "    model_name = \"rnn\"\n",
    "    print(model_name)\n",
    "    model = Sequential()\n",
    "    model.add(custom_encoder)\n",
    "    model.add(custom_embedding)\n",
    "    model.add(SimpleRNN(128))\n",
    "    add_connected_layer(model)\n",
    "    return compile_fit_save(x_train, y_train_binary, x_test,y_test_binary,model, model_name, embedding_name, dataset_name, save, epoch, batch_size, lr)\n",
    "\n",
    "def lstm(x_train, y_train_binary, x_test,y_test_binary,custom_encoder, custom_embedding, embedding_name, dataset_name, save = True, epoch = 10, batch_size=32, lr=0.01):\n",
    "    model_name = \"lstm\"\n",
    "    print(model_name)\n",
    "    model = Sequential()\n",
    "    model.add(custom_encoder)\n",
    "    model.add(custom_embedding)\n",
    "    # model.add(SpatialDropout1D(0.2))\n",
    "    model.add(LSTM(128))\n",
    "    add_connected_layer(model)\n",
    "    return compile_fit_save(x_train, y_train_binary, x_test,y_test_binary,model, model_name, embedding_name, dataset_name, save, epoch, batch_size, lr)\n",
    "\n",
    "def gru(x_train, y_train_binary, x_test,y_test_binary,custom_encoder, custom_embedding, embedding_name, dataset_name, save = True, epoch = 10, batch_size=32, lr=0.01):\n",
    "    model_name = \"gru\"\n",
    "    print(model_name)\n",
    "    model = Sequential()\n",
    "    model.add(custom_encoder)\n",
    "    model.add(custom_embedding)\n",
    "    # model.add(SpatialDropout1D(0.2))\n",
    "    model.add(GRU(128))\n",
    "    add_connected_layer(model)\n",
    "    return compile_fit_save(x_train, y_train_binary, x_test,y_test_binary,model, model_name, embedding_name, dataset_name, save, epoch, batch_size, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(model_history, model_name):\n",
    "    # Model performance charts\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "\n",
    "    plt.plot(model_history.history['accuracy'])\n",
    "    plt.plot(model_history.history['val_accuracy'])\n",
    "\n",
    "    plt.title(f'{model_name} model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.ylim(None, 1)\n",
    "    plt.subplot(1, 2, 2)\n",
    "\n",
    "    plt.plot(model_history.history['loss'])\n",
    "    plt.plot(model_history.history['val_loss'])\n",
    "\n",
    "    plt.title(f'{model_name} model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='lower right')\n",
    "    plt.ylim(0, None)\n",
    "    \n",
    "    \n",
    "    # plt.figure(figsize=(4,4))\n",
    "    # plt.plot(model_history.history['loss'])\n",
    "    # plt.plot(model_history.history['val_loss'])\n",
    "    \n",
    "    # plt.title(f'{model_name} model loss')\n",
    "    # plt.ylabel('loss')\n",
    "    # plt.xlabel('epoch')\n",
    "    # plt.legend(['train', 'test'], loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(y_test, y_test_pred):\n",
    "    # Heatmap\n",
    "    ax = plt.subplot()\n",
    "\n",
    "    # Plot the two-way Confusion Matrix\n",
    "    sb.heatmap(confusion_matrix(y_test, y_test_pred, labels=[\"Hate\",\"Non-Hate\"]), \n",
    "            annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax=ax)\n",
    "\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.xaxis.set_ticklabels([\"Hate\",\"Non-Hate\"])\n",
    "    ax.yaxis.set_ticklabels([\"Hate\",\"Non-Hate\"])\n",
    "\n",
    "    # Count\n",
    "    df1 = pd.DataFrame({'Actual':y_test, 'Predict':y_test_pred})\n",
    "    # print(df1.describe())\n",
    "    print(f\"Count: {df1['Actual'].value_counts()}\")\n",
    "    print()\n",
    "    print(f\"Count: {df1['Predict'].value_counts()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "Hate        0.500427\n",
      "Non-Hate    0.499573\n",
      "Name: proportion, dtype: float64\n",
      "Train Set : (27178,) (27178,)\n",
      "Test Set  : (6795,) (6795,)\n",
      "Total  33973\n",
      "Balanced\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' if you think im sweating about your petty ass think again bitch bitchimnotscared obviouslyyouare youthreatenedhoe '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df, dataset_name = load_dataset(4,2) \n",
    "x_train, y_train, y_train_binary, x_test, y_test, y_test_binary = split_data(df)\n",
    "df_result = get_result_table()\n",
    "print(dataset_name)\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_start_train(x_train, y_train_binary, x_test,y_test_binary,custom_encoder, custom_embedding, embedding_name, dataset_name):\n",
    "    model, h = cnn(x_train, y_train_binary, x_test,y_test_binary,custom_encoder, custom_embedding, embedding_name, dataset_name,save=False, epoch=8, batch_size=256, lr=0.001)\n",
    "    y_test_pred = nn_predict(model, x_test, y_test_binary)\n",
    "    get_result_single(y_test, y_test_pred, dataset_name+\"_\"+embedding_name+\"_cnn\", df_result)\n",
    "\n",
    "    model, h = rnn(x_train, y_train_binary, x_test,y_test_binary,custom_encoder, custom_embedding, embedding_name, dataset_name,save=False, epoch=8, batch_size=256, lr=0.001)\n",
    "    y_test_pred = nn_predict(model, x_test, y_test_binary)\n",
    "    get_result_single(y_test, y_test_pred, dataset_name+\"_\"+embedding_name+\"_rnn\", df_result)\n",
    "\n",
    "    model, h = lstm(x_train, y_train_binary, x_test,y_test_binary,custom_encoder, custom_embedding, embedding_name, dataset_name,save=False, epoch=8, batch_size=256, lr=0.001)\n",
    "    y_test_pred = nn_predict(model, x_test, y_test_binary)\n",
    "    get_result_single(y_test, y_test_pred, dataset_name+\"_\"+embedding_name+\"_lstm\", df_result)\n",
    "\n",
    "    model, h = gru(x_train, y_train_binary, x_test,y_test_binary,custom_encoder, custom_embedding, embedding_name, dataset_name,save=False, epoch=8, batch_size=256, lr=0.001)\n",
    "    y_test_pred = nn_predict(model, x_test, y_test_binary)\n",
    "    get_result_single(y_test, y_test_pred, dataset_name+\"_\"+embedding_name+\"_gru\", df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total vocab 33332\n",
      "total vector 3000000\n",
      "Converted 21403 words (11929 misses)\n",
      "word2vec_trained\n",
      "Balanced\n",
      "cnn\n",
      "Epoch 1/8\n",
      "107/107 [==============================] - 30s 18ms/step - loss: 0.6126 - accuracy: 0.6649 - precision: 0.6648 - recall: 0.6679 - val_loss: 0.5761 - val_accuracy: 0.7011 - val_precision: 0.7388 - val_recall: 0.6185\n",
      "Epoch 2/8\n",
      "107/107 [==============================] - 1s 11ms/step - loss: 0.5499 - accuracy: 0.7241 - precision: 0.7278 - recall: 0.7178 - val_loss: 0.5573 - val_accuracy: 0.7236 - val_precision: 0.7176 - val_recall: 0.7337\n",
      "Epoch 3/8\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 0.5198 - accuracy: 0.7475 - precision: 0.7499 - recall: 0.7443 - val_loss: 0.5537 - val_accuracy: 0.7167 - val_precision: 0.7360 - val_recall: 0.6723\n",
      "Epoch 4/8\n",
      "107/107 [==============================] - 1s 11ms/step - loss: 0.4928 - accuracy: 0.7664 - precision: 0.7689 - recall: 0.7630 - val_loss: 0.5476 - val_accuracy: 0.7282 - val_precision: 0.7239 - val_recall: 0.7343\n",
      "Epoch 5/8\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 0.4679 - accuracy: 0.7810 - precision: 0.7828 - recall: 0.7789 - val_loss: 0.5493 - val_accuracy: 0.7207 - val_precision: 0.7519 - val_recall: 0.6554\n",
      "Epoch 6/8\n",
      "107/107 [==============================] - 1s 11ms/step - loss: 0.4426 - accuracy: 0.7994 - precision: 0.8019 - recall: 0.7963 - val_loss: 0.5457 - val_accuracy: 0.7304 - val_precision: 0.7312 - val_recall: 0.7252\n",
      "Epoch 7/8\n",
      "107/107 [==============================] - 1s 11ms/step - loss: 0.4219 - accuracy: 0.8118 - precision: 0.8156 - recall: 0.8066 - val_loss: 0.5464 - val_accuracy: 0.7289 - val_precision: 0.7281 - val_recall: 0.7272\n",
      "Epoch 8/8\n",
      "107/107 [==============================] - 1s 11ms/step - loss: 0.3988 - accuracy: 0.8240 - precision: 0.8260 - recall: 0.8218 - val_loss: 0.5537 - val_accuracy: 0.7230 - val_precision: 0.7497 - val_recall: 0.6664\n",
      "training time 38.80705976486206\n",
      "acc 0.7011037468910217\n",
      "Score:  0.5536502599716187\n",
      "Accuracy:  0.7230316400527954\n",
      "rnn\n",
      "Epoch 1/8\n",
      "107/107 [==============================] - 13s 114ms/step - loss: 0.6362 - accuracy: 0.6580 - precision: 0.6554 - recall: 0.6673 - val_loss: 0.6194 - val_accuracy: 0.6661 - val_precision: 0.6210 - val_recall: 0.8457\n",
      "Epoch 2/8\n",
      "107/107 [==============================] - 12s 112ms/step - loss: 0.5962 - accuracy: 0.6849 - precision: 0.6794 - recall: 0.7027 - val_loss: 0.6067 - val_accuracy: 0.6723 - val_precision: 0.7143 - val_recall: 0.5697\n",
      "Epoch 3/8\n",
      "107/107 [==============================] - 12s 112ms/step - loss: 0.5789 - accuracy: 0.6980 - precision: 0.6946 - recall: 0.7087 - val_loss: 0.6158 - val_accuracy: 0.6804 - val_precision: 0.6355 - val_recall: 0.8398\n",
      "Epoch 4/8\n",
      "107/107 [==============================] - 12s 114ms/step - loss: 0.5671 - accuracy: 0.7071 - precision: 0.7034 - recall: 0.7181 - val_loss: 0.6093 - val_accuracy: 0.6796 - val_precision: 0.6498 - val_recall: 0.7736\n",
      "Epoch 5/8\n",
      "107/107 [==============================] - 12s 110ms/step - loss: 0.5581 - accuracy: 0.7154 - precision: 0.7118 - recall: 0.7258 - val_loss: 0.6047 - val_accuracy: 0.6902 - val_precision: 0.6679 - val_recall: 0.7518\n",
      "Epoch 6/8\n",
      "107/107 [==============================] - 11s 105ms/step - loss: 0.5481 - accuracy: 0.7248 - precision: 0.7200 - recall: 0.7375 - val_loss: 0.6085 - val_accuracy: 0.6749 - val_precision: 0.6832 - val_recall: 0.6475\n",
      "Epoch 7/8\n",
      "107/107 [==============================] - 12s 111ms/step - loss: 0.5338 - accuracy: 0.7324 - precision: 0.7296 - recall: 0.7403 - val_loss: 0.6145 - val_accuracy: 0.6842 - val_precision: 0.6729 - val_recall: 0.7119\n",
      "Epoch 8/8\n",
      "107/107 [==============================] - 12s 112ms/step - loss: 0.5168 - accuracy: 0.7453 - precision: 0.7434 - recall: 0.7509 - val_loss: 0.6457 - val_accuracy: 0.6512 - val_precision: 0.6343 - val_recall: 0.7077\n",
      "training time 96.49803233146667\n",
      "acc 0.6660779714584351\n",
      "Score:  0.6457046866416931\n",
      "Accuracy:  0.6512141227722168\n",
      "lstm\n",
      "Epoch 1/8\n",
      "107/107 [==============================] - 15s 40ms/step - loss: 0.6200 - accuracy: 0.6542 - precision: 0.6458 - recall: 0.6843 - val_loss: 0.5829 - val_accuracy: 0.6948 - val_precision: 0.6981 - val_recall: 0.6820\n",
      "Epoch 2/8\n",
      "107/107 [==============================] - 3s 24ms/step - loss: 0.5819 - accuracy: 0.6966 - precision: 0.6928 - recall: 0.7087 - val_loss: 0.5730 - val_accuracy: 0.6993 - val_precision: 0.7031 - val_recall: 0.6859\n",
      "Epoch 3/8\n",
      "107/107 [==============================] - 2s 23ms/step - loss: 0.5663 - accuracy: 0.7081 - precision: 0.7032 - recall: 0.7222 - val_loss: 0.5661 - val_accuracy: 0.7063 - val_precision: 0.6917 - val_recall: 0.7400\n",
      "Epoch 4/8\n",
      "107/107 [==============================] - 2s 23ms/step - loss: 0.5522 - accuracy: 0.7182 - precision: 0.7123 - recall: 0.7340 - val_loss: 0.5661 - val_accuracy: 0.7055 - val_precision: 0.6742 - val_recall: 0.7908\n",
      "Epoch 5/8\n",
      "107/107 [==============================] - 3s 23ms/step - loss: 0.5427 - accuracy: 0.7237 - precision: 0.7164 - recall: 0.7426 - val_loss: 0.5645 - val_accuracy: 0.7116 - val_precision: 0.6870 - val_recall: 0.7730\n",
      "Epoch 6/8\n",
      "107/107 [==============================] - 2s 23ms/step - loss: 0.5320 - accuracy: 0.7318 - precision: 0.7268 - recall: 0.7447 - val_loss: 0.5558 - val_accuracy: 0.7155 - val_precision: 0.7066 - val_recall: 0.7332\n",
      "Epoch 7/8\n",
      "107/107 [==============================] - 2s 23ms/step - loss: 0.5166 - accuracy: 0.7404 - precision: 0.7354 - recall: 0.7525 - val_loss: 0.5694 - val_accuracy: 0.7043 - val_precision: 0.7439 - val_recall: 0.6197\n",
      "Epoch 8/8\n",
      "107/107 [==============================] - 2s 23ms/step - loss: 0.5052 - accuracy: 0.7499 - precision: 0.7435 - recall: 0.7646 - val_loss: 0.5592 - val_accuracy: 0.7167 - val_precision: 0.6899 - val_recall: 0.7831\n",
      "training time 32.28439450263977\n",
      "acc 0.6947755813598633\n",
      "Score:  0.5591960549354553\n",
      "Accuracy:  0.716703474521637\n",
      "gru\n",
      "Epoch 1/8\n",
      "107/107 [==============================] - 8s 37ms/step - loss: 0.6182 - accuracy: 0.6651 - precision: 0.6596 - recall: 0.6836 - val_loss: 0.5815 - val_accuracy: 0.6933 - val_precision: 0.7015 - val_recall: 0.6687\n",
      "Epoch 2/8\n",
      "107/107 [==============================] - 2s 22ms/step - loss: 0.5768 - accuracy: 0.7007 - precision: 0.6977 - recall: 0.7104 - val_loss: 0.5682 - val_accuracy: 0.7032 - val_precision: 0.7020 - val_recall: 0.7018\n",
      "Epoch 3/8\n",
      "107/107 [==============================] - 2s 22ms/step - loss: 0.5630 - accuracy: 0.7102 - precision: 0.7053 - recall: 0.7241 - val_loss: 0.5592 - val_accuracy: 0.7086 - val_precision: 0.7031 - val_recall: 0.7181\n",
      "Epoch 4/8\n",
      "107/107 [==============================] - 2s 22ms/step - loss: 0.5519 - accuracy: 0.7169 - precision: 0.7106 - recall: 0.7336 - val_loss: 0.5771 - val_accuracy: 0.6926 - val_precision: 0.6463 - val_recall: 0.8454\n",
      "Epoch 5/8\n",
      "107/107 [==============================] - 2s 22ms/step - loss: 0.5434 - accuracy: 0.7239 - precision: 0.7128 - recall: 0.7518 - val_loss: 0.5626 - val_accuracy: 0.7110 - val_precision: 0.7357 - val_recall: 0.6548\n",
      "Epoch 6/8\n",
      "107/107 [==============================] - 2s 21ms/step - loss: 0.5312 - accuracy: 0.7325 - precision: 0.7252 - recall: 0.7505 - val_loss: 0.5525 - val_accuracy: 0.7167 - val_precision: 0.7104 - val_recall: 0.7278\n",
      "Epoch 7/8\n",
      "107/107 [==============================] - 2s 21ms/step - loss: 0.5221 - accuracy: 0.7374 - precision: 0.7313 - recall: 0.7521 - val_loss: 0.5538 - val_accuracy: 0.7188 - val_precision: 0.7050 - val_recall: 0.7485\n",
      "Epoch 8/8\n",
      "107/107 [==============================] - 2s 21ms/step - loss: 0.5097 - accuracy: 0.7471 - precision: 0.7391 - recall: 0.7654 - val_loss: 0.5617 - val_accuracy: 0.7141 - val_precision: 0.6817 - val_recall: 0.7988\n",
      "training time 24.194915056228638\n",
      "acc 0.6933038830757141\n",
      "Score:  0.5616813898086548\n",
      "Accuracy:  0.7140544652938843\n"
     ]
    }
   ],
   "source": [
    "# word2vec word embedding \n",
    "pre_trained_model, model_name = get_word2vec_model()\n",
    "custom_encoder, custom_embedding, embedding_name, missWord = pre_trained_em(x_train, pre_trained_model, model_name)\n",
    "print(embedding_name)\n",
    "print(dataset_name)\n",
    "\n",
    "model_start_train(x_train, y_train_binary, x_test,y_test_binary,custom_encoder, custom_embedding, embedding_name, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fasttext word embedding \n",
    "pre_trained_model, model_name = get_fasttext_model()\n",
    "custom_encoder, custom_embedding, embedding_name, missWord = pre_trained_em(x_train, pre_trained_model, model_name)\n",
    "print(embedding_name)\n",
    "print(dataset_name)\n",
    "\n",
    "#model_start_train(x_train, y_train_binary, x_test,y_test_binary,custom_encoder, custom_embedding, embedding_name, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove word embedding\n",
    "custom_encoder, custom_embedding, embedding_name, missWord = glove_em(x_train)\n",
    "print(embedding_name)\n",
    "print(dataset_name)\n",
    "\n",
    "#model_start_train(x_train, y_train_binary, x_test,y_test_binary,custom_encoder, custom_embedding, embedding_name, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learned word embedding\n",
    "custom_encoder, custom_embedding, embedding_name = noTrained_em(x_train)\n",
    "print(embedding_name)\n",
    "print(dataset_name)\n",
    "\n",
    "#model_start_train(x_train, y_train_binary, x_test,y_test_binary,custom_encoder, custom_embedding, embedding_name, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>hate f1</th>\n",
       "      <th>non-hate f1</th>\n",
       "      <th>hate support</th>\n",
       "      <th>non-hate support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Balanced_word2vec_trained_cnn</td>\n",
       "      <td>0.723032</td>\n",
       "      <td>0.725771</td>\n",
       "      <td>0.722807</td>\n",
       "      <td>0.722054</td>\n",
       "      <td>0.705569</td>\n",
       "      <td>0.738538</td>\n",
       "      <td>3384.0</td>\n",
       "      <td>3411.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Balanced_word2vec_trained_rnn</td>\n",
       "      <td>0.651214</td>\n",
       "      <td>0.653339</td>\n",
       "      <td>0.651438</td>\n",
       "      <td>0.650205</td>\n",
       "      <td>0.668994</td>\n",
       "      <td>0.631415</td>\n",
       "      <td>3384.0</td>\n",
       "      <td>3411.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Balanced_word2vec_trained_lstm</td>\n",
       "      <td>0.716703</td>\n",
       "      <td>0.720724</td>\n",
       "      <td>0.716966</td>\n",
       "      <td>0.715564</td>\n",
       "      <td>0.733564</td>\n",
       "      <td>0.697565</td>\n",
       "      <td>3384.0</td>\n",
       "      <td>3411.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Balanced_word2vec_trained_gru</td>\n",
       "      <td>0.714054</td>\n",
       "      <td>0.720539</td>\n",
       "      <td>0.714390</td>\n",
       "      <td>0.712141</td>\n",
       "      <td>0.735610</td>\n",
       "      <td>0.688672</td>\n",
       "      <td>3384.0</td>\n",
       "      <td>3411.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Model  Accuracy  precision    recall  f1-score  \\\n",
       "0   Balanced_word2vec_trained_cnn  0.723032   0.725771  0.722807  0.722054   \n",
       "1   Balanced_word2vec_trained_rnn  0.651214   0.653339  0.651438  0.650205   \n",
       "2  Balanced_word2vec_trained_lstm  0.716703   0.720724  0.716966  0.715564   \n",
       "3   Balanced_word2vec_trained_gru  0.714054   0.720539  0.714390  0.712141   \n",
       "\n",
       "    hate f1  non-hate f1  hate support  non-hate support  \n",
       "0  0.705569     0.738538        3384.0            3411.0  \n",
       "1  0.668994     0.631415        3384.0            3411.0  \n",
       "2  0.733564     0.697565        3384.0            3411.0  \n",
       "3  0.735610     0.688672        3384.0            3411.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
