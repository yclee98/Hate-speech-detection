{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle \n",
    "\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    counter = 0\n",
    "    for i in text:\n",
    "        counter += len(i.split())\n",
    "    return counter\n",
    "\n",
    "def count_token(text):\n",
    "    s = set()\n",
    "    for i in text:\n",
    "        tokenize = i.split()\n",
    "        for j in tokenize:\n",
    "            s.add(j)    \n",
    "    return len(s)\n",
    "\n",
    "\n",
    "def load_dataset(ds):\n",
    "    if ds == 1:\n",
    "        dataset_name = \"GabHateCorpus\"\n",
    "    elif ds == 2:\n",
    "        dataset_name = \"Implicit_hate_corpus\"\n",
    "    elif ds == 3:\n",
    "        dataset_name = \"SE2019\"\n",
    "    else:\n",
    "        dataset_name = \"Balanced\"\n",
    "\n",
    "    filepath = \"Dataset/\"+dataset_name\n",
    "    df = pd.read_csv(filepath+\"/data_final.csv\")\n",
    "    \n",
    "    print(df['class'].value_counts(normalize=True))\n",
    "    return df, dataset_name\n",
    "\n",
    "def split_data(df):\n",
    "    test_size = 0.20\n",
    "    x = np.array(df[\"text\"])\n",
    "    y = np.array(df[\"class\"])\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = test_size, random_state=42) #random state ensure same sample\n",
    "    print(\"Train Set :\", x_train.shape, y_train.shape) \n",
    "    print(\"Test Set  :\", x_test.shape, y_test.shape) \n",
    "    print(\"Total \", len(df))\n",
    "    # y in digit form\n",
    "    y_train_binary = np.array(list(map(lambda x:1 if x==\"Hate\" else 0, y_train)))\n",
    "    y_test_binary = np.array(list(map(lambda x:1 if x==\"Hate\" else 0, y_test)))\n",
    "    return x_train, y_train, y_train_binary, x_test, y_test, y_test_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.utils.data_utils import pad_sequences\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, GlobalMaxPooling1D, Embedding\n",
    "from keras.layers import Conv1D, LSTM, SpatialDropout1D, Bidirectional, GRU, SimpleRNN, TextVectorization\n",
    "\n",
    "from keras.metrics import BinaryAccuracy,Precision,Recall\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "\n",
    "from gensim.models import FastText, Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimingCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, logs={}):\n",
    "        super(TimingCallback, self).__init__()\n",
    "\n",
    "    def on_train_begin(self, epoch, logs={}):\n",
    "        self.starttime = time.time()\n",
    "    def on_train_end(self, epoch, logs={}):\n",
    "        self.stoptime = time.time()\n",
    "        print(f\"training time {self.stoptime - self.starttime}\")\n",
    "        \n",
    "def get_classification_report(i, cr):\n",
    "    return [i, cr['accuracy'], cr['macro avg']['precision'], \n",
    "            cr['macro avg']['recall'], cr['macro avg']['f1-score'],\n",
    "            cr['Hate']['f1-score'],cr['Non-Hate']['f1-score'], \n",
    "            cr['Hate']['support'],cr['Non-Hate']['support']]\n",
    "\n",
    "def get_result_table():\n",
    "    c = ['Model', 'Accuracy', 'precision', 'recall', 'f1-score', 'hate f1', \"non-hate f1\", 'hate support', 'non-hate support']\n",
    "    result_table = pd.DataFrame(columns=c)\n",
    "    return result_table\n",
    "\n",
    "def get_result_single(y_test, y_test_pred, model_name, result_table):\n",
    "    cr = classification_report(y_test, y_test_pred, labels=[\"Hate\",\"Non-Hate\"], output_dict=True)\n",
    "    result_table.loc[len(result_table)] = get_classification_report(model_name, cr)\n",
    "\n",
    "# def get_result_multiple(x_test, y_test, model_to_load):\n",
    "#     c = ['Model', 'Accuracy', 'precision', 'recall', 'f1-score', 'hate f1', \"non-hate f1\", 'hate support', 'non-hate support']\n",
    "#     result_table = pd.DataFrame(columns=c)\n",
    "#     for i in model_to_load:\n",
    "#         filename = f\"models/{i}\"\n",
    "#         print(filename)\n",
    "#         old_model = load_model(filename)\n",
    "\n",
    "#         y_test_pred = old_model.predict(x_test, verbose=0)\n",
    "#         y_test_pred = np.where(y_test_pred > 0.5, \"Hate\", \"Non-Hate\") \n",
    "#         y_test_pred = y_test_pred.flatten()\n",
    "\n",
    "#         cr = classification_report(y_test, y_test_pred, labels=[\"Hate\",\"Non-Hate\"], output_dict=True)\n",
    "#         result_table.loc[len(result_table)] = get_classification_report(i, cr)\n",
    "#     return result_table.style.highlight_max(color = 'red', axis = 0)\n",
    "\n",
    "def nn_predict(model,x_test, y_test_binary):\n",
    "    score = model.evaluate(x_test, y_test_binary, verbose=0)\n",
    "    print(\"Score: \", score[0])\n",
    "    print(\"Accuracy: \", score[1])\n",
    "\n",
    "    y_test_pred_percent = model.predict(x_test, verbose=0)\n",
    "    y_test_pred = np.where(y_test_pred_percent > 0.5, \"Hate\", \"Non-Hate\") \n",
    "    y_test_pred = y_test_pred.flatten()\n",
    "\n",
    "    return y_test_pred\n",
    "\n",
    "def save_model_nn(model, model_name, embedding_name, dataset_name):\n",
    "    filename = f\"models/{dataset_name}_{embedding_name}_{model_name}\"\n",
    "    model.save(filename)\n",
    "    return filename\n",
    "\n",
    "def load_model_nn(model_name):\n",
    "    filename = f\"models/{model_name}\"\n",
    "    print(filename)\n",
    "    return load_model(filename) \n",
    "\n",
    "METRICS = [\n",
    "    BinaryAccuracy(name=\"accuracy\"),\n",
    "    Precision(name=\"precision\"),\n",
    "    Recall(name=\"recall\")\n",
    "]\n",
    "\n",
    "def compile_fit_save(x_train, y_train_binary, x_test,y_test_binary, model, model_name, embedding_name, dataset_name, save, epoch=5, batch_size=32, lr=0.01):    \n",
    "    opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=opt,\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=METRICS)\n",
    "    \n",
    "    history = model.fit(x_train, y_train_binary, epochs=epoch,\n",
    "                        validation_data=(x_test,y_test_binary),\n",
    "                        batch_size = batch_size,\n",
    "                        callbacks=[TimingCallback()])\n",
    "\n",
    "    if save: \n",
    "        save_model_nn(model, model_name, embedding_name, dataset_name)        \n",
    "    print(f\"acc {history.history['val_accuracy'][0]}\")\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_em(x_train):\n",
    "    embedding_name = \"glove\"\n",
    "    text_length = 50 #pad/truncate text to this long, such that each text after token will be this long\n",
    "\n",
    "    custom_encoder = TextVectorization(\n",
    "        standardize = None,\n",
    "        output_sequence_length=text_length, \n",
    "    )\n",
    "    custom_encoder.adapt(x_train)\n",
    "    vocab = custom_encoder.get_vocabulary()\n",
    "    print(f\"total vocab {len(vocab)}\")\n",
    "    vocab_dict = dict(zip(vocab, range(len(vocab))))\n",
    "\n",
    "    # load glove to dictionay\n",
    "    embeddings_dic = dict()\n",
    "    glove_file = open(\"Dataset/trained/glove.42B.300d.txt\", encoding=\"utf8\")\n",
    "\n",
    "    for line in glove_file:\n",
    "        records = line.split()\n",
    "        word = records[0]\n",
    "        vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "        embeddings_dic[word] = vector_dimensions\n",
    "    glove_file.close()\n",
    "    print(\"Total words \", len(embeddings_dic))\n",
    "\n",
    "    # create vocab length is the size of token in dictionary\n",
    "    # Size of the vocabulary\n",
    "    vocab_length = len(vocab) + 1\n",
    "    embedding_dim = 300 #each glove word is 100 long\n",
    "\n",
    "    hits = 0\n",
    "    miss = 0\n",
    "    missWord = []\n",
    "\n",
    "    # create embedding matrix having 100 col\n",
    "    # for all vocab word we give it a vector value from glove\n",
    "    # for those not found in glove will be empty 0\n",
    "    # size of embedding_matriz = size of word_tokenizer.word_index.items()\n",
    "    # embedding_matrix is the weight \n",
    "    embedding_matrix = np.zeros((vocab_length, embedding_dim))\n",
    "    for word, index in vocab_dict.items():\n",
    "        embedding_vector = embeddings_dic.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            miss +=1\n",
    "            missWord.append(word)\n",
    "    print(\"Converted %d words (%d misses)\" % (hits, miss))\n",
    "\n",
    "    custom_embedding = Embedding(vocab_length, embedding_dim, \n",
    "                embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "                trainable = False,\n",
    "                input_length=text_length,\n",
    "                mask_zero=True)\n",
    "    \n",
    "    return custom_encoder, custom_embedding, embedding_name, missWord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText, Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fasttext_model():\n",
    "    model_name=\"fasttext_trained\"\n",
    "    return KeyedVectors.load_word2vec_format(\"./Dataset/trained/wiki-news-300d-1M-subword.vec\", binary=False), model_name\n",
    "\n",
    "def get_word2vec_model():\n",
    "    model_name = \"word2vec_trained\"\n",
    "    return KeyedVectors.load_word2vec_format(\"./Dataset/trained/GoogleNews-vectors-negative300.bin\", binary=True), model_name\n",
    "\n",
    "def pre_trained_em(x_train, model_em, embedding_name):\n",
    "    text_length = 50 #pad/truncate text to this long, such that each text after token will be this long\n",
    "\n",
    "    custom_encoder = TextVectorization(\n",
    "        standardize = None,\n",
    "        output_sequence_length=text_length, \n",
    "    )\n",
    "    custom_encoder.adapt(x_train)\n",
    "    vocab = custom_encoder.get_vocabulary()\n",
    "    print(f\"total vocab {len(vocab)}\")\n",
    "    vocab_dict = dict(zip(vocab, range(len(vocab))))\n",
    "\n",
    "    vocab_length = len(vocab) + 1\n",
    "    embedding_dim = 300 \n",
    "\n",
    "    hits = 0\n",
    "    miss = 0\n",
    "    missWord = []\n",
    "\n",
    "    embedding_matrix = np.zeros((vocab_length, embedding_dim))\n",
    "    keyVector_key = model_em.index_to_key\n",
    "    print(f\"total vector {len(keyVector_key)}\")\n",
    "    for word, index in vocab_dict.items():\n",
    "        if word in keyVector_key:\n",
    "            embedding_vector = np.array(model_em[word])\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[index] = embedding_vector\n",
    "                hits += 1\n",
    "        else:\n",
    "            miss +=1\n",
    "            missWord.append(word)\n",
    "            \n",
    "    print(\"Converted %d words (%d misses)\" % (hits, miss))\n",
    "\n",
    "    custom_embedding = Embedding(vocab_length, embedding_dim, \n",
    "                embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "                trainable = False,\n",
    "                input_length=text_length,\n",
    "                mask_zero=True)\n",
    "    \n",
    "    return custom_encoder, custom_embedding, embedding_name, missWord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No pre-trained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noTrained_em(x_train):\n",
    "    embedding_name = \"no_train\"\n",
    "    text_length = 50 #pad/truncate text to this long, such that each text after token will be this long\n",
    "    vector_size= 300\n",
    "\n",
    "    custom_encoder = TextVectorization(\n",
    "        standardize = None,\n",
    "        output_sequence_length=text_length, \n",
    "    )\n",
    "    custom_encoder.adapt(x_train)\n",
    "    vocab = custom_encoder.get_vocabulary()\n",
    "    print(f\"total vocab {len(vocab)}\")\n",
    "    vocab_dict = dict(zip(vocab, range(len(vocab))))\n",
    "\n",
    "    vocab_length = len(vocab) + 1\n",
    "    embedding_dim = vector_size\n",
    "\n",
    "    custom_embedding = Embedding(vocab_length, embedding_dim,\n",
    "                input_length=text_length,\n",
    "                mask_zero=True)\n",
    "    return custom_encoder, custom_embedding, embedding_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization\n",
    "def add_connected_layer(model):\n",
    "    # model.add(Dropout(0.2))\n",
    "    # model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(x_train, y_train_binary, x_test,y_test_binary,custom_encoder, custom_embedding, embedding_name, dataset_name, save = True, epoch = 10, batch_size=32, lr=0.01):\n",
    "    model_name = \"cnn\"\n",
    "    print(model_name)\n",
    "    model = Sequential()\n",
    "    model.add(custom_encoder)\n",
    "    model.add(custom_embedding)\n",
    "    model.add(Conv1D(128, 3, activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    add_connected_layer(model)\n",
    "    return compile_fit_save(x_train, y_train_binary, x_test,y_test_binary,model, model_name, embedding_name, dataset_name, save, epoch, batch_size, lr)\n",
    "\n",
    "def rnn(x_train, y_train_binary, x_test,y_test_binary,custom_encoder, custom_embedding, embedding_name, dataset_name, save = True, epoch = 10, batch_size=32, lr=0.01):\n",
    "    model_name = \"rnn\"\n",
    "    print(model_name)\n",
    "    model = Sequential()\n",
    "    model.add(custom_encoder)\n",
    "    model.add(custom_embedding)\n",
    "    model.add(SimpleRNN(128))\n",
    "    add_connected_layer(model)\n",
    "    return compile_fit_save(x_train, y_train_binary, x_test,y_test_binary,model, model_name, embedding_name, dataset_name, save, epoch, batch_size, lr)\n",
    "\n",
    "def lstm(x_train, y_train_binary, x_test,y_test_binary,custom_encoder, custom_embedding, embedding_name, dataset_name, save = True, epoch = 10, batch_size=32, lr=0.01):\n",
    "    model_name = \"lstm\"\n",
    "    print(model_name)\n",
    "    model = Sequential()\n",
    "    model.add(custom_encoder)\n",
    "    model.add(custom_embedding)\n",
    "    # model.add(SpatialDropout1D(0.2))\n",
    "    model.add(LSTM(128))\n",
    "    add_connected_layer(model)\n",
    "    return compile_fit_save(x_train, y_train_binary, x_test,y_test_binary,model, model_name, embedding_name, dataset_name, save, epoch, batch_size, lr)\n",
    "\n",
    "def gru(x_train, y_train_binary, x_test,y_test_binary,custom_encoder, custom_embedding, embedding_name, dataset_name, save = True, epoch = 10, batch_size=32, lr=0.01):\n",
    "    model_name = \"gru\"\n",
    "    print(model_name)\n",
    "    model = Sequential()\n",
    "    model.add(custom_encoder)\n",
    "    model.add(custom_embedding)\n",
    "    # model.add(SpatialDropout1D(0.2))\n",
    "    model.add(GRU(128))\n",
    "    add_connected_layer(model)\n",
    "    return compile_fit_save(x_train, y_train_binary, x_test,y_test_binary,model, model_name, embedding_name, dataset_name, save, epoch, batch_size, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "Hate        0.500427\n",
      "Non-Hate    0.499573\n",
      "Name: proportion, dtype: float64\n",
      "Train Set : (27178,) (27178,)\n",
      "Test Set  : (6795,) (6795,)\n",
      "Total  33973\n",
      "Balanced\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' if you think im sweating about your petty ass think again bitch bitchimnotscared obviouslyyouare youthreatenedhoe '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df, dataset_name = load_dataset(4) \n",
    "x_train, y_train, y_train_binary, x_test, y_test, y_test_binary = split_data(df)\n",
    "df_result = get_result_table()\n",
    "print(dataset_name)\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_start_train(x_train, y_train_binary, x_test,y_test_binary,custom_encoder, custom_embedding, embedding_name, dataset_name):\n",
    "    model, h = cnn(x_train, y_train_binary, x_test,y_test_binary,custom_encoder, custom_embedding, embedding_name, dataset_name,save=False, epoch=8, batch_size=256, lr=0.001)\n",
    "    y_test_pred = nn_predict(model, x_test, y_test_binary)\n",
    "    get_result_single(y_test, y_test_pred, dataset_name+\"_\"+embedding_name+\"_cnn\", df_result)\n",
    "\n",
    "    model, h = rnn(x_train, y_train_binary, x_test,y_test_binary,custom_encoder, custom_embedding, embedding_name, dataset_name,save=False, epoch=8, batch_size=256, lr=0.001)\n",
    "    y_test_pred = nn_predict(model, x_test, y_test_binary)\n",
    "    get_result_single(y_test, y_test_pred, dataset_name+\"_\"+embedding_name+\"_rnn\", df_result)\n",
    "\n",
    "    model, h = lstm(x_train, y_train_binary, x_test,y_test_binary,custom_encoder, custom_embedding, embedding_name, dataset_name,save=False, epoch=8, batch_size=256, lr=0.001)\n",
    "    y_test_pred = nn_predict(model, x_test, y_test_binary)\n",
    "    get_result_single(y_test, y_test_pred, dataset_name+\"_\"+embedding_name+\"_lstm\", df_result)\n",
    "\n",
    "    model, h = gru(x_train, y_train_binary, x_test,y_test_binary,custom_encoder, custom_embedding, embedding_name, dataset_name,save=False, epoch=8, batch_size=256, lr=0.001)\n",
    "    y_test_pred = nn_predict(model, x_test, y_test_binary)\n",
    "    get_result_single(y_test, y_test_pred, dataset_name+\"_\"+embedding_name+\"_gru\", df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec word embedding \n",
    "pre_trained_model, model_name = get_word2vec_model()\n",
    "custom_encoder, custom_embedding, embedding_name, missWord = pre_trained_em(x_train, pre_trained_model, model_name)\n",
    "print(embedding_name)\n",
    "print(dataset_name)\n",
    "\n",
    "model_start_train(x_train, y_train_binary, x_test,y_test_binary,custom_encoder, custom_embedding, embedding_name, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fasttext word embedding \n",
    "pre_trained_model, model_name = get_fasttext_model()\n",
    "custom_encoder, custom_embedding, embedding_name, missWord = pre_trained_em(x_train, pre_trained_model, model_name)\n",
    "print(embedding_name)\n",
    "print(dataset_name)\n",
    "\n",
    "#model_start_train(x_train, y_train_binary, x_test,y_test_binary,custom_encoder, custom_embedding, embedding_name, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total vocab 33332\n",
      "Total words  1917494\n",
      "Converted 25654 words (7678 misses)\n",
      "glove\n",
      "Balanced\n"
     ]
    }
   ],
   "source": [
    "# glove word embedding\n",
    "custom_encoder, custom_embedding, embedding_name, missWord = glove_em(x_train)\n",
    "print(embedding_name)\n",
    "print(dataset_name)\n",
    "\n",
    "#model_start_train(x_train, y_train_binary, x_test,y_test_binary,custom_encoder, custom_embedding, embedding_name, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learned word embedding\n",
    "custom_encoder, custom_embedding, embedding_name = noTrained_em(x_train)\n",
    "print(embedding_name)\n",
    "print(dataset_name)\n",
    "\n",
    "#model_start_train(x_train, y_train_binary, x_test,y_test_binary,custom_encoder, custom_embedding, embedding_name, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>hate f1</th>\n",
       "      <th>non-hate f1</th>\n",
       "      <th>hate support</th>\n",
       "      <th>non-hate support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Balanced_glove_cnn</td>\n",
       "      <td>0.721413</td>\n",
       "      <td>0.722470</td>\n",
       "      <td>0.721545</td>\n",
       "      <td>0.721157</td>\n",
       "      <td>0.729610</td>\n",
       "      <td>0.712703</td>\n",
       "      <td>3384.0</td>\n",
       "      <td>3411.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Balanced_glove_cnn</td>\n",
       "      <td>0.724945</td>\n",
       "      <td>0.724994</td>\n",
       "      <td>0.724970</td>\n",
       "      <td>0.724941</td>\n",
       "      <td>0.725913</td>\n",
       "      <td>0.723970</td>\n",
       "      <td>3384.0</td>\n",
       "      <td>3411.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Balanced_glove_cnn</td>\n",
       "      <td>0.587196</td>\n",
       "      <td>0.601943</td>\n",
       "      <td>0.587934</td>\n",
       "      <td>0.572833</td>\n",
       "      <td>0.651163</td>\n",
       "      <td>0.494504</td>\n",
       "      <td>3384.0</td>\n",
       "      <td>3411.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Balanced_glove_cnn</td>\n",
       "      <td>0.720677</td>\n",
       "      <td>0.721616</td>\n",
       "      <td>0.720801</td>\n",
       "      <td>0.720451</td>\n",
       "      <td>0.728392</td>\n",
       "      <td>0.712511</td>\n",
       "      <td>3384.0</td>\n",
       "      <td>3411.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Balanced_glove_cnn</td>\n",
       "      <td>0.712730</td>\n",
       "      <td>0.714015</td>\n",
       "      <td>0.712570</td>\n",
       "      <td>0.712195</td>\n",
       "      <td>0.699785</td>\n",
       "      <td>0.724605</td>\n",
       "      <td>3384.0</td>\n",
       "      <td>3411.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Model  Accuracy  precision    recall  f1-score   hate f1  \\\n",
       "0  Balanced_glove_cnn  0.721413   0.722470  0.721545  0.721157  0.729610   \n",
       "1  Balanced_glove_cnn  0.724945   0.724994  0.724970  0.724941  0.725913   \n",
       "2  Balanced_glove_cnn  0.587196   0.601943  0.587934  0.572833  0.651163   \n",
       "3  Balanced_glove_cnn  0.720677   0.721616  0.720801  0.720451  0.728392   \n",
       "4  Balanced_glove_cnn  0.712730   0.714015  0.712570  0.712195  0.699785   \n",
       "\n",
       "   non-hate f1  hate support  non-hate support  \n",
       "0     0.712703        3384.0            3411.0  \n",
       "1     0.723970        3384.0            3411.0  \n",
       "2     0.494504        3384.0            3411.0  \n",
       "3     0.712511        3384.0            3411.0  \n",
       "4     0.724605        3384.0            3411.0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
