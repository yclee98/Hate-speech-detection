{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    counter = 0\n",
    "    for i in text:\n",
    "        counter += len(i.split())\n",
    "    return counter\n",
    "\n",
    "def count_token(text):\n",
    "    s = set()\n",
    "    for i in text:\n",
    "        tokenize = i.split()\n",
    "        for j in tokenize:\n",
    "            s.add(j)    \n",
    "    return len(s)\n",
    "\n",
    "def load_dataset(ds):\n",
    "    if ds == 1:\n",
    "        dataset_name = \"GabHateCorpus\"\n",
    "    elif ds == 2:\n",
    "        dataset_name = \"Implicit_hate_corpus\"\n",
    "    elif ds == 3:\n",
    "        dataset_name = \"SE2019\"\n",
    "    else:\n",
    "        dataset_name = \"Balanced\"\n",
    "\n",
    "    filepath = \"Dataset/\"+dataset_name\n",
    "    df = pd.read_csv(filepath+\"/data_final.csv\")\n",
    "    \n",
    "    print(df['class'].value_counts(normalize=True))\n",
    "    return df, dataset_name\n",
    "\n",
    "def split_data(df):\n",
    "    test_size = 0.20\n",
    "    x = np.array(df[\"text\"])\n",
    "    y = np.array(df[\"class\"])\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = test_size, random_state=42) #random state ensure same sample\n",
    "    print(\"Train Set :\", x_train.shape, y_train.shape) \n",
    "    print(\"Test Set  :\", x_test.shape, y_test.shape) \n",
    "    print(\"Total \", len(df))\n",
    "    # y in digit form\n",
    "    y_train_binary = np.array(list(map(lambda x:1 if x==\"Hate\" else 0, y_train)))\n",
    "    y_test_binary = np.array(list(map(lambda x:1 if x==\"Hate\" else 0, y_test)))\n",
    "    return x_train, y_train, y_train_binary, x_test, y_test, y_test_binary\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.utils.data_utils import pad_sequences\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, GlobalMaxPooling1D, Embedding\n",
    "from keras.layers import Conv1D, LSTM, SpatialDropout1D, Bidirectional, GRU, SimpleRNN, TextVectorization\n",
    "\n",
    "from keras.metrics import BinaryAccuracy,Precision,Recall\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "\n",
    "from gensim.models import FastText, Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_report(i, cr):\n",
    "    return [i, cr['accuracy'], cr['macro avg']['precision'], \n",
    "            cr['macro avg']['recall'], cr['macro avg']['f1-score'],\n",
    "            cr['Hate']['f1-score'],cr['Non-Hate']['f1-score'], \n",
    "            cr['Hate']['support'],cr['Non-Hate']['support']]\n",
    "\n",
    "def get_result_table():\n",
    "    c = ['Model', 'Accuracy', 'precision', 'recall', 'f1-score', 'hate f1', \"non-hate f1\", 'hate support', 'non-hate support']\n",
    "    result_table = pd.DataFrame(columns=c)\n",
    "    return result_table\n",
    "\n",
    "def get_result_single(y_test, y_test_pred, model_name, result_table):\n",
    "    cr = classification_report(y_test, y_test_pred, labels=[\"Hate\",\"Non-Hate\"], output_dict=True)\n",
    "    result_table.loc[len(result_table)] = get_classification_report(model_name, cr)\n",
    "\n",
    "def nn_predict(model,x_test, y_test_binary):\n",
    "    score = model.evaluate(x_test, y_test_binary, verbose=0)\n",
    "    print(\"Score: \", score[0])\n",
    "    print(\"Accuracy: \", score[1])\n",
    "\n",
    "    y_test_pred_percent = model.predict(x_test, verbose=0)\n",
    "    y_test_pred = np.where(y_test_pred_percent > 0.5, \"Hate\", \"Non-Hate\") \n",
    "    y_test_pred = y_test_pred.flatten()\n",
    "\n",
    "    return y_test_pred\n",
    "\n",
    "def save_model_nn(model, model_name, embedding_name, dataset_name):\n",
    "    filename = f\"models/{dataset_name}_{embedding_name}_{model_name}\"\n",
    "    model.save(filename)\n",
    "    return filename\n",
    "\n",
    "def load_model_nn(model_name):\n",
    "    filename = f\"models/{model_name}\"\n",
    "    print(filename)\n",
    "    return load_model(filename) \n",
    "\n",
    "METRICS = [\n",
    "    BinaryAccuracy(name=\"accuracy\"),\n",
    "    Precision(name=\"precision\"),\n",
    "    Recall(name=\"recall\")\n",
    "]\n",
    "\n",
    "def compile_fit_save(x_train, y_train_binary, x_test,y_test_binary, model, model_name, embedding_name, dataset_name, save, epoch=5, batch_size=32, lr=0.01):    \n",
    "    opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=opt,\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=METRICS)\n",
    "    \n",
    "    history = model.fit(x_train, y_train_binary, epochs=epoch,\n",
    "                        validation_data=(x_test,y_test_binary),\n",
    "                        batch_size = batch_size,\n",
    "                        )\n",
    "\n",
    "    if save: \n",
    "        save_model_nn(model, model_name, embedding_name, dataset_name)        \n",
    "    print(f\"acc {history.history['val_accuracy'][0]}\")\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_em(x_train):\n",
    "    embedding_name = \"glove\"\n",
    "    text_length = 50 \n",
    "\n",
    "    custom_encoder = TextVectorization(\n",
    "        standardize = None,\n",
    "        output_sequence_length=text_length, \n",
    "    )\n",
    "    custom_encoder.adapt(x_train)\n",
    "    vocab = custom_encoder.get_vocabulary()\n",
    "    print(f\"total vocab {len(vocab)}\")\n",
    "    vocab_dict = dict(zip(vocab, range(len(vocab))))\n",
    "\n",
    "    embeddings_dic = dict()\n",
    "    glove_file = open(\"Dataset/trained/glove.42B.300d.txt\", encoding=\"utf8\")\n",
    "\n",
    "    for line in glove_file:\n",
    "        records = line.split()\n",
    "        word = records[0]\n",
    "        vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "        embeddings_dic[word] = vector_dimensions\n",
    "    glove_file.close()\n",
    "    print(\"Total words \", len(embeddings_dic))\n",
    "\n",
    "    vocab_length = len(vocab) + 1\n",
    "    embedding_dim = 300 \n",
    "\n",
    "    hits = 0\n",
    "    miss = 0\n",
    "    missWord = []\n",
    "\n",
    "    embedding_matrix = np.zeros((vocab_length, embedding_dim))\n",
    "    for word, index in vocab_dict.items():\n",
    "        embedding_vector = embeddings_dic.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            miss +=1\n",
    "            missWord.append(word)\n",
    "    print(\"Converted %d words (%d misses)\" % (hits, miss))\n",
    "\n",
    "    custom_embedding = Embedding(vocab_length, embedding_dim, \n",
    "                embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "                trainable = False,\n",
    "                input_length=text_length,\n",
    "                mask_zero=True)\n",
    "    \n",
    "    return custom_encoder, custom_embedding, embedding_name, missWord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText, Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fasttext_model():\n",
    "    model_name=\"fasttext_trained\"\n",
    "    return KeyedVectors.load_word2vec_format(\"./Dataset/trained/wiki-news-300d-1M-subword.vec\", binary=False), model_name\n",
    "\n",
    "def get_word2vec_model():\n",
    "    model_name = \"word2vec_trained\"\n",
    "    return KeyedVectors.load_word2vec_format(\"./Dataset/trained/GoogleNews-vectors-negative300.bin\", binary=True), model_name\n",
    "\n",
    "def pre_trained_em(x_train, model_em, embedding_name):\n",
    "    text_length = 50 \n",
    "\n",
    "    custom_encoder = TextVectorization(\n",
    "        standardize = None,\n",
    "        output_sequence_length=text_length, \n",
    "    )\n",
    "    custom_encoder.adapt(x_train)\n",
    "    vocab = custom_encoder.get_vocabulary()\n",
    "    print(f\"total vocab {len(vocab)}\")\n",
    "    vocab_dict = dict(zip(vocab, range(len(vocab))))\n",
    "\n",
    "    vocab_length = len(vocab) + 1\n",
    "    embedding_dim = 300 \n",
    "\n",
    "    hits = 0\n",
    "    miss = 0\n",
    "    missWord = []\n",
    "\n",
    "    embedding_matrix = np.zeros((vocab_length, embedding_dim))\n",
    "    keyVector_key = model_em.index_to_key\n",
    "    print(f\"total vector {len(keyVector_key)}\")\n",
    "    for word, index in vocab_dict.items():\n",
    "        if word in keyVector_key:\n",
    "            embedding_vector = np.array(model_em[word])\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[index] = embedding_vector\n",
    "                hits += 1\n",
    "        else:\n",
    "            miss +=1\n",
    "            missWord.append(word)\n",
    "            \n",
    "    print(\"Converted %d words (%d misses)\" % (hits, miss))\n",
    "\n",
    "    custom_embedding = Embedding(vocab_length, embedding_dim, \n",
    "                embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "                trainable = False,\n",
    "                input_length=text_length,\n",
    "                mask_zero=True)\n",
    "    \n",
    "    return custom_encoder, custom_embedding, embedding_name, missWord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No pre-trained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noTrained_em(x_train):\n",
    "    embedding_name = \"no_train\"\n",
    "    text_length = 50 \n",
    "    vector_size= 300\n",
    "\n",
    "    custom_encoder = TextVectorization(\n",
    "        standardize = None,\n",
    "        output_sequence_length=text_length, \n",
    "    )\n",
    "    custom_encoder.adapt(x_train)\n",
    "    vocab = custom_encoder.get_vocabulary()\n",
    "    print(f\"total vocab {len(vocab)}\")\n",
    "    vocab_dict = dict(zip(vocab, range(len(vocab))))\n",
    "\n",
    "    vocab_length = len(vocab) + 1\n",
    "    embedding_dim = vector_size\n",
    "\n",
    "    custom_embedding = Embedding(vocab_length, embedding_dim,\n",
    "                input_length=text_length,\n",
    "                mask_zero=True)\n",
    "    return custom_encoder, custom_embedding, embedding_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization\n",
    "def add_connected_layer(model):\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(x_train, y_train_binary, x_test,y_test_binary,custom_encoder, custom_embedding, embedding_name, dataset_name, save = True, epoch = 10, batch_size=32, lr=0.01):\n",
    "    model_name = \"cnn\"\n",
    "    print(model_name)\n",
    "    model = Sequential()\n",
    "    model.add(custom_encoder)\n",
    "    model.add(custom_embedding)\n",
    "    model.add(Conv1D(128, 3, activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    add_connected_layer(model)\n",
    "    return compile_fit_save(x_train, y_train_binary, x_test,y_test_binary,model, model_name, embedding_name, dataset_name, save, epoch, batch_size, lr)\n",
    "\n",
    "def rnn(x_train, y_train_binary, x_test,y_test_binary,custom_encoder, custom_embedding, embedding_name, dataset_name, save = True, epoch = 10, batch_size=32, lr=0.01):\n",
    "    model_name = \"rnn\"\n",
    "    print(model_name)\n",
    "    model = Sequential()\n",
    "    model.add(custom_encoder)\n",
    "    model.add(custom_embedding)\n",
    "    model.add(SimpleRNN(128))\n",
    "    add_connected_layer(model)\n",
    "    return compile_fit_save(x_train, y_train_binary, x_test,y_test_binary,model, model_name, embedding_name, dataset_name, save, epoch, batch_size, lr)\n",
    "\n",
    "def lstm(x_train, y_train_binary, x_test,y_test_binary,custom_encoder, custom_embedding, embedding_name, dataset_name, save = True, epoch = 10, batch_size=32, lr=0.01):\n",
    "    model_name = \"lstm\"\n",
    "    print(model_name)\n",
    "    model = Sequential()\n",
    "    model.add(custom_encoder)\n",
    "    model.add(custom_embedding)\n",
    "    model.add(LSTM(128))\n",
    "    add_connected_layer(model)\n",
    "    return compile_fit_save(x_train, y_train_binary, x_test,y_test_binary,model, model_name, embedding_name, dataset_name, save, epoch, batch_size, lr)\n",
    "\n",
    "def gru(x_train, y_train_binary, x_test,y_test_binary,custom_encoder, custom_embedding, embedding_name, dataset_name, save = True, epoch = 10, batch_size=32, lr=0.01):\n",
    "    model_name = \"gru\"\n",
    "    print(model_name)\n",
    "    model = Sequential()\n",
    "    model.add(custom_encoder)\n",
    "    model.add(custom_embedding)\n",
    "    model.add(GRU(128))\n",
    "    add_connected_layer(model)\n",
    "    return compile_fit_save(x_train, y_train_binary, x_test,y_test_binary,model, model_name, embedding_name, dataset_name, save, epoch, batch_size, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_start_train(x_train, y_train_binary, x_test, y_test_binary, y_test, custom_encoder, custom_embedding, embedding_name, dataset_name, df_result):\n",
    "    model, h = cnn(x_train, y_train_binary, x_test,y_test_binary,custom_encoder, custom_embedding, embedding_name, dataset_name,save=False, epoch=8, batch_size=256, lr=0.001)\n",
    "    y_test_pred = nn_predict(model, x_test, y_test_binary)\n",
    "    get_result_single(y_test, y_test_pred, dataset_name+\"_\"+embedding_name+\"_cnn\", df_result)\n",
    "\n",
    "    model, h = rnn(x_train, y_train_binary, x_test,y_test_binary,custom_encoder, custom_embedding, embedding_name, dataset_name,save=False, epoch=8, batch_size=256, lr=0.001)\n",
    "    y_test_pred = nn_predict(model, x_test, y_test_binary)\n",
    "    get_result_single(y_test, y_test_pred, dataset_name+\"_\"+embedding_name+\"_rnn\", df_result)\n",
    "\n",
    "    model, h = lstm(x_train, y_train_binary, x_test,y_test_binary,custom_encoder, custom_embedding, embedding_name, dataset_name,save=False, epoch=8, batch_size=256, lr=0.001)\n",
    "    y_test_pred = nn_predict(model, x_test, y_test_binary)\n",
    "    get_result_single(y_test, y_test_pred, dataset_name+\"_\"+embedding_name+\"_lstm\", df_result)\n",
    "\n",
    "    model, h = gru(x_train, y_train_binary, x_test,y_test_binary,custom_encoder, custom_embedding, embedding_name, dataset_name,save=False, epoch=8, batch_size=256, lr=0.001)\n",
    "    y_test_pred = nn_predict(model, x_test, y_test_binary)\n",
    "    get_result_single(y_test, y_test_pred, dataset_name+\"_\"+embedding_name+\"_gru\", df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dataset_all_model(df, dataset_name, df_result):\n",
    "    x_train, y_train, y_train_binary, x_test, y_test, y_test_binary = split_data(df)\n",
    "    print(dataset_name)\n",
    "\n",
    "    # word2vec word embedding \n",
    "    pre_trained_model, model_name = get_word2vec_model()\n",
    "    custom_encoder, custom_embedding, embedding_name, missWord = pre_trained_em(x_train, pre_trained_model, model_name)\n",
    "    print(embedding_name)\n",
    "    print(dataset_name)\n",
    "    model_start_train(x_train, y_train_binary, x_test,y_test_binary,y_test, custom_encoder, custom_embedding, embedding_name, dataset_name, df_result)\n",
    "\n",
    "    # fasttext word embedding \n",
    "    pre_trained_model, model_name = get_fasttext_model()\n",
    "    custom_encoder, custom_embedding, embedding_name, missWord = pre_trained_em(x_train, pre_trained_model, model_name)\n",
    "    print(embedding_name)\n",
    "    print(dataset_name)\n",
    "    model_start_train(x_train, y_train_binary, x_test,y_test_binary,y_test, custom_encoder, custom_embedding, embedding_name, dataset_name, df_result)\n",
    "\n",
    "    # glove word embedding\n",
    "    custom_encoder, custom_embedding, embedding_name, missWord = glove_em(x_train)\n",
    "    print(embedding_name)\n",
    "    print(dataset_name)\n",
    "    model_start_train(x_train, y_train_binary, x_test,y_test_binary,y_test, custom_encoder, custom_embedding, embedding_name, dataset_name, df_result)\n",
    "\n",
    "    # learned word embedding\n",
    "    custom_encoder, custom_embedding, embedding_name = noTrained_em(x_train)\n",
    "    print(embedding_name)\n",
    "    print(dataset_name)\n",
    "    model_start_train(x_train, y_train_binary, x_test,y_test_binary,y_test, custom_encoder, custom_embedding, embedding_name, dataset_name, df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "Non-Hate    0.876805\n",
      "Hate        0.123195\n",
      "Name: proportion, dtype: float64\n",
      "Train Set : (21715,) (21715,)\n",
      "Test Set  : (5429,) (5429,)\n",
      "Total  27144\n",
      "GabHateCorpus\n",
      "total vocab 33351\n",
      "total vector 3000000\n",
      "Converted 22819 words (10532 misses)\n",
      "word2vec_trained\n",
      "GabHateCorpus\n",
      "cnn\n",
      "Epoch 1/8\n",
      "85/85 [==============================] - 2s 17ms/step - loss: 0.3565 - accuracy: 0.8695 - precision: 0.4109 - recall: 0.1358 - val_loss: 0.2806 - val_accuracy: 0.8934 - val_precision: 0.6173 - val_recall: 0.1621\n",
      "Epoch 2/8\n",
      "85/85 [==============================] - 1s 12ms/step - loss: 0.2748 - accuracy: 0.8919 - precision: 0.6555 - recall: 0.2930 - val_loss: 0.2674 - val_accuracy: 0.8965 - val_precision: 0.5700 - val_recall: 0.3630\n",
      "Epoch 3/8\n",
      "85/85 [==============================] - 1s 11ms/step - loss: 0.2534 - accuracy: 0.8999 - precision: 0.6957 - recall: 0.3605 - val_loss: 0.2598 - val_accuracy: 0.9027 - val_precision: 0.7129 - val_recall: 0.2415\n",
      "Epoch 4/8\n",
      "85/85 [==============================] - 1s 12ms/step - loss: 0.2373 - accuracy: 0.9065 - precision: 0.7296 - recall: 0.4056 - val_loss: 0.2534 - val_accuracy: 0.9040 - val_precision: 0.6778 - val_recall: 0.2966\n",
      "Epoch 5/8\n",
      "85/85 [==============================] - 1s 11ms/step - loss: 0.2216 - accuracy: 0.9138 - precision: 0.7726 - recall: 0.4448 - val_loss: 0.2531 - val_accuracy: 0.9046 - val_precision: 0.6689 - val_recall: 0.3177\n",
      "Epoch 6/8\n",
      "85/85 [==============================] - 1s 11ms/step - loss: 0.2084 - accuracy: 0.9179 - precision: 0.7883 - recall: 0.4738 - val_loss: 0.2521 - val_accuracy: 0.9016 - val_precision: 0.6196 - val_recall: 0.3485\n",
      "Epoch 7/8\n",
      "85/85 [==============================] - 1s 12ms/step - loss: 0.1936 - accuracy: 0.9240 - precision: 0.8096 - recall: 0.5160 - val_loss: 0.2522 - val_accuracy: 0.9066 - val_precision: 0.6884 - val_recall: 0.3258\n",
      "Epoch 8/8\n",
      "85/85 [==============================] - 1s 12ms/step - loss: 0.1792 - accuracy: 0.9295 - precision: 0.8305 - recall: 0.5515 - val_loss: 0.2518 - val_accuracy: 0.9037 - val_precision: 0.6313 - val_recall: 0.3663\n",
      "acc 0.8933505415916443\n",
      "Score:  0.25177374482154846\n",
      "Accuracy:  0.9036654829978943\n",
      "rnn\n",
      "Epoch 1/8\n",
      "85/85 [==============================] - 12s 132ms/step - loss: 0.3817 - accuracy: 0.8764 - precision: 0.4914 - recall: 0.1029 - val_loss: 0.3185 - val_accuracy: 0.8862 - val_precision: 0.4952 - val_recall: 0.0843\n",
      "Epoch 2/8\n",
      "85/85 [==============================] - 9s 111ms/step - loss: 0.3200 - accuracy: 0.8773 - precision: 0.5305 - recall: 0.1977 - val_loss: 0.2872 - val_accuracy: 0.8908 - val_precision: 0.5845 - val_recall: 0.1345\n",
      "Epoch 3/8\n",
      "85/85 [==============================] - 10s 113ms/step - loss: 0.2989 - accuracy: 0.8843 - precision: 0.5940 - recall: 0.2479 - val_loss: 0.2829 - val_accuracy: 0.8910 - val_precision: 0.5418 - val_recall: 0.2626\n",
      "Epoch 4/8\n",
      "85/85 [==============================] - 9s 112ms/step - loss: 0.2746 - accuracy: 0.8896 - precision: 0.6160 - recall: 0.3212 - val_loss: 0.2842 - val_accuracy: 0.8906 - val_precision: 0.5324 - val_recall: 0.3063\n",
      "Epoch 5/8\n",
      "85/85 [==============================] - 10s 114ms/step - loss: 0.2672 - accuracy: 0.8944 - precision: 0.6572 - recall: 0.3319 - val_loss: 0.2846 - val_accuracy: 0.8941 - val_precision: 0.5955 - val_recall: 0.2123\n",
      "Epoch 6/8\n",
      "85/85 [==============================] - 10s 115ms/step - loss: 0.2567 - accuracy: 0.8976 - precision: 0.6546 - recall: 0.3913 - val_loss: 0.2919 - val_accuracy: 0.8784 - val_precision: 0.4553 - val_recall: 0.3549\n",
      "Epoch 7/8\n",
      "85/85 [==============================] - 10s 112ms/step - loss: 0.2448 - accuracy: 0.9060 - precision: 0.7118 - recall: 0.4221 - val_loss: 0.2924 - val_accuracy: 0.8843 - val_precision: 0.4821 - val_recall: 0.2399\n",
      "Epoch 8/8\n",
      "85/85 [==============================] - 9s 109ms/step - loss: 0.2270 - accuracy: 0.9137 - precision: 0.7266 - recall: 0.5009 - val_loss: 0.3014 - val_accuracy: 0.8934 - val_precision: 0.5440 - val_recall: 0.3809\n",
      "acc 0.8861668705940247\n",
      "Score:  0.3013761639595032\n",
      "Accuracy:  0.8933505415916443\n",
      "lstm\n",
      "Epoch 1/8\n",
      "85/85 [==============================] - 7s 40ms/step - loss: 0.3640 - accuracy: 0.8755 - precision: 0.4818 - recall: 0.1423 - val_loss: 0.3027 - val_accuracy: 0.8908 - val_precision: 0.5357 - val_recall: 0.2917\n",
      "Epoch 2/8\n",
      "85/85 [==============================] - 2s 26ms/step - loss: 0.2891 - accuracy: 0.8871 - precision: 0.6090 - recall: 0.2827 - val_loss: 0.2781 - val_accuracy: 0.8978 - val_precision: 0.6761 - val_recall: 0.1929\n",
      "Epoch 3/8\n",
      "85/85 [==============================] - 2s 24ms/step - loss: 0.2774 - accuracy: 0.8917 - precision: 0.6409 - recall: 0.3128 - val_loss: 0.2674 - val_accuracy: 0.9003 - val_precision: 0.6624 - val_recall: 0.2512\n",
      "Epoch 4/8\n",
      "85/85 [==============================] - 2s 24ms/step - loss: 0.2655 - accuracy: 0.8956 - precision: 0.6676 - recall: 0.3352 - val_loss: 0.2708 - val_accuracy: 0.8998 - val_precision: 0.6367 - val_recall: 0.2755\n",
      "Epoch 5/8\n",
      "85/85 [==============================] - 2s 25ms/step - loss: 0.2590 - accuracy: 0.8983 - precision: 0.6866 - recall: 0.3502 - val_loss: 0.2586 - val_accuracy: 0.9033 - val_precision: 0.6565 - val_recall: 0.3128\n",
      "Epoch 6/8\n",
      "85/85 [==============================] - 2s 25ms/step - loss: 0.2529 - accuracy: 0.9006 - precision: 0.6955 - recall: 0.3711 - val_loss: 0.2587 - val_accuracy: 0.9024 - val_precision: 0.6867 - val_recall: 0.2593\n",
      "Epoch 7/8\n",
      "85/85 [==============================] - 2s 25ms/step - loss: 0.2497 - accuracy: 0.9009 - precision: 0.6992 - recall: 0.3707 - val_loss: 0.2558 - val_accuracy: 0.9029 - val_precision: 0.6520 - val_recall: 0.3128\n",
      "Epoch 8/8\n",
      "85/85 [==============================] - 2s 24ms/step - loss: 0.2429 - accuracy: 0.9028 - precision: 0.7037 - recall: 0.3902 - val_loss: 0.2573 - val_accuracy: 0.9027 - val_precision: 0.6226 - val_recall: 0.3663\n",
      "acc 0.8907718062400818\n",
      "Score:  0.2573467791080475\n",
      "Accuracy:  0.9027445316314697\n",
      "gru\n",
      "Epoch 1/8\n",
      "85/85 [==============================] - 7s 39ms/step - loss: 0.3723 - accuracy: 0.8784 - precision: 0.5261 - recall: 0.1295 - val_loss: 0.2868 - val_accuracy: 0.8941 - val_precision: 0.6694 - val_recall: 0.1345\n",
      "Epoch 2/8\n",
      "85/85 [==============================] - 2s 22ms/step - loss: 0.2833 - accuracy: 0.8903 - precision: 0.6342 - recall: 0.2981 - val_loss: 0.2668 - val_accuracy: 0.9005 - val_precision: 0.6246 - val_recall: 0.3128\n",
      "Epoch 3/8\n",
      "85/85 [==============================] - 2s 22ms/step - loss: 0.2672 - accuracy: 0.8958 - precision: 0.6641 - recall: 0.3443 - val_loss: 0.2615 - val_accuracy: 0.9005 - val_precision: 0.5832 - val_recall: 0.4376\n",
      "Epoch 4/8\n",
      "85/85 [==============================] - 2s 22ms/step - loss: 0.2581 - accuracy: 0.8986 - precision: 0.6792 - recall: 0.3649 - val_loss: 0.2544 - val_accuracy: 0.9048 - val_precision: 0.6825 - val_recall: 0.3031\n",
      "Epoch 5/8\n",
      "85/85 [==============================] - 2s 22ms/step - loss: 0.2530 - accuracy: 0.8991 - precision: 0.6775 - recall: 0.3759 - val_loss: 0.2542 - val_accuracy: 0.9053 - val_precision: 0.6758 - val_recall: 0.3209\n",
      "Epoch 6/8\n",
      "85/85 [==============================] - 2s 22ms/step - loss: 0.2480 - accuracy: 0.9021 - precision: 0.6976 - recall: 0.3891 - val_loss: 0.2559 - val_accuracy: 0.9055 - val_precision: 0.7080 - val_recall: 0.2869\n",
      "Epoch 7/8\n",
      "85/85 [==============================] - 2s 23ms/step - loss: 0.2413 - accuracy: 0.9057 - precision: 0.7145 - recall: 0.4147 - val_loss: 0.2662 - val_accuracy: 0.9037 - val_precision: 0.6546 - val_recall: 0.3225\n",
      "Epoch 8/8\n",
      "85/85 [==============================] - 2s 21ms/step - loss: 0.2390 - accuracy: 0.9043 - precision: 0.7095 - recall: 0.4030 - val_loss: 0.2581 - val_accuracy: 0.9038 - val_precision: 0.6215 - val_recall: 0.3938\n",
      "acc 0.8940873146057129\n",
      "Score:  0.2581101357936859\n",
      "Accuracy:  0.903849720954895\n",
      "total vocab 33351\n",
      "total vector 999994\n",
      "Converted 24169 words (9182 misses)\n",
      "fasttext_trained\n",
      "GabHateCorpus\n",
      "cnn\n",
      "Epoch 1/8\n",
      "85/85 [==============================] - 2s 19ms/step - loss: 0.4157 - accuracy: 0.8763 - precision: 0.4864 - recall: 0.0801 - val_loss: 0.3695 - val_accuracy: 0.8864 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 2/8\n",
      "85/85 [==============================] - 1s 13ms/step - loss: 0.3661 - accuracy: 0.8744 - precision: 0.5000 - recall: 0.0018 - val_loss: 0.3248 - val_accuracy: 0.8873 - val_precision: 0.6087 - val_recall: 0.0227\n",
      "Epoch 3/8\n",
      "85/85 [==============================] - 1s 13ms/step - loss: 0.3133 - accuracy: 0.8803 - precision: 0.6441 - recall: 0.1049 - val_loss: 0.2803 - val_accuracy: 0.8934 - val_precision: 0.6284 - val_recall: 0.1507\n",
      "Epoch 4/8\n",
      "85/85 [==============================] - 1s 13ms/step - loss: 0.2856 - accuracy: 0.8901 - precision: 0.6635 - recall: 0.2530 - val_loss: 0.2660 - val_accuracy: 0.8956 - val_precision: 0.6068 - val_recall: 0.2301\n",
      "Epoch 5/8\n",
      "85/85 [==============================] - 1s 13ms/step - loss: 0.2723 - accuracy: 0.8919 - precision: 0.6586 - recall: 0.2886 - val_loss: 0.2601 - val_accuracy: 0.8970 - val_precision: 0.5924 - val_recall: 0.3015\n",
      "Epoch 6/8\n",
      "85/85 [==============================] - 1s 13ms/step - loss: 0.2641 - accuracy: 0.8953 - precision: 0.6730 - recall: 0.3238 - val_loss: 0.2556 - val_accuracy: 0.8972 - val_precision: 0.6105 - val_recall: 0.2642\n",
      "Epoch 7/8\n",
      "85/85 [==============================] - 1s 13ms/step - loss: 0.2560 - accuracy: 0.8988 - precision: 0.7027 - recall: 0.3363 - val_loss: 0.2564 - val_accuracy: 0.8994 - val_precision: 0.5937 - val_recall: 0.3647\n",
      "Epoch 8/8\n",
      "85/85 [==============================] - 1s 13ms/step - loss: 0.2514 - accuracy: 0.9021 - precision: 0.7127 - recall: 0.3693 - val_loss: 0.2504 - val_accuracy: 0.8996 - val_precision: 0.6250 - val_recall: 0.2917\n",
      "acc 0.8863510489463806\n",
      "Score:  0.2504425346851349\n",
      "Accuracy:  0.8996132016181946\n",
      "rnn\n",
      "Epoch 1/8\n",
      "85/85 [==============================] - 12s 125ms/step - loss: 0.4047 - accuracy: 0.8742 - precision: 0.4268 - recall: 0.0610 - val_loss: 0.3533 - val_accuracy: 0.8865 - val_precision: 1.0000 - val_recall: 0.0016\n",
      "Epoch 2/8\n",
      "85/85 [==============================] - 10s 120ms/step - loss: 0.3657 - accuracy: 0.8744 - precision: 0.4954 - recall: 0.0198 - val_loss: 0.3270 - val_accuracy: 0.8880 - val_precision: 0.7647 - val_recall: 0.0211\n",
      "Epoch 3/8\n",
      "85/85 [==============================] - 9s 111ms/step - loss: 0.3227 - accuracy: 0.8803 - precision: 0.5997 - recall: 0.1401 - val_loss: 0.2810 - val_accuracy: 0.8965 - val_precision: 0.6590 - val_recall: 0.1848\n",
      "Epoch 4/8\n",
      "85/85 [==============================] - 9s 110ms/step - loss: 0.2932 - accuracy: 0.8874 - precision: 0.6248 - recall: 0.2589 - val_loss: 0.2806 - val_accuracy: 0.8946 - val_precision: 0.5587 - val_recall: 0.3468\n",
      "Epoch 5/8\n",
      "85/85 [==============================] - 10s 112ms/step - loss: 0.2863 - accuracy: 0.8897 - precision: 0.6334 - recall: 0.2882 - val_loss: 0.3162 - val_accuracy: 0.8924 - val_precision: 0.7037 - val_recall: 0.0924\n",
      "Epoch 6/8\n",
      "85/85 [==============================] - 9s 111ms/step - loss: 0.2766 - accuracy: 0.8935 - precision: 0.6516 - recall: 0.3271 - val_loss: 0.2928 - val_accuracy: 0.8983 - val_precision: 0.6087 - val_recall: 0.2950\n",
      "Epoch 7/8\n",
      "85/85 [==============================] - 10s 114ms/step - loss: 0.2692 - accuracy: 0.8943 - precision: 0.6556 - recall: 0.3330 - val_loss: 0.2854 - val_accuracy: 0.8945 - val_precision: 0.5608 - val_recall: 0.3290\n",
      "Epoch 8/8\n",
      "85/85 [==============================] - 9s 110ms/step - loss: 0.2665 - accuracy: 0.8946 - precision: 0.6478 - recall: 0.3520 - val_loss: 0.2842 - val_accuracy: 0.8948 - val_precision: 0.5793 - val_recall: 0.2723\n",
      "acc 0.8865352869033813\n",
      "Score:  0.28420159220695496\n",
      "Accuracy:  0.8948240876197815\n",
      "lstm\n",
      "Epoch 1/8\n",
      "85/85 [==============================] - 7s 40ms/step - loss: 0.4175 - accuracy: 0.8725 - precision: 0.3829 - recall: 0.0577 - val_loss: 0.3551 - val_accuracy: 0.8864 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 2/8\n",
      "85/85 [==============================] - 2s 23ms/step - loss: 0.3500 - accuracy: 0.8750 - precision: 0.5297 - recall: 0.0425 - val_loss: 0.3034 - val_accuracy: 0.8941 - val_precision: 0.6875 - val_recall: 0.1248\n",
      "Epoch 3/8\n",
      "85/85 [==============================] - 2s 24ms/step - loss: 0.3076 - accuracy: 0.8837 - precision: 0.6209 - recall: 0.1892 - val_loss: 0.2830 - val_accuracy: 0.8974 - val_precision: 0.7143 - val_recall: 0.1621\n",
      "Epoch 4/8\n",
      "85/85 [==============================] - 2s 24ms/step - loss: 0.2890 - accuracy: 0.8893 - precision: 0.6371 - recall: 0.2761 - val_loss: 0.2799 - val_accuracy: 0.8981 - val_precision: 0.7623 - val_recall: 0.1507\n",
      "Epoch 5/8\n",
      "85/85 [==============================] - 2s 22ms/step - loss: 0.2874 - accuracy: 0.8892 - precision: 0.6349 - recall: 0.2761 - val_loss: 0.2647 - val_accuracy: 0.9005 - val_precision: 0.6638 - val_recall: 0.2528\n",
      "Epoch 6/8\n",
      "85/85 [==============================] - 2s 22ms/step - loss: 0.2817 - accuracy: 0.8907 - precision: 0.6379 - recall: 0.3003 - val_loss: 0.2839 - val_accuracy: 0.9005 - val_precision: 0.7584 - val_recall: 0.1831\n",
      "Epoch 7/8\n",
      "85/85 [==============================] - 2s 23ms/step - loss: 0.2724 - accuracy: 0.8954 - precision: 0.6643 - recall: 0.3374 - val_loss: 0.2591 - val_accuracy: 0.9013 - val_precision: 0.6484 - val_recall: 0.2869\n",
      "Epoch 8/8\n",
      "85/85 [==============================] - 2s 23ms/step - loss: 0.2703 - accuracy: 0.8953 - precision: 0.6684 - recall: 0.3304 - val_loss: 0.2607 - val_accuracy: 0.9024 - val_precision: 0.6951 - val_recall: 0.2512\n",
      "acc 0.8863510489463806\n",
      "Score:  0.2607216238975525\n",
      "Accuracy:  0.902376115322113\n",
      "gru\n",
      "Epoch 1/8\n",
      "85/85 [==============================] - 8s 43ms/step - loss: 0.4116 - accuracy: 0.8748 - precision: 0.4318 - recall: 0.0520 - val_loss: 0.3523 - val_accuracy: 0.8864 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 2/8\n",
      "85/85 [==============================] - 2s 24ms/step - loss: 0.3424 - accuracy: 0.8771 - precision: 0.6255 - recall: 0.0539 - val_loss: 0.3093 - val_accuracy: 0.8976 - val_precision: 0.7521 - val_recall: 0.1475\n",
      "Epoch 3/8\n",
      "85/85 [==============================] - 2s 23ms/step - loss: 0.2909 - accuracy: 0.8885 - precision: 0.6454 - recall: 0.2483 - val_loss: 0.2672 - val_accuracy: 0.9009 - val_precision: 0.6491 - val_recall: 0.2788\n",
      "Epoch 4/8\n",
      "85/85 [==============================] - 2s 23ms/step - loss: 0.2779 - accuracy: 0.8923 - precision: 0.6416 - recall: 0.3223 - val_loss: 0.2593 - val_accuracy: 0.9040 - val_precision: 0.6395 - val_recall: 0.3566\n",
      "Epoch 5/8\n",
      "85/85 [==============================] - 2s 24ms/step - loss: 0.2716 - accuracy: 0.8936 - precision: 0.6446 - recall: 0.3399 - val_loss: 0.2575 - val_accuracy: 0.9027 - val_precision: 0.6831 - val_recall: 0.2690\n",
      "Epoch 6/8\n",
      "85/85 [==============================] - 2s 24ms/step - loss: 0.2654 - accuracy: 0.8952 - precision: 0.6533 - recall: 0.3531 - val_loss: 0.2544 - val_accuracy: 0.9018 - val_precision: 0.6419 - val_recall: 0.3079\n",
      "Epoch 7/8\n",
      "85/85 [==============================] - 2s 24ms/step - loss: 0.2625 - accuracy: 0.8958 - precision: 0.6534 - recall: 0.3623 - val_loss: 0.2554 - val_accuracy: 0.9007 - val_precision: 0.6016 - val_recall: 0.3744\n",
      "Epoch 8/8\n",
      "85/85 [==============================] - 2s 24ms/step - loss: 0.2591 - accuracy: 0.8967 - precision: 0.6603 - recall: 0.3656 - val_loss: 0.2585 - val_accuracy: 0.9035 - val_precision: 0.6755 - val_recall: 0.2901\n",
      "acc 0.8863510489463806\n",
      "Score:  0.2584759294986725\n",
      "Accuracy:  0.9034813046455383\n",
      "total vocab 33351\n",
      "Total words  1917494\n",
      "Converted 27229 words (6122 misses)\n",
      "glove\n",
      "GabHateCorpus\n",
      "cnn\n",
      "Epoch 1/8\n",
      "85/85 [==============================] - 2s 19ms/step - loss: 0.3965 - accuracy: 0.8643 - precision: 0.4013 - recall: 0.2060 - val_loss: 0.2737 - val_accuracy: 0.8969 - val_precision: 0.6223 - val_recall: 0.2350\n",
      "Epoch 2/8\n",
      "85/85 [==============================] - 1s 13ms/step - loss: 0.2684 - accuracy: 0.8961 - precision: 0.6610 - recall: 0.3539 - val_loss: 0.2585 - val_accuracy: 0.9020 - val_precision: 0.6471 - val_recall: 0.3031\n",
      "Epoch 3/8\n",
      "85/85 [==============================] - 1s 13ms/step - loss: 0.2426 - accuracy: 0.9051 - precision: 0.7126 - recall: 0.4092 - val_loss: 0.2521 - val_accuracy: 0.9026 - val_precision: 0.6538 - val_recall: 0.3031\n",
      "Epoch 4/8\n",
      "85/85 [==============================] - 1s 13ms/step - loss: 0.2224 - accuracy: 0.9134 - precision: 0.7568 - recall: 0.4576 - val_loss: 0.2490 - val_accuracy: 0.9044 - val_precision: 0.6332 - val_recall: 0.3776\n",
      "Epoch 5/8\n",
      "85/85 [==============================] - 1s 13ms/step - loss: 0.2003 - accuracy: 0.9227 - precision: 0.7998 - recall: 0.5127 - val_loss: 0.2466 - val_accuracy: 0.9026 - val_precision: 0.6325 - val_recall: 0.3404\n",
      "Epoch 6/8\n",
      "85/85 [==============================] - 1s 13ms/step - loss: 0.1822 - accuracy: 0.9281 - precision: 0.8217 - recall: 0.5460 - val_loss: 0.2448 - val_accuracy: 0.9035 - val_precision: 0.6317 - val_recall: 0.3614\n",
      "Epoch 7/8\n",
      "85/85 [==============================] - 1s 13ms/step - loss: 0.1665 - accuracy: 0.9362 - precision: 0.8466 - recall: 0.6010 - val_loss: 0.2464 - val_accuracy: 0.9037 - val_precision: 0.6237 - val_recall: 0.3841\n",
      "Epoch 8/8\n",
      "85/85 [==============================] - 1s 13ms/step - loss: 0.1474 - accuracy: 0.9452 - precision: 0.8829 - recall: 0.6498 - val_loss: 0.2535 - val_accuracy: 0.9042 - val_precision: 0.6702 - val_recall: 0.3096\n",
      "acc 0.8968502283096313\n",
      "Score:  0.2534816563129425\n",
      "Accuracy:  0.9042180776596069\n",
      "rnn\n",
      "Epoch 1/8\n",
      "85/85 [==============================] - 12s 122ms/step - loss: 0.3854 - accuracy: 0.8708 - precision: 0.4246 - recall: 0.1373 - val_loss: 0.3028 - val_accuracy: 0.8893 - val_precision: 0.5556 - val_recall: 0.1297\n",
      "Epoch 2/8\n",
      "85/85 [==============================] - 10s 117ms/step - loss: 0.3069 - accuracy: 0.8783 - precision: 0.5321 - recall: 0.2582 - val_loss: 0.3257 - val_accuracy: 0.8865 - val_precision: 0.5294 - val_recall: 0.0146\n",
      "Epoch 3/8\n",
      "85/85 [==============================] - 10s 114ms/step - loss: 0.3133 - accuracy: 0.8799 - precision: 0.5624 - recall: 0.1951 - val_loss: 0.2848 - val_accuracy: 0.8895 - val_precision: 0.5603 - val_recall: 0.1280\n",
      "Epoch 4/8\n",
      "85/85 [==============================] - 10s 114ms/step - loss: 0.2772 - accuracy: 0.8863 - precision: 0.5866 - recall: 0.3190 - val_loss: 0.3009 - val_accuracy: 0.8932 - val_precision: 0.6082 - val_recall: 0.1686\n",
      "Epoch 5/8\n",
      "85/85 [==============================] - 10s 114ms/step - loss: 0.2855 - accuracy: 0.8869 - precision: 0.5994 - recall: 0.3007 - val_loss: 0.2808 - val_accuracy: 0.8860 - val_precision: 0.4958 - val_recall: 0.1929\n",
      "Epoch 6/8\n",
      "85/85 [==============================] - 10s 114ms/step - loss: 0.2611 - accuracy: 0.8941 - precision: 0.6464 - recall: 0.3465 - val_loss: 0.2922 - val_accuracy: 0.8887 - val_precision: 0.6000 - val_recall: 0.0632\n",
      "Epoch 7/8\n",
      "85/85 [==============================] - 10s 116ms/step - loss: 0.2490 - accuracy: 0.8995 - precision: 0.6681 - recall: 0.3971 - val_loss: 0.2763 - val_accuracy: 0.8865 - val_precision: 0.5011 - val_recall: 0.3712\n",
      "Epoch 8/8\n",
      "85/85 [==============================] - 10s 114ms/step - loss: 0.2438 - accuracy: 0.9038 - precision: 0.6861 - recall: 0.4320 - val_loss: 0.2860 - val_accuracy: 0.8928 - val_precision: 0.5593 - val_recall: 0.2674\n",
      "acc 0.8892982006072998\n",
      "Score:  0.2859695553779602\n",
      "Accuracy:  0.8927979469299316\n",
      "lstm\n",
      "Epoch 1/8\n",
      "85/85 [==============================] - 7s 45ms/step - loss: 0.3602 - accuracy: 0.8704 - precision: 0.4191 - recall: 0.1355 - val_loss: 0.2808 - val_accuracy: 0.8978 - val_precision: 0.6192 - val_recall: 0.2609\n",
      "Epoch 2/8\n",
      "85/85 [==============================] - 2s 24ms/step - loss: 0.2757 - accuracy: 0.8900 - precision: 0.6118 - recall: 0.3392 - val_loss: 0.2630 - val_accuracy: 0.9009 - val_precision: 0.6653 - val_recall: 0.2577\n",
      "Epoch 3/8\n",
      "85/85 [==============================] - 2s 24ms/step - loss: 0.2566 - accuracy: 0.9006 - precision: 0.6839 - recall: 0.3872 - val_loss: 0.2488 - val_accuracy: 0.9044 - val_precision: 0.6828 - val_recall: 0.2966\n",
      "Epoch 4/8\n",
      "85/85 [==============================] - 2s 25ms/step - loss: 0.2479 - accuracy: 0.9034 - precision: 0.7017 - recall: 0.4012 - val_loss: 0.2520 - val_accuracy: 0.9066 - val_precision: 0.6382 - val_recall: 0.4117\n",
      "Epoch 5/8\n",
      "85/85 [==============================] - 2s 26ms/step - loss: 0.2356 - accuracy: 0.9073 - precision: 0.7081 - recall: 0.4448 - val_loss: 0.2472 - val_accuracy: 0.9064 - val_precision: 0.6253 - val_recall: 0.4408\n",
      "Epoch 6/8\n",
      "85/85 [==============================] - 2s 26ms/step - loss: 0.2269 - accuracy: 0.9119 - precision: 0.7369 - recall: 0.4642 - val_loss: 0.2594 - val_accuracy: 0.9061 - val_precision: 0.6877 - val_recall: 0.3177\n",
      "Epoch 7/8\n",
      "85/85 [==============================] - 2s 26ms/step - loss: 0.2182 - accuracy: 0.9148 - precision: 0.7513 - recall: 0.4807 - val_loss: 0.2650 - val_accuracy: 0.8965 - val_precision: 0.5455 - val_recall: 0.5348\n",
      "Epoch 8/8\n",
      "85/85 [==============================] - 2s 26ms/step - loss: 0.2061 - accuracy: 0.9200 - precision: 0.7608 - recall: 0.5295 - val_loss: 0.2625 - val_accuracy: 0.9073 - val_precision: 0.6583 - val_recall: 0.3841\n",
      "acc 0.8977712392807007\n",
      "Score:  0.26248204708099365\n",
      "Accuracy:  0.9073494076728821\n",
      "gru\n",
      "Epoch 1/8\n",
      "85/85 [==============================] - 9s 44ms/step - loss: 0.3685 - accuracy: 0.8804 - precision: 0.5513 - recall: 0.1558 - val_loss: 0.2905 - val_accuracy: 0.8937 - val_precision: 0.6429 - val_recall: 0.1459\n",
      "Epoch 2/8\n",
      "85/85 [==============================] - 2s 24ms/step - loss: 0.2711 - accuracy: 0.8955 - precision: 0.6558 - recall: 0.3528 - val_loss: 0.2511 - val_accuracy: 0.9038 - val_precision: 0.6610 - val_recall: 0.3160\n",
      "Epoch 3/8\n",
      "85/85 [==============================] - 2s 24ms/step - loss: 0.2501 - accuracy: 0.9008 - precision: 0.6792 - recall: 0.3982 - val_loss: 0.2454 - val_accuracy: 0.9064 - val_precision: 0.6823 - val_recall: 0.3306\n",
      "Epoch 4/8\n",
      "85/85 [==============================] - 2s 23ms/step - loss: 0.2406 - accuracy: 0.9040 - precision: 0.6945 - recall: 0.4210 - val_loss: 0.2448 - val_accuracy: 0.9055 - val_precision: 0.7063 - val_recall: 0.2885\n",
      "Epoch 5/8\n",
      "85/85 [==============================] - 2s 23ms/step - loss: 0.2308 - accuracy: 0.9084 - precision: 0.7202 - recall: 0.4426 - val_loss: 0.2477 - val_accuracy: 0.9086 - val_precision: 0.6933 - val_recall: 0.3517\n",
      "Epoch 6/8\n",
      "85/85 [==============================] - 2s 24ms/step - loss: 0.2210 - accuracy: 0.9125 - precision: 0.7314 - recall: 0.4793 - val_loss: 0.2539 - val_accuracy: 0.9018 - val_precision: 0.5742 - val_recall: 0.5267\n",
      "Epoch 7/8\n",
      "85/85 [==============================] - 2s 24ms/step - loss: 0.2108 - accuracy: 0.9169 - precision: 0.7536 - recall: 0.5024 - val_loss: 0.2490 - val_accuracy: 0.9020 - val_precision: 0.5938 - val_recall: 0.4360\n",
      "Epoch 8/8\n",
      "85/85 [==============================] - 2s 24ms/step - loss: 0.1980 - accuracy: 0.9204 - precision: 0.7613 - recall: 0.5332 - val_loss: 0.2565 - val_accuracy: 0.9003 - val_precision: 0.5776 - val_recall: 0.4587\n",
      "acc 0.8937188982963562\n",
      "Score:  0.25650858879089355\n",
      "Accuracy:  0.9003499746322632\n",
      "total vocab 33351\n",
      "no_train\n",
      "GabHateCorpus\n",
      "cnn\n",
      "Epoch 1/8\n",
      "85/85 [==============================] - 3s 22ms/step - loss: 0.3830 - accuracy: 0.8712 - precision: 0.4207 - recall: 0.1214 - val_loss: 0.2892 - val_accuracy: 0.8910 - val_precision: 0.5472 - val_recall: 0.2350\n",
      "Epoch 2/8\n",
      "85/85 [==============================] - 2s 19ms/step - loss: 0.2558 - accuracy: 0.8997 - precision: 0.7003 - recall: 0.3513 - val_loss: 0.2630 - val_accuracy: 0.8974 - val_precision: 0.5758 - val_recall: 0.3695\n",
      "Epoch 3/8\n",
      "85/85 [==============================] - 2s 19ms/step - loss: 0.1751 - accuracy: 0.9307 - precision: 0.8147 - recall: 0.5805 - val_loss: 0.2824 - val_accuracy: 0.8930 - val_precision: 0.5466 - val_recall: 0.3420\n",
      "Epoch 4/8\n",
      "85/85 [==============================] - 2s 19ms/step - loss: 0.1065 - accuracy: 0.9599 - precision: 0.8835 - recall: 0.7840 - val_loss: 0.3337 - val_accuracy: 0.8838 - val_precision: 0.4850 - val_recall: 0.3679\n",
      "Epoch 5/8\n",
      "85/85 [==============================] - 2s 19ms/step - loss: 0.0576 - accuracy: 0.9812 - precision: 0.9496 - recall: 0.8981 - val_loss: 0.3953 - val_accuracy: 0.8805 - val_precision: 0.4679 - val_recall: 0.3776\n",
      "Epoch 6/8\n",
      "85/85 [==============================] - 2s 19ms/step - loss: 0.0306 - accuracy: 0.9905 - precision: 0.9755 - recall: 0.9479 - val_loss: 0.4627 - val_accuracy: 0.8808 - val_precision: 0.4677 - val_recall: 0.3517\n",
      "Epoch 7/8\n",
      "85/85 [==============================] - 2s 19ms/step - loss: 0.0177 - accuracy: 0.9958 - precision: 0.9900 - recall: 0.9765 - val_loss: 0.5286 - val_accuracy: 0.8810 - val_precision: 0.4660 - val_recall: 0.3225\n",
      "Epoch 8/8\n",
      "85/85 [==============================] - 2s 19ms/step - loss: 0.0112 - accuracy: 0.9978 - precision: 0.9959 - recall: 0.9868 - val_loss: 0.5775 - val_accuracy: 0.8746 - val_precision: 0.4360 - val_recall: 0.3533\n",
      "acc 0.8909559845924377\n",
      "Score:  0.5774862766265869\n",
      "Accuracy:  0.8745625615119934\n",
      "rnn\n",
      "Epoch 1/8\n",
      "85/85 [==============================] - 15s 155ms/step - loss: 0.2783 - accuracy: 0.8954 - precision: 0.6104 - recall: 0.4166 - val_loss: 0.3562 - val_accuracy: 0.8747 - val_precision: 0.4182 - val_recall: 0.2609\n",
      "Epoch 2/8\n",
      "85/85 [==============================] - 14s 162ms/step - loss: 0.1158 - accuracy: 0.9605 - precision: 0.8816 - recall: 0.7917 - val_loss: 0.3990 - val_accuracy: 0.8714 - val_precision: 0.4069 - val_recall: 0.2869\n",
      "Epoch 3/8\n",
      "85/85 [==============================] - 13s 154ms/step - loss: 0.0426 - accuracy: 0.9886 - precision: 0.9651 - recall: 0.9432 - val_loss: 0.4912 - val_accuracy: 0.8714 - val_precision: 0.4121 - val_recall: 0.3079\n",
      "Epoch 4/8\n",
      "85/85 [==============================] - 12s 143ms/step - loss: 0.0139 - accuracy: 0.9969 - precision: 0.9900 - recall: 0.9850 - val_loss: 0.5498 - val_accuracy: 0.8604 - val_precision: 0.3803 - val_recall: 0.3630\n",
      "Epoch 5/8\n",
      "85/85 [==============================] - 11s 133ms/step - loss: 0.0056 - accuracy: 0.9988 - precision: 0.9960 - recall: 0.9949 - val_loss: 0.6133 - val_accuracy: 0.8711 - val_precision: 0.4059 - val_recall: 0.2901\n",
      "Epoch 6/8\n",
      "85/85 [==============================] - 11s 132ms/step - loss: 0.0068 - accuracy: 0.9983 - precision: 0.9930 - recall: 0.9934 - val_loss: 0.6296 - val_accuracy: 0.8530 - val_precision: 0.3538 - val_recall: 0.3549\n",
      "Epoch 7/8\n",
      "85/85 [==============================] - 11s 132ms/step - loss: 0.0035 - accuracy: 0.9992 - precision: 0.9974 - recall: 0.9963 - val_loss: 0.7182 - val_accuracy: 0.8637 - val_precision: 0.3850 - val_recall: 0.3339\n",
      "Epoch 8/8\n",
      "85/85 [==============================] - 11s 132ms/step - loss: 0.0032 - accuracy: 0.9989 - precision: 0.9952 - recall: 0.9963 - val_loss: 0.7413 - val_accuracy: 0.8712 - val_precision: 0.4120 - val_recall: 0.3112\n",
      "acc 0.8747467398643494\n",
      "Score:  0.7412781715393066\n",
      "Accuracy:  0.8712469935417175\n",
      "lstm\n",
      "Epoch 1/8\n",
      "85/85 [==============================] - 8s 44ms/step - loss: 0.2066 - accuracy: 0.9156 - precision: 0.7231 - recall: 0.5099 - val_loss: 0.4138 - val_accuracy: 0.8722 - val_precision: 0.4306 - val_recall: 0.3874\n",
      "Epoch 2/8\n",
      "85/85 [==============================] - 2s 29ms/step - loss: 0.0546 - accuracy: 0.9819 - precision: 0.9404 - recall: 0.9142 - val_loss: 0.4629 - val_accuracy: 0.8666 - val_precision: 0.4053 - val_recall: 0.3712\n",
      "Epoch 3/8\n",
      "85/85 [==============================] - 2s 29ms/step - loss: 0.0277 - accuracy: 0.9915 - precision: 0.9721 - recall: 0.9597 - val_loss: 0.6436 - val_accuracy: 0.8674 - val_precision: 0.3984 - val_recall: 0.3274\n",
      "Epoch 4/8\n",
      "85/85 [==============================] - 2s 29ms/step - loss: 0.0151 - accuracy: 0.9956 - precision: 0.9870 - recall: 0.9780 - val_loss: 0.7721 - val_accuracy: 0.8473 - val_precision: 0.3579 - val_recall: 0.4327\n",
      "Epoch 5/8\n",
      "85/85 [==============================] - 2s 29ms/step - loss: 0.0119 - accuracy: 0.9967 - precision: 0.9879 - recall: 0.9857 - val_loss: 0.7949 - val_accuracy: 0.8654 - val_precision: 0.3982 - val_recall: 0.3614\n",
      "Epoch 6/8\n",
      "85/85 [==============================] - 3s 30ms/step - loss: 0.0079 - accuracy: 0.9981 - precision: 0.9934 - recall: 0.9912 - val_loss: 0.8829 - val_accuracy: 0.8661 - val_precision: 0.3978 - val_recall: 0.3468\n",
      "Epoch 7/8\n",
      "85/85 [==============================] - 2s 29ms/step - loss: 0.0067 - accuracy: 0.9986 - precision: 0.9963 - recall: 0.9927 - val_loss: 0.8894 - val_accuracy: 0.8626 - val_precision: 0.3834 - val_recall: 0.3436\n",
      "Epoch 8/8\n",
      "85/85 [==============================] - 3s 30ms/step - loss: 0.0094 - accuracy: 0.9972 - precision: 0.9908 - recall: 0.9872 - val_loss: 0.9672 - val_accuracy: 0.8654 - val_precision: 0.3944 - val_recall: 0.3452\n",
      "acc 0.8721680045127869\n",
      "Score:  0.96717369556427\n",
      "Accuracy:  0.8653527498245239\n",
      "gru\n",
      "Epoch 1/8\n",
      "85/85 [==============================] - 8s 46ms/step - loss: 0.1848 - accuracy: 0.9203 - precision: 0.7285 - recall: 0.5625 - val_loss: 0.5035 - val_accuracy: 0.8626 - val_precision: 0.3955 - val_recall: 0.3955\n",
      "Epoch 2/8\n",
      "85/85 [==============================] - 2s 27ms/step - loss: 0.0236 - accuracy: 0.9932 - precision: 0.9788 - recall: 0.9670 - val_loss: 0.7173 - val_accuracy: 0.8646 - val_precision: 0.3907 - val_recall: 0.3420\n",
      "Epoch 3/8\n",
      "85/85 [==============================] - 2s 27ms/step - loss: 0.0082 - accuracy: 0.9978 - precision: 0.9937 - recall: 0.9886 - val_loss: 0.8263 - val_accuracy: 0.8576 - val_precision: 0.3682 - val_recall: 0.3533\n",
      "Epoch 4/8\n",
      "85/85 [==============================] - 2s 28ms/step - loss: 0.0044 - accuracy: 0.9989 - precision: 0.9974 - recall: 0.9941 - val_loss: 0.9519 - val_accuracy: 0.8563 - val_precision: 0.3675 - val_recall: 0.3663\n",
      "Epoch 5/8\n",
      "85/85 [==============================] - 2s 28ms/step - loss: 0.0037 - accuracy: 0.9990 - precision: 0.9974 - recall: 0.9945 - val_loss: 0.9633 - val_accuracy: 0.8624 - val_precision: 0.3750 - val_recall: 0.3160\n",
      "Epoch 6/8\n",
      "85/85 [==============================] - 2s 28ms/step - loss: 0.0029 - accuracy: 0.9993 - precision: 0.9985 - recall: 0.9956 - val_loss: 1.0797 - val_accuracy: 0.8681 - val_precision: 0.3967 - val_recall: 0.3079\n",
      "Epoch 7/8\n",
      "85/85 [==============================] - 2s 28ms/step - loss: 0.0045 - accuracy: 0.9987 - precision: 0.9949 - recall: 0.9945 - val_loss: 0.9449 - val_accuracy: 0.8506 - val_precision: 0.3450 - val_recall: 0.3501\n",
      "Epoch 8/8\n",
      "85/85 [==============================] - 2s 27ms/step - loss: 0.0067 - accuracy: 0.9980 - precision: 0.9937 - recall: 0.9901 - val_loss: 1.0545 - val_accuracy: 0.8606 - val_precision: 0.3793 - val_recall: 0.3566\n",
      "acc 0.8625897765159607\n",
      "Score:  1.0545376539230347\n",
      "Accuracy:  0.8605636358261108\n"
     ]
    }
   ],
   "source": [
    "df_result = get_result_table()\n",
    "# train gab dataset\n",
    "df, dataset_name = load_dataset(1) \n",
    "train_dataset_all_model(df, dataset_name, df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>hate f1</th>\n",
       "      <th>non-hate f1</th>\n",
       "      <th>hate support</th>\n",
       "      <th>non-hate support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GabHateCorpus_word2vec_trained_cnn</td>\n",
       "      <td>0.903666</td>\n",
       "      <td>0.777090</td>\n",
       "      <td>0.669429</td>\n",
       "      <td>0.705335</td>\n",
       "      <td>0.463590</td>\n",
       "      <td>0.947081</td>\n",
       "      <td>617.0</td>\n",
       "      <td>4812.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GabHateCorpus_word2vec_trained_rnn</td>\n",
       "      <td>0.893351</td>\n",
       "      <td>0.733768</td>\n",
       "      <td>0.669968</td>\n",
       "      <td>0.694509</td>\n",
       "      <td>0.448046</td>\n",
       "      <td>0.940973</td>\n",
       "      <td>617.0</td>\n",
       "      <td>4812.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GabHateCorpus_word2vec_trained_lstm</td>\n",
       "      <td>0.902745</td>\n",
       "      <td>0.772704</td>\n",
       "      <td>0.668909</td>\n",
       "      <td>0.703886</td>\n",
       "      <td>0.461224</td>\n",
       "      <td>0.946548</td>\n",
       "      <td>617.0</td>\n",
       "      <td>4812.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GabHateCorpus_word2vec_trained_gru</td>\n",
       "      <td>0.903850</td>\n",
       "      <td>0.773624</td>\n",
       "      <td>0.681542</td>\n",
       "      <td>0.714574</td>\n",
       "      <td>0.482143</td>\n",
       "      <td>0.947005</td>\n",
       "      <td>617.0</td>\n",
       "      <td>4812.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GabHateCorpus_fasttext_trained_cnn</td>\n",
       "      <td>0.899613</td>\n",
       "      <td>0.769999</td>\n",
       "      <td>0.634645</td>\n",
       "      <td>0.671516</td>\n",
       "      <td>0.397790</td>\n",
       "      <td>0.945243</td>\n",
       "      <td>617.0</td>\n",
       "      <td>4812.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GabHateCorpus_fasttext_trained_rnn</td>\n",
       "      <td>0.894824</td>\n",
       "      <td>0.745970</td>\n",
       "      <td>0.623466</td>\n",
       "      <td>0.656535</td>\n",
       "      <td>0.370452</td>\n",
       "      <td>0.942619</td>\n",
       "      <td>617.0</td>\n",
       "      <td>4812.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GabHateCorpus_fasttext_trained_lstm</td>\n",
       "      <td>0.902376</td>\n",
       "      <td>0.803162</td>\n",
       "      <td>0.618542</td>\n",
       "      <td>0.658071</td>\n",
       "      <td>0.369048</td>\n",
       "      <td>0.947095</td>\n",
       "      <td>617.0</td>\n",
       "      <td>4812.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GabHateCorpus_fasttext_trained_gru</td>\n",
       "      <td>0.903481</td>\n",
       "      <td>0.795327</td>\n",
       "      <td>0.636121</td>\n",
       "      <td>0.676685</td>\n",
       "      <td>0.405896</td>\n",
       "      <td>0.947474</td>\n",
       "      <td>617.0</td>\n",
       "      <td>4812.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GabHateCorpus_glove_cnn</td>\n",
       "      <td>0.904218</td>\n",
       "      <td>0.793680</td>\n",
       "      <td>0.645014</td>\n",
       "      <td>0.685637</td>\n",
       "      <td>0.423503</td>\n",
       "      <td>0.947770</td>\n",
       "      <td>617.0</td>\n",
       "      <td>4812.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GabHateCorpus_glove_rnn</td>\n",
       "      <td>0.892798</td>\n",
       "      <td>0.735641</td>\n",
       "      <td>0.620204</td>\n",
       "      <td>0.651663</td>\n",
       "      <td>0.361842</td>\n",
       "      <td>0.941484</td>\n",
       "      <td>617.0</td>\n",
       "      <td>4812.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GabHateCorpus_glove_lstm</td>\n",
       "      <td>0.907349</td>\n",
       "      <td>0.791684</td>\n",
       "      <td>0.679278</td>\n",
       "      <td>0.717126</td>\n",
       "      <td>0.485159</td>\n",
       "      <td>0.949094</td>\n",
       "      <td>617.0</td>\n",
       "      <td>4812.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>GabHateCorpus_glove_gru</td>\n",
       "      <td>0.900350</td>\n",
       "      <td>0.754963</td>\n",
       "      <td>0.707827</td>\n",
       "      <td>0.727905</td>\n",
       "      <td>0.511292</td>\n",
       "      <td>0.944519</td>\n",
       "      <td>617.0</td>\n",
       "      <td>4812.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>GabHateCorpus_no_train_cnn</td>\n",
       "      <td>0.874563</td>\n",
       "      <td>0.677525</td>\n",
       "      <td>0.647360</td>\n",
       "      <td>0.660210</td>\n",
       "      <td>0.390331</td>\n",
       "      <td>0.930089</td>\n",
       "      <td>617.0</td>\n",
       "      <td>4812.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>GabHateCorpus_no_train_rnn</td>\n",
       "      <td>0.871247</td>\n",
       "      <td>0.663192</td>\n",
       "      <td>0.627121</td>\n",
       "      <td>0.641531</td>\n",
       "      <td>0.354571</td>\n",
       "      <td>0.928491</td>\n",
       "      <td>617.0</td>\n",
       "      <td>4812.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>GabHateCorpus_no_train_lstm</td>\n",
       "      <td>0.865353</td>\n",
       "      <td>0.655905</td>\n",
       "      <td>0.638632</td>\n",
       "      <td>0.646420</td>\n",
       "      <td>0.368194</td>\n",
       "      <td>0.924647</td>\n",
       "      <td>617.0</td>\n",
       "      <td>4812.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>GabHateCorpus_no_train_gru</td>\n",
       "      <td>0.860564</td>\n",
       "      <td>0.648719</td>\n",
       "      <td>0.640876</td>\n",
       "      <td>0.644615</td>\n",
       "      <td>0.367586</td>\n",
       "      <td>0.921644</td>\n",
       "      <td>617.0</td>\n",
       "      <td>4812.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Model  Accuracy  precision    recall  \\\n",
       "0    GabHateCorpus_word2vec_trained_cnn  0.903666   0.777090  0.669429   \n",
       "1    GabHateCorpus_word2vec_trained_rnn  0.893351   0.733768  0.669968   \n",
       "2   GabHateCorpus_word2vec_trained_lstm  0.902745   0.772704  0.668909   \n",
       "3    GabHateCorpus_word2vec_trained_gru  0.903850   0.773624  0.681542   \n",
       "4    GabHateCorpus_fasttext_trained_cnn  0.899613   0.769999  0.634645   \n",
       "5    GabHateCorpus_fasttext_trained_rnn  0.894824   0.745970  0.623466   \n",
       "6   GabHateCorpus_fasttext_trained_lstm  0.902376   0.803162  0.618542   \n",
       "7    GabHateCorpus_fasttext_trained_gru  0.903481   0.795327  0.636121   \n",
       "8               GabHateCorpus_glove_cnn  0.904218   0.793680  0.645014   \n",
       "9               GabHateCorpus_glove_rnn  0.892798   0.735641  0.620204   \n",
       "10             GabHateCorpus_glove_lstm  0.907349   0.791684  0.679278   \n",
       "11              GabHateCorpus_glove_gru  0.900350   0.754963  0.707827   \n",
       "12           GabHateCorpus_no_train_cnn  0.874563   0.677525  0.647360   \n",
       "13           GabHateCorpus_no_train_rnn  0.871247   0.663192  0.627121   \n",
       "14          GabHateCorpus_no_train_lstm  0.865353   0.655905  0.638632   \n",
       "15           GabHateCorpus_no_train_gru  0.860564   0.648719  0.640876   \n",
       "\n",
       "    f1-score   hate f1  non-hate f1  hate support  non-hate support  \n",
       "0   0.705335  0.463590     0.947081         617.0            4812.0  \n",
       "1   0.694509  0.448046     0.940973         617.0            4812.0  \n",
       "2   0.703886  0.461224     0.946548         617.0            4812.0  \n",
       "3   0.714574  0.482143     0.947005         617.0            4812.0  \n",
       "4   0.671516  0.397790     0.945243         617.0            4812.0  \n",
       "5   0.656535  0.370452     0.942619         617.0            4812.0  \n",
       "6   0.658071  0.369048     0.947095         617.0            4812.0  \n",
       "7   0.676685  0.405896     0.947474         617.0            4812.0  \n",
       "8   0.685637  0.423503     0.947770         617.0            4812.0  \n",
       "9   0.651663  0.361842     0.941484         617.0            4812.0  \n",
       "10  0.717126  0.485159     0.949094         617.0            4812.0  \n",
       "11  0.727905  0.511292     0.944519         617.0            4812.0  \n",
       "12  0.660210  0.390331     0.930089         617.0            4812.0  \n",
       "13  0.641531  0.354571     0.928491         617.0            4812.0  \n",
       "14  0.646420  0.368194     0.924647         617.0            4812.0  \n",
       "15  0.644615  0.367586     0.921644         617.0            4812.0  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "Non-Hate    0.618726\n",
      "Hate        0.381274\n",
      "Name: proportion, dtype: float64\n",
      "Train Set : (17182,) (17182,)\n",
      "Test Set  : (4296,) (4296,)\n",
      "Total  21478\n",
      "Implicit_hate_corpus\n",
      "total vocab 20616\n",
      "total vector 3000000\n",
      "Converted 15542 words (5074 misses)\n",
      "word2vec_trained\n",
      "Implicit_hate_corpus\n",
      "cnn\n",
      "Epoch 1/8\n",
      "68/68 [==============================] - 2s 22ms/step - loss: 0.6036 - accuracy: 0.7200 - precision: 0.6148 - recall: 0.3147 - val_loss: 0.5601 - val_accuracy: 0.7160 - val_precision: 0.6537 - val_recall: 0.5368\n",
      "Epoch 2/8\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.5312 - accuracy: 0.7378 - precision: 0.7070 - recall: 0.5344 - val_loss: 0.5403 - val_accuracy: 0.7314 - val_precision: 0.7371 - val_recall: 0.4553\n",
      "Epoch 3/8\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.4988 - accuracy: 0.7592 - precision: 0.7325 - recall: 0.5812 - val_loss: 0.5269 - val_accuracy: 0.7384 - val_precision: 0.7026 - val_recall: 0.5398\n",
      "Epoch 4/8\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.4704 - accuracy: 0.7802 - precision: 0.7561 - recall: 0.6260 - val_loss: 0.5247 - val_accuracy: 0.7416 - val_precision: 0.7238 - val_recall: 0.5172\n",
      "Epoch 5/8\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.4487 - accuracy: 0.7945 - precision: 0.7733 - recall: 0.6529 - val_loss: 0.5212 - val_accuracy: 0.7409 - val_precision: 0.6950 - val_recall: 0.5668\n",
      "Epoch 6/8\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.4252 - accuracy: 0.8082 - precision: 0.7895 - recall: 0.6784 - val_loss: 0.5351 - val_accuracy: 0.7346 - val_precision: 0.7568 - val_recall: 0.4442\n",
      "Epoch 7/8\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.4013 - accuracy: 0.8232 - precision: 0.8091 - recall: 0.7026 - val_loss: 0.5195 - val_accuracy: 0.7453 - val_precision: 0.7063 - val_recall: 0.5643\n",
      "Epoch 8/8\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.3821 - accuracy: 0.8369 - precision: 0.8262 - recall: 0.7250 - val_loss: 0.5222 - val_accuracy: 0.7474 - val_precision: 0.6822 - val_recall: 0.6275\n",
      "acc 0.7160149216651917\n",
      "Score:  0.5221508145332336\n",
      "Accuracy:  0.7474395036697388\n",
      "rnn\n",
      "Epoch 1/8\n",
      "68/68 [==============================] - 9s 116ms/step - loss: 0.6348 - accuracy: 0.6636 - precision: 0.5835 - recall: 0.4113 - val_loss: 0.6116 - val_accuracy: 0.6809 - val_precision: 0.6805 - val_recall: 0.3015\n",
      "Epoch 2/8\n",
      "68/68 [==============================] - 7s 108ms/step - loss: 0.5738 - accuracy: 0.7050 - precision: 0.6442 - recall: 0.5069 - val_loss: 0.6048 - val_accuracy: 0.6881 - val_precision: 0.7045 - val_recall: 0.3082\n",
      "Epoch 3/8\n",
      "68/68 [==============================] - 8s 114ms/step - loss: 0.5518 - accuracy: 0.7187 - precision: 0.6593 - recall: 0.5438 - val_loss: 0.5862 - val_accuracy: 0.6953 - val_precision: 0.6115 - val_recall: 0.5429\n",
      "Epoch 4/8\n",
      "68/68 [==============================] - 7s 110ms/step - loss: 0.5305 - accuracy: 0.7328 - precision: 0.6782 - recall: 0.5705 - val_loss: 0.5868 - val_accuracy: 0.6946 - val_precision: 0.6354 - val_recall: 0.4602\n",
      "Epoch 5/8\n",
      "68/68 [==============================] - 7s 108ms/step - loss: 0.5150 - accuracy: 0.7455 - precision: 0.6962 - recall: 0.5910 - val_loss: 0.5939 - val_accuracy: 0.6895 - val_precision: 0.6254 - val_recall: 0.4553\n",
      "Epoch 6/8\n",
      "68/68 [==============================] - 8s 112ms/step - loss: 0.5003 - accuracy: 0.7567 - precision: 0.7153 - recall: 0.6023 - val_loss: 0.6077 - val_accuracy: 0.6767 - val_precision: 0.5716 - val_recall: 0.5944\n",
      "Epoch 7/8\n",
      "68/68 [==============================] - 8s 112ms/step - loss: 0.4822 - accuracy: 0.7670 - precision: 0.7191 - recall: 0.6392 - val_loss: 0.6189 - val_accuracy: 0.6792 - val_precision: 0.5948 - val_recall: 0.4884\n",
      "Epoch 8/8\n",
      "68/68 [==============================] - 8s 113ms/step - loss: 0.4595 - accuracy: 0.7863 - precision: 0.7504 - recall: 0.6593 - val_loss: 0.6316 - val_accuracy: 0.6727 - val_precision: 0.5779 - val_recall: 0.5135\n",
      "acc 0.6808659434318542\n",
      "Score:  0.6316041350364685\n",
      "Accuracy:  0.6727188229560852\n",
      "lstm\n",
      "Epoch 1/8\n",
      "68/68 [==============================] - 7s 44ms/step - loss: 0.6035 - accuracy: 0.6720 - precision: 0.6144 - recall: 0.3751 - val_loss: 0.5790 - val_accuracy: 0.6953 - val_precision: 0.6977 - val_recall: 0.3493\n",
      "Epoch 2/8\n",
      "68/68 [==============================] - 2s 23ms/step - loss: 0.5564 - accuracy: 0.7132 - precision: 0.6671 - recall: 0.4958 - val_loss: 0.5545 - val_accuracy: 0.7097 - val_precision: 0.6720 - val_recall: 0.4608\n",
      "Epoch 3/8\n",
      "68/68 [==============================] - 1s 22ms/step - loss: 0.5399 - accuracy: 0.7280 - precision: 0.6870 - recall: 0.5278 - val_loss: 0.5495 - val_accuracy: 0.7155 - val_precision: 0.6830 - val_recall: 0.4688\n",
      "Epoch 4/8\n",
      "68/68 [==============================] - 2s 22ms/step - loss: 0.5228 - accuracy: 0.7391 - precision: 0.7028 - recall: 0.5484 - val_loss: 0.5464 - val_accuracy: 0.7158 - val_precision: 0.6640 - val_recall: 0.5098\n",
      "Epoch 5/8\n",
      "68/68 [==============================] - 2s 22ms/step - loss: 0.5135 - accuracy: 0.7449 - precision: 0.7042 - recall: 0.5716 - val_loss: 0.5369 - val_accuracy: 0.7251 - val_precision: 0.6755 - val_recall: 0.5319\n",
      "Epoch 6/8\n",
      "68/68 [==============================] - 2s 23ms/step - loss: 0.4997 - accuracy: 0.7545 - precision: 0.7179 - recall: 0.5873 - val_loss: 0.5373 - val_accuracy: 0.7235 - val_precision: 0.6677 - val_recall: 0.5417\n",
      "Epoch 7/8\n",
      "68/68 [==============================] - 1s 22ms/step - loss: 0.4890 - accuracy: 0.7628 - precision: 0.7319 - recall: 0.5974 - val_loss: 0.5439 - val_accuracy: 0.7246 - val_precision: 0.6806 - val_recall: 0.5184\n",
      "Epoch 8/8\n",
      "68/68 [==============================] - 2s 22ms/step - loss: 0.4760 - accuracy: 0.7716 - precision: 0.7458 - recall: 0.6090 - val_loss: 0.5357 - val_accuracy: 0.7288 - val_precision: 0.6435 - val_recall: 0.6415\n",
      "acc 0.6952979564666748\n",
      "Score:  0.5356711745262146\n",
      "Accuracy:  0.7288175225257874\n",
      "gru\n",
      "Epoch 1/8\n",
      "68/68 [==============================] - 7s 45ms/step - loss: 0.6202 - accuracy: 0.6705 - precision: 0.6288 - recall: 0.3312 - val_loss: 0.6473 - val_accuracy: 0.6546 - val_precision: 0.5314 - val_recall: 0.7684\n",
      "Epoch 2/8\n",
      "68/68 [==============================] - 2s 22ms/step - loss: 0.5674 - accuracy: 0.7054 - precision: 0.6492 - recall: 0.4964 - val_loss: 0.5566 - val_accuracy: 0.7090 - val_precision: 0.6419 - val_recall: 0.5294\n",
      "Epoch 3/8\n",
      "68/68 [==============================] - 1s 20ms/step - loss: 0.5392 - accuracy: 0.7261 - precision: 0.6786 - recall: 0.5364 - val_loss: 0.5552 - val_accuracy: 0.7165 - val_precision: 0.6794 - val_recall: 0.4804\n",
      "Epoch 4/8\n",
      "68/68 [==============================] - 1s 21ms/step - loss: 0.5353 - accuracy: 0.7341 - precision: 0.6880 - recall: 0.5550 - val_loss: 0.5480 - val_accuracy: 0.7116 - val_precision: 0.6265 - val_recall: 0.5962\n",
      "Epoch 5/8\n",
      "68/68 [==============================] - 1s 22ms/step - loss: 0.5190 - accuracy: 0.7432 - precision: 0.6967 - recall: 0.5791 - val_loss: 0.5438 - val_accuracy: 0.7183 - val_precision: 0.6976 - val_recall: 0.4565\n",
      "Epoch 6/8\n",
      "68/68 [==============================] - 1s 21ms/step - loss: 0.5105 - accuracy: 0.7482 - precision: 0.7057 - recall: 0.5833 - val_loss: 0.5445 - val_accuracy: 0.7221 - val_precision: 0.7168 - val_recall: 0.4436\n",
      "Epoch 7/8\n",
      "68/68 [==============================] - 1s 21ms/step - loss: 0.5058 - accuracy: 0.7508 - precision: 0.7103 - recall: 0.5862 - val_loss: 0.5347 - val_accuracy: 0.7267 - val_precision: 0.6669 - val_recall: 0.5607\n",
      "Epoch 8/8\n",
      "68/68 [==============================] - 1s 21ms/step - loss: 0.4924 - accuracy: 0.7606 - precision: 0.7227 - recall: 0.6048 - val_loss: 0.5430 - val_accuracy: 0.7267 - val_precision: 0.7144 - val_recall: 0.4675\n",
      "acc 0.6545623540878296\n",
      "Score:  0.5429683923721313\n",
      "Accuracy:  0.7267225384712219\n",
      "total vocab 20616\n",
      "total vector 999994\n",
      "Converted 16317 words (4299 misses)\n",
      "fasttext_trained\n",
      "Implicit_hate_corpus\n",
      "cnn\n",
      "Epoch 1/8\n",
      "68/68 [==============================] - 2s 19ms/step - loss: 0.6598 - accuracy: 0.6421 - precision: 0.6933 - recall: 0.1101 - val_loss: 0.6416 - val_accuracy: 0.6331 - val_precision: 0.6892 - val_recall: 0.0625\n",
      "Epoch 2/8\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.6227 - accuracy: 0.6547 - precision: 0.7066 - recall: 0.1627 - val_loss: 0.6076 - val_accuracy: 0.6555 - val_precision: 0.7235 - val_recall: 0.1507\n",
      "Epoch 3/8\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 0.5880 - accuracy: 0.6879 - precision: 0.6940 - recall: 0.3258 - val_loss: 0.5825 - val_accuracy: 0.6839 - val_precision: 0.7322 - val_recall: 0.2647\n",
      "Epoch 4/8\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.5609 - accuracy: 0.7139 - precision: 0.7142 - recall: 0.4174 - val_loss: 0.5655 - val_accuracy: 0.7172 - val_precision: 0.6897 - val_recall: 0.4645\n",
      "Epoch 5/8\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.5415 - accuracy: 0.7272 - precision: 0.7139 - recall: 0.4760 - val_loss: 0.5547 - val_accuracy: 0.7169 - val_precision: 0.7008 - val_recall: 0.4449\n",
      "Epoch 6/8\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 0.5266 - accuracy: 0.7415 - precision: 0.7204 - recall: 0.5274 - val_loss: 0.5469 - val_accuracy: 0.7253 - val_precision: 0.6922 - val_recall: 0.4988\n",
      "Epoch 7/8\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.5137 - accuracy: 0.7510 - precision: 0.7328 - recall: 0.5467 - val_loss: 0.5424 - val_accuracy: 0.7279 - val_precision: 0.7054 - val_recall: 0.4871\n",
      "Epoch 8/8\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.5029 - accuracy: 0.7576 - precision: 0.7366 - recall: 0.5678 - val_loss: 0.5416 - val_accuracy: 0.7267 - val_precision: 0.7193 - val_recall: 0.4602\n",
      "acc 0.633147120475769\n",
      "Score:  0.5416138172149658\n",
      "Accuracy:  0.7267225384712219\n",
      "rnn\n",
      "Epoch 1/8\n",
      "68/68 [==============================] - 10s 123ms/step - loss: 0.6484 - accuracy: 0.6482 - precision: 0.5876 - recall: 0.2591 - val_loss: 0.6128 - val_accuracy: 0.6595 - val_precision: 0.5655 - val_recall: 0.4473\n",
      "Epoch 2/8\n",
      "68/68 [==============================] - 8s 118ms/step - loss: 0.6004 - accuracy: 0.6817 - precision: 0.6262 - recall: 0.4116 - val_loss: 0.6253 - val_accuracy: 0.6567 - val_precision: 0.5375 - val_recall: 0.6900\n",
      "Epoch 3/8\n",
      "68/68 [==============================] - 8s 117ms/step - loss: 0.5805 - accuracy: 0.7005 - precision: 0.6500 - recall: 0.4662 - val_loss: 0.5885 - val_accuracy: 0.6860 - val_precision: 0.7024 - val_recall: 0.3009\n",
      "Epoch 4/8\n",
      "68/68 [==============================] - 8s 114ms/step - loss: 0.5658 - accuracy: 0.7106 - precision: 0.6708 - recall: 0.4743 - val_loss: 0.5832 - val_accuracy: 0.6962 - val_precision: 0.6186 - val_recall: 0.5227\n",
      "Epoch 5/8\n",
      "68/68 [==============================] - 7s 109ms/step - loss: 0.5506 - accuracy: 0.7201 - precision: 0.6810 - recall: 0.5013 - val_loss: 0.5810 - val_accuracy: 0.6981 - val_precision: 0.6245 - val_recall: 0.5147\n",
      "Epoch 6/8\n",
      "68/68 [==============================] - 7s 109ms/step - loss: 0.5451 - accuracy: 0.7291 - precision: 0.6900 - recall: 0.5271 - val_loss: 0.5844 - val_accuracy: 0.6988 - val_precision: 0.6786 - val_recall: 0.3934\n",
      "Epoch 7/8\n",
      "68/68 [==============================] - 8s 112ms/step - loss: 0.5369 - accuracy: 0.7341 - precision: 0.6991 - recall: 0.5326 - val_loss: 0.5855 - val_accuracy: 0.6923 - val_precision: 0.6085 - val_recall: 0.5325\n",
      "Epoch 8/8\n",
      "68/68 [==============================] - 7s 110ms/step - loss: 0.5349 - accuracy: 0.7365 - precision: 0.7009 - recall: 0.5400 - val_loss: 0.5897 - val_accuracy: 0.6946 - val_precision: 0.6276 - val_recall: 0.4822\n",
      "acc 0.6594506502151489\n",
      "Score:  0.5897368788719177\n",
      "Accuracy:  0.6945996284484863\n",
      "lstm\n",
      "Epoch 1/8\n",
      "68/68 [==============================] - 8s 63ms/step - loss: 0.6519 - accuracy: 0.6431 - precision: 0.6143 - recall: 0.1716 - val_loss: 0.6273 - val_accuracy: 0.6569 - val_precision: 0.5539 - val_recall: 0.4975\n",
      "Epoch 2/8\n",
      "68/68 [==============================] - 2s 25ms/step - loss: 0.5971 - accuracy: 0.6844 - precision: 0.6364 - recall: 0.4038 - val_loss: 0.5881 - val_accuracy: 0.6788 - val_precision: 0.6909 - val_recall: 0.2794\n",
      "Epoch 3/8\n",
      "68/68 [==============================] - 2s 23ms/step - loss: 0.5819 - accuracy: 0.6979 - precision: 0.6520 - recall: 0.4467 - val_loss: 0.5887 - val_accuracy: 0.6799 - val_precision: 0.6962 - val_recall: 0.2794\n",
      "Epoch 4/8\n",
      "68/68 [==============================] - 2s 23ms/step - loss: 0.5675 - accuracy: 0.7080 - precision: 0.6690 - recall: 0.4648 - val_loss: 0.5803 - val_accuracy: 0.6916 - val_precision: 0.6946 - val_recall: 0.3358\n",
      "Epoch 5/8\n",
      "68/68 [==============================] - 2s 24ms/step - loss: 0.5570 - accuracy: 0.7160 - precision: 0.6786 - recall: 0.4862 - val_loss: 0.5759 - val_accuracy: 0.7004 - val_precision: 0.6177 - val_recall: 0.5545\n",
      "Epoch 6/8\n",
      "68/68 [==============================] - 2s 23ms/step - loss: 0.5525 - accuracy: 0.7163 - precision: 0.6762 - recall: 0.4925 - val_loss: 0.5697 - val_accuracy: 0.7030 - val_precision: 0.6294 - val_recall: 0.5306\n",
      "Epoch 7/8\n",
      "68/68 [==============================] - 2s 23ms/step - loss: 0.5467 - accuracy: 0.7208 - precision: 0.6794 - recall: 0.5080 - val_loss: 0.5657 - val_accuracy: 0.7027 - val_precision: 0.6152 - val_recall: 0.5809\n",
      "Epoch 8/8\n",
      "68/68 [==============================] - 2s 24ms/step - loss: 0.5433 - accuracy: 0.7231 - precision: 0.6829 - recall: 0.5124 - val_loss: 0.5631 - val_accuracy: 0.7097 - val_precision: 0.6228 - val_recall: 0.5980\n",
      "acc 0.6568901538848877\n",
      "Score:  0.563090980052948\n",
      "Accuracy:  0.7097299695014954\n",
      "gru\n",
      "Epoch 1/8\n",
      "68/68 [==============================] - 9s 52ms/step - loss: 0.6589 - accuracy: 0.6371 - precision: 0.5992 - recall: 0.1453 - val_loss: 0.6155 - val_accuracy: 0.6618 - val_precision: 0.6238 - val_recall: 0.2763\n",
      "Epoch 2/8\n",
      "68/68 [==============================] - 2s 24ms/step - loss: 0.5926 - accuracy: 0.6833 - precision: 0.6380 - recall: 0.3935 - val_loss: 0.5789 - val_accuracy: 0.6995 - val_precision: 0.6301 - val_recall: 0.5061\n",
      "Epoch 3/8\n",
      "68/68 [==============================] - 2s 22ms/step - loss: 0.5676 - accuracy: 0.7047 - precision: 0.6593 - recall: 0.4684 - val_loss: 0.5837 - val_accuracy: 0.6834 - val_precision: 0.7179 - val_recall: 0.2745\n",
      "Epoch 4/8\n",
      "68/68 [==============================] - 1s 22ms/step - loss: 0.5594 - accuracy: 0.7156 - precision: 0.6757 - recall: 0.4897 - val_loss: 0.5672 - val_accuracy: 0.7090 - val_precision: 0.6568 - val_recall: 0.4902\n",
      "Epoch 5/8\n",
      "68/68 [==============================] - 1s 22ms/step - loss: 0.5563 - accuracy: 0.7191 - precision: 0.6820 - recall: 0.4946 - val_loss: 0.5602 - val_accuracy: 0.7102 - val_precision: 0.6577 - val_recall: 0.4945\n",
      "Epoch 6/8\n",
      "68/68 [==============================] - 1s 22ms/step - loss: 0.5454 - accuracy: 0.7244 - precision: 0.6816 - recall: 0.5211 - val_loss: 0.5602 - val_accuracy: 0.7081 - val_precision: 0.6871 - val_recall: 0.4252\n",
      "Epoch 7/8\n",
      "68/68 [==============================] - 2s 22ms/step - loss: 0.5451 - accuracy: 0.7233 - precision: 0.6756 - recall: 0.5286 - val_loss: 0.5547 - val_accuracy: 0.7137 - val_precision: 0.6751 - val_recall: 0.4749\n",
      "Epoch 8/8\n",
      "68/68 [==============================] - 1s 22ms/step - loss: 0.5423 - accuracy: 0.7265 - precision: 0.6787 - recall: 0.5382 - val_loss: 0.5564 - val_accuracy: 0.7097 - val_precision: 0.6871 - val_recall: 0.4332\n",
      "acc 0.6617783904075623\n",
      "Score:  0.5564103722572327\n",
      "Accuracy:  0.7097299695014954\n",
      "total vocab 20616\n",
      "Total words  1917494\n",
      "Converted 17766 words (2850 misses)\n",
      "glove\n",
      "Implicit_hate_corpus\n",
      "cnn\n",
      "Epoch 1/8\n",
      "68/68 [==============================] - 3s 28ms/step - loss: 0.6183 - accuracy: 0.6738 - precision: 0.5992 - recall: 0.4364 - val_loss: 0.5486 - val_accuracy: 0.7223 - val_precision: 0.6897 - val_recall: 0.4890\n",
      "Epoch 2/8\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.5150 - accuracy: 0.7490 - precision: 0.7084 - recall: 0.5817 - val_loss: 0.5393 - val_accuracy: 0.7288 - val_precision: 0.7291 - val_recall: 0.4553\n",
      "Epoch 3/8\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.4732 - accuracy: 0.7760 - precision: 0.7496 - recall: 0.6204 - val_loss: 0.5299 - val_accuracy: 0.7325 - val_precision: 0.6924 - val_recall: 0.5325\n",
      "Epoch 4/8\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.4386 - accuracy: 0.7972 - precision: 0.7747 - recall: 0.6608 - val_loss: 0.5298 - val_accuracy: 0.7309 - val_precision: 0.6446 - val_recall: 0.6501\n",
      "Epoch 5/8\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.4074 - accuracy: 0.8199 - precision: 0.7971 - recall: 0.7083 - val_loss: 0.5221 - val_accuracy: 0.7402 - val_precision: 0.7071 - val_recall: 0.5398\n",
      "Epoch 6/8\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.3751 - accuracy: 0.8391 - precision: 0.8270 - recall: 0.7314 - val_loss: 0.5429 - val_accuracy: 0.7346 - val_precision: 0.7421 - val_recall: 0.4620\n",
      "Epoch 7/8\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.3533 - accuracy: 0.8514 - precision: 0.8399 - recall: 0.7545 - val_loss: 0.5449 - val_accuracy: 0.7358 - val_precision: 0.7401 - val_recall: 0.4694\n",
      "Epoch 8/8\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.3267 - accuracy: 0.8679 - precision: 0.8625 - recall: 0.7779 - val_loss: 0.5376 - val_accuracy: 0.7386 - val_precision: 0.7067 - val_recall: 0.5331\n",
      "acc 0.7222998142242432\n",
      "Score:  0.5376251339912415\n",
      "Accuracy:  0.7385940551757812\n",
      "rnn\n",
      "Epoch 1/8\n",
      "68/68 [==============================] - 9s 120ms/step - loss: 0.6578 - accuracy: 0.6487 - precision: 0.5568 - recall: 0.3850 - val_loss: 0.5934 - val_accuracy: 0.6725 - val_precision: 0.5715 - val_recall: 0.5509\n",
      "Epoch 2/8\n",
      "68/68 [==============================] - 8s 117ms/step - loss: 0.5724 - accuracy: 0.7027 - precision: 0.6338 - recall: 0.5230 - val_loss: 0.5900 - val_accuracy: 0.6981 - val_precision: 0.6945 - val_recall: 0.3664\n",
      "Epoch 3/8\n",
      "68/68 [==============================] - 8s 116ms/step - loss: 0.5403 - accuracy: 0.7305 - precision: 0.6721 - recall: 0.5734 - val_loss: 0.5654 - val_accuracy: 0.7086 - val_precision: 0.6484 - val_recall: 0.5086\n",
      "Epoch 4/8\n",
      "68/68 [==============================] - 8s 115ms/step - loss: 0.5288 - accuracy: 0.7355 - precision: 0.6785 - recall: 0.5832 - val_loss: 0.5772 - val_accuracy: 0.7062 - val_precision: 0.6598 - val_recall: 0.4681\n",
      "Epoch 5/8\n",
      "68/68 [==============================] - 8s 115ms/step - loss: 0.5098 - accuracy: 0.7486 - precision: 0.7000 - recall: 0.5972 - val_loss: 0.5754 - val_accuracy: 0.7086 - val_precision: 0.6325 - val_recall: 0.5558\n",
      "Epoch 6/8\n",
      "68/68 [==============================] - 8s 115ms/step - loss: 0.4894 - accuracy: 0.7646 - precision: 0.7172 - recall: 0.6328 - val_loss: 0.5939 - val_accuracy: 0.7090 - val_precision: 0.6816 - val_recall: 0.4393\n",
      "Epoch 7/8\n",
      "68/68 [==============================] - 8s 118ms/step - loss: 0.4746 - accuracy: 0.7731 - precision: 0.7311 - recall: 0.6413 - val_loss: 0.5904 - val_accuracy: 0.7018 - val_precision: 0.6165 - val_recall: 0.5692\n",
      "Epoch 8/8\n",
      "68/68 [==============================] - 8s 118ms/step - loss: 0.4539 - accuracy: 0.7858 - precision: 0.7477 - recall: 0.6622 - val_loss: 0.6163 - val_accuracy: 0.6948 - val_precision: 0.6960 - val_recall: 0.3493\n",
      "acc 0.6724860072135925\n",
      "Score:  0.6163302063941956\n",
      "Accuracy:  0.6948323845863342\n",
      "lstm\n",
      "Epoch 1/8\n",
      "68/68 [==============================] - 7s 49ms/step - loss: 0.5916 - accuracy: 0.6845 - precision: 0.6443 - recall: 0.3849 - val_loss: 0.5564 - val_accuracy: 0.7076 - val_precision: 0.6269 - val_recall: 0.5692\n",
      "Epoch 2/8\n",
      "68/68 [==============================] - 2s 23ms/step - loss: 0.5372 - accuracy: 0.7299 - precision: 0.6830 - recall: 0.5457 - val_loss: 0.5612 - val_accuracy: 0.7128 - val_precision: 0.7859 - val_recall: 0.3352\n",
      "Epoch 3/8\n",
      "68/68 [==============================] - 2s 23ms/step - loss: 0.5137 - accuracy: 0.7481 - precision: 0.7119 - recall: 0.5708 - val_loss: 0.5314 - val_accuracy: 0.7251 - val_precision: 0.6949 - val_recall: 0.4926\n",
      "Epoch 4/8\n",
      "68/68 [==============================] - 2s 23ms/step - loss: 0.4942 - accuracy: 0.7581 - precision: 0.7200 - recall: 0.5992 - val_loss: 0.5770 - val_accuracy: 0.7200 - val_precision: 0.8000 - val_recall: 0.3505\n",
      "Epoch 5/8\n",
      "68/68 [==============================] - 1s 21ms/step - loss: 0.4820 - accuracy: 0.7620 - precision: 0.7246 - recall: 0.6071 - val_loss: 0.5194 - val_accuracy: 0.7332 - val_precision: 0.6552 - val_recall: 0.6287\n",
      "Epoch 6/8\n",
      "68/68 [==============================] - 2s 22ms/step - loss: 0.4537 - accuracy: 0.7843 - precision: 0.7546 - recall: 0.6440 - val_loss: 0.5115 - val_accuracy: 0.7453 - val_precision: 0.6756 - val_recall: 0.6342\n",
      "Epoch 7/8\n",
      "68/68 [==============================] - 2s 23ms/step - loss: 0.4314 - accuracy: 0.7994 - precision: 0.7669 - recall: 0.6814 - val_loss: 0.5487 - val_accuracy: 0.7332 - val_precision: 0.7460 - val_recall: 0.4516\n",
      "Epoch 8/8\n",
      "68/68 [==============================] - 2s 23ms/step - loss: 0.4142 - accuracy: 0.8081 - precision: 0.7811 - recall: 0.6907 - val_loss: 0.5574 - val_accuracy: 0.7414 - val_precision: 0.6557 - val_recall: 0.6722\n",
      "acc 0.7076349854469299\n",
      "Score:  0.5573943853378296\n",
      "Accuracy:  0.7413873076438904\n",
      "gru\n",
      "Epoch 1/8\n",
      "68/68 [==============================] - 7s 48ms/step - loss: 0.6371 - accuracy: 0.6637 - precision: 0.5944 - recall: 0.3714 - val_loss: 0.5718 - val_accuracy: 0.6997 - val_precision: 0.6078 - val_recall: 0.5907\n",
      "Epoch 2/8\n",
      "68/68 [==============================] - 1s 22ms/step - loss: 0.5420 - accuracy: 0.7284 - precision: 0.6742 - recall: 0.5579 - val_loss: 0.5400 - val_accuracy: 0.7214 - val_precision: 0.6747 - val_recall: 0.5147\n",
      "Epoch 3/8\n",
      "68/68 [==============================] - 1s 21ms/step - loss: 0.5148 - accuracy: 0.7463 - precision: 0.6991 - recall: 0.5885 - val_loss: 0.5329 - val_accuracy: 0.7267 - val_precision: 0.6483 - val_recall: 0.6134\n",
      "Epoch 4/8\n",
      "68/68 [==============================] - 1s 21ms/step - loss: 0.4991 - accuracy: 0.7553 - precision: 0.7090 - recall: 0.6087 - val_loss: 0.5292 - val_accuracy: 0.7297 - val_precision: 0.7075 - val_recall: 0.4920\n",
      "Epoch 5/8\n",
      "68/68 [==============================] - 1s 21ms/step - loss: 0.4812 - accuracy: 0.7664 - precision: 0.7248 - recall: 0.6254 - val_loss: 0.5179 - val_accuracy: 0.7365 - val_precision: 0.6820 - val_recall: 0.5741\n",
      "Epoch 6/8\n",
      "68/68 [==============================] - 1s 21ms/step - loss: 0.4629 - accuracy: 0.7769 - precision: 0.7376 - recall: 0.6447 - val_loss: 0.5187 - val_accuracy: 0.7435 - val_precision: 0.6893 - val_recall: 0.5913\n",
      "Epoch 7/8\n",
      "68/68 [==============================] - 1s 21ms/step - loss: 0.4432 - accuracy: 0.7923 - precision: 0.7573 - recall: 0.6706 - val_loss: 0.5186 - val_accuracy: 0.7484 - val_precision: 0.6904 - val_recall: 0.6121\n",
      "Epoch 8/8\n",
      "68/68 [==============================] - 1s 21ms/step - loss: 0.4264 - accuracy: 0.8002 - precision: 0.7660 - recall: 0.6860 - val_loss: 0.5215 - val_accuracy: 0.7456 - val_precision: 0.6665 - val_recall: 0.6612\n",
      "acc 0.6997206807136536\n",
      "Score:  0.5214904546737671\n",
      "Accuracy:  0.7455772757530212\n",
      "total vocab 20616\n",
      "no_train\n",
      "Implicit_hate_corpus\n",
      "cnn\n",
      "Epoch 1/8\n",
      "68/68 [==============================] - 2s 23ms/step - loss: 0.6330 - accuracy: 0.6631 - precision: 0.6553 - recall: 0.2458 - val_loss: 0.5611 - val_accuracy: 0.7104 - val_precision: 0.6830 - val_recall: 0.4436\n",
      "Epoch 2/8\n",
      "68/68 [==============================] - 1s 15ms/step - loss: 0.4819 - accuracy: 0.7677 - precision: 0.7439 - recall: 0.5966 - val_loss: 0.5160 - val_accuracy: 0.7437 - val_precision: 0.6807 - val_recall: 0.6127\n",
      "Epoch 3/8\n",
      "68/68 [==============================] - 1s 15ms/step - loss: 0.3384 - accuracy: 0.8582 - precision: 0.8403 - recall: 0.7760 - val_loss: 0.5684 - val_accuracy: 0.7349 - val_precision: 0.6846 - val_recall: 0.5600\n",
      "Epoch 4/8\n",
      "68/68 [==============================] - 1s 15ms/step - loss: 0.2209 - accuracy: 0.9176 - precision: 0.9065 - recall: 0.8743 - val_loss: 0.6521 - val_accuracy: 0.7265 - val_precision: 0.6539 - val_recall: 0.5950\n",
      "Epoch 5/8\n",
      "68/68 [==============================] - 1s 16ms/step - loss: 0.1431 - accuracy: 0.9487 - precision: 0.9434 - recall: 0.9207 - val_loss: 0.7750 - val_accuracy: 0.7162 - val_precision: 0.6429 - val_recall: 0.5692\n",
      "Epoch 6/8\n",
      "68/68 [==============================] - 1s 15ms/step - loss: 0.1015 - accuracy: 0.9655 - precision: 0.9589 - recall: 0.9504 - val_loss: 0.8940 - val_accuracy: 0.7146 - val_precision: 0.6506 - val_recall: 0.5374\n",
      "Epoch 7/8\n",
      "68/68 [==============================] - 1s 15ms/step - loss: 0.0728 - accuracy: 0.9764 - precision: 0.9735 - recall: 0.9645 - val_loss: 0.9873 - val_accuracy: 0.7100 - val_precision: 0.6361 - val_recall: 0.5527\n",
      "Epoch 8/8\n",
      "68/68 [==============================] - 1s 15ms/step - loss: 0.0574 - accuracy: 0.9806 - precision: 0.9783 - recall: 0.9706 - val_loss: 1.0866 - val_accuracy: 0.7069 - val_precision: 0.6350 - val_recall: 0.5374\n",
      "acc 0.7104282975196838\n",
      "Score:  1.0866361856460571\n",
      "Accuracy:  0.7069366574287415\n",
      "rnn\n",
      "Epoch 1/8\n",
      "68/68 [==============================] - 11s 142ms/step - loss: 0.3105 - accuracy: 0.8325 - precision: 0.8053 - recall: 0.7394 - val_loss: 0.6841 - val_accuracy: 0.7051 - val_precision: 0.6156 - val_recall: 0.5956\n",
      "Epoch 2/8\n",
      "68/68 [==============================] - 9s 136ms/step - loss: 0.1548 - accuracy: 0.9392 - precision: 0.9245 - recall: 0.9155 - val_loss: 0.8105 - val_accuracy: 0.7025 - val_precision: 0.6183 - val_recall: 0.5668\n",
      "Epoch 3/8\n",
      "68/68 [==============================] - 9s 138ms/step - loss: 0.0818 - accuracy: 0.9722 - precision: 0.9648 - recall: 0.9622 - val_loss: 1.0955 - val_accuracy: 0.7093 - val_precision: 0.6357 - val_recall: 0.5496\n",
      "Epoch 4/8\n",
      "68/68 [==============================] - 9s 135ms/step - loss: 0.0429 - accuracy: 0.9870 - precision: 0.9842 - recall: 0.9815 - val_loss: 1.1623 - val_accuracy: 0.6890 - val_precision: 0.5955 - val_recall: 0.5656\n",
      "Epoch 5/8\n",
      "68/68 [==============================] - 9s 133ms/step - loss: 0.0273 - accuracy: 0.9929 - precision: 0.9919 - recall: 0.9895 - val_loss: 1.2964 - val_accuracy: 0.6920 - val_precision: 0.6042 - val_recall: 0.5490\n",
      "Epoch 6/8\n",
      "68/68 [==============================] - 10s 140ms/step - loss: 0.0208 - accuracy: 0.9952 - precision: 0.9944 - recall: 0.9931 - val_loss: 1.3818 - val_accuracy: 0.6925 - val_precision: 0.6049 - val_recall: 0.5496\n",
      "Epoch 7/8\n",
      "68/68 [==============================] - 9s 137ms/step - loss: 0.0171 - accuracy: 0.9954 - precision: 0.9945 - recall: 0.9934 - val_loss: 1.3943 - val_accuracy: 0.6851 - val_precision: 0.5924 - val_recall: 0.5478\n",
      "Epoch 8/8\n",
      "68/68 [==============================] - 9s 138ms/step - loss: 0.0182 - accuracy: 0.9951 - precision: 0.9936 - recall: 0.9934 - val_loss: 1.5155 - val_accuracy: 0.6916 - val_precision: 0.5970 - val_recall: 0.5790\n",
      "acc 0.7050744891166687\n",
      "Score:  1.5155251026153564\n",
      "Accuracy:  0.6915735602378845\n",
      "lstm\n",
      "Epoch 1/8\n",
      "68/68 [==============================] - 7s 49ms/step - loss: 0.3148 - accuracy: 0.8338 - precision: 0.8164 - recall: 0.7278 - val_loss: 0.8033 - val_accuracy: 0.7095 - val_precision: 0.6196 - val_recall: 0.6097\n",
      "Epoch 2/8\n",
      "68/68 [==============================] - 2s 23ms/step - loss: 0.1586 - accuracy: 0.9391 - precision: 0.9242 - recall: 0.9155 - val_loss: 0.8551 - val_accuracy: 0.7121 - val_precision: 0.6251 - val_recall: 0.6048\n",
      "Epoch 3/8\n",
      "68/68 [==============================] - 2s 23ms/step - loss: 0.1077 - accuracy: 0.9609 - precision: 0.9517 - recall: 0.9456 - val_loss: 1.0382 - val_accuracy: 0.6995 - val_precision: 0.5975 - val_recall: 0.6403\n",
      "Epoch 4/8\n",
      "68/68 [==============================] - 2s 23ms/step - loss: 0.0806 - accuracy: 0.9716 - precision: 0.9636 - recall: 0.9619 - val_loss: 1.1157 - val_accuracy: 0.7051 - val_precision: 0.6131 - val_recall: 0.6060\n",
      "Epoch 5/8\n",
      "68/68 [==============================] - 2s 23ms/step - loss: 0.0637 - accuracy: 0.9779 - precision: 0.9744 - recall: 0.9677 - val_loss: 1.2400 - val_accuracy: 0.6995 - val_precision: 0.6070 - val_recall: 0.5925\n",
      "Epoch 6/8\n",
      "68/68 [==============================] - 2s 23ms/step - loss: 0.0518 - accuracy: 0.9829 - precision: 0.9774 - recall: 0.9779 - val_loss: 1.3761 - val_accuracy: 0.6944 - val_precision: 0.6004 - val_recall: 0.5846\n",
      "Epoch 7/8\n",
      "68/68 [==============================] - 2s 23ms/step - loss: 0.0462 - accuracy: 0.9840 - precision: 0.9809 - recall: 0.9771 - val_loss: 1.4686 - val_accuracy: 0.6832 - val_precision: 0.5792 - val_recall: 0.6072\n",
      "Epoch 8/8\n",
      "68/68 [==============================] - 2s 24ms/step - loss: 0.0362 - accuracy: 0.9873 - precision: 0.9835 - recall: 0.9832 - val_loss: 1.7121 - val_accuracy: 0.6920 - val_precision: 0.5957 - val_recall: 0.5895\n",
      "acc 0.7094972133636475\n",
      "Score:  1.7121257781982422\n",
      "Accuracy:  0.6920391321182251\n",
      "gru\n",
      "Epoch 1/8\n",
      "68/68 [==============================] - 7s 42ms/step - loss: 0.2666 - accuracy: 0.8447 - precision: 0.8274 - recall: 0.7491 - val_loss: 1.0186 - val_accuracy: 0.7007 - val_precision: 0.6081 - val_recall: 0.5962\n",
      "Epoch 2/8\n",
      "68/68 [==============================] - 2s 22ms/step - loss: 0.0926 - accuracy: 0.9664 - precision: 0.9577 - recall: 0.9539 - val_loss: 1.2304 - val_accuracy: 0.6890 - val_precision: 0.5827 - val_recall: 0.6391\n",
      "Epoch 3/8\n",
      "68/68 [==============================] - 1s 21ms/step - loss: 0.0523 - accuracy: 0.9822 - precision: 0.9784 - recall: 0.9748 - val_loss: 1.5576 - val_accuracy: 0.6965 - val_precision: 0.6053 - val_recall: 0.5778\n",
      "Epoch 4/8\n",
      "68/68 [==============================] - 1s 21ms/step - loss: 0.0434 - accuracy: 0.9848 - precision: 0.9812 - recall: 0.9788 - val_loss: 1.5098 - val_accuracy: 0.6906 - val_precision: 0.5946 - val_recall: 0.5833\n",
      "Epoch 5/8\n",
      "68/68 [==============================] - 1s 21ms/step - loss: 0.0341 - accuracy: 0.9887 - precision: 0.9865 - recall: 0.9838 - val_loss: 1.6426 - val_accuracy: 0.6834 - val_precision: 0.5802 - val_recall: 0.6029\n",
      "Epoch 6/8\n",
      "68/68 [==============================] - 1s 21ms/step - loss: 0.0388 - accuracy: 0.9869 - precision: 0.9832 - recall: 0.9825 - val_loss: 1.7368 - val_accuracy: 0.6827 - val_precision: 0.5791 - val_recall: 0.6036\n",
      "Epoch 7/8\n",
      "68/68 [==============================] - 1s 22ms/step - loss: 0.0286 - accuracy: 0.9894 - precision: 0.9869 - recall: 0.9854 - val_loss: 1.9071 - val_accuracy: 0.6832 - val_precision: 0.5772 - val_recall: 0.6207\n",
      "Epoch 8/8\n",
      "68/68 [==============================] - 2s 22ms/step - loss: 0.0249 - accuracy: 0.9905 - precision: 0.9873 - recall: 0.9878 - val_loss: 1.9421 - val_accuracy: 0.6757 - val_precision: 0.5690 - val_recall: 0.6036\n",
      "acc 0.7006517648696899\n",
      "Score:  1.942081332206726\n",
      "Accuracy:  0.675744891166687\n"
     ]
    }
   ],
   "source": [
    "df_result = get_result_table()\n",
    "# train implicit dataset\n",
    "df, dataset_name = load_dataset(2) \n",
    "train_dataset_all_model(df, dataset_name, df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>hate f1</th>\n",
       "      <th>non-hate f1</th>\n",
       "      <th>hate support</th>\n",
       "      <th>non-hate support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Implicit_hate_corpus_word2vec_trained_cnn</td>\n",
       "      <td>0.747439</td>\n",
       "      <td>0.732340</td>\n",
       "      <td>0.724198</td>\n",
       "      <td>0.727466</td>\n",
       "      <td>0.653687</td>\n",
       "      <td>0.801246</td>\n",
       "      <td>1632.0</td>\n",
       "      <td>2664.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Implicit_hate_corpus_word2vec_trained_rnn</td>\n",
       "      <td>0.672719</td>\n",
       "      <td>0.649471</td>\n",
       "      <td>0.641875</td>\n",
       "      <td>0.644315</td>\n",
       "      <td>0.543803</td>\n",
       "      <td>0.744828</td>\n",
       "      <td>1632.0</td>\n",
       "      <td>2664.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Implicit_hate_corpus_word2vec_trained_lstm</td>\n",
       "      <td>0.728818</td>\n",
       "      <td>0.712166</td>\n",
       "      <td>0.711913</td>\n",
       "      <td>0.712039</td>\n",
       "      <td>0.642528</td>\n",
       "      <td>0.781549</td>\n",
       "      <td>1632.0</td>\n",
       "      <td>2664.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Implicit_hate_corpus_word2vec_trained_gru</td>\n",
       "      <td>0.726723</td>\n",
       "      <td>0.722606</td>\n",
       "      <td>0.676518</td>\n",
       "      <td>0.682966</td>\n",
       "      <td>0.565185</td>\n",
       "      <td>0.800747</td>\n",
       "      <td>1632.0</td>\n",
       "      <td>2664.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Implicit_hate_corpus_fasttext_trained_cnn</td>\n",
       "      <td>0.726723</td>\n",
       "      <td>0.724219</td>\n",
       "      <td>0.675093</td>\n",
       "      <td>0.681420</td>\n",
       "      <td>0.561286</td>\n",
       "      <td>0.801555</td>\n",
       "      <td>1632.0</td>\n",
       "      <td>2664.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Implicit_hate_corpus_fasttext_trained_rnn</td>\n",
       "      <td>0.694600</td>\n",
       "      <td>0.674907</td>\n",
       "      <td>0.653465</td>\n",
       "      <td>0.657729</td>\n",
       "      <td>0.545392</td>\n",
       "      <td>0.770067</td>\n",
       "      <td>1632.0</td>\n",
       "      <td>2664.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Implicit_hate_corpus_fasttext_trained_lstm</td>\n",
       "      <td>0.709730</td>\n",
       "      <td>0.691233</td>\n",
       "      <td>0.688096</td>\n",
       "      <td>0.689483</td>\n",
       "      <td>0.610191</td>\n",
       "      <td>0.768774</td>\n",
       "      <td>1632.0</td>\n",
       "      <td>2664.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Implicit_hate_corpus_fasttext_trained_gru</td>\n",
       "      <td>0.709730</td>\n",
       "      <td>0.701970</td>\n",
       "      <td>0.656170</td>\n",
       "      <td>0.660564</td>\n",
       "      <td>0.531379</td>\n",
       "      <td>0.789749</td>\n",
       "      <td>1632.0</td>\n",
       "      <td>2664.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Implicit_hate_corpus_glove_cnn</td>\n",
       "      <td>0.738594</td>\n",
       "      <td>0.729065</td>\n",
       "      <td>0.698789</td>\n",
       "      <td>0.705867</td>\n",
       "      <td>0.607754</td>\n",
       "      <td>0.803980</td>\n",
       "      <td>1632.0</td>\n",
       "      <td>2664.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Implicit_hate_corpus_glove_rnn</td>\n",
       "      <td>0.694832</td>\n",
       "      <td>0.695267</td>\n",
       "      <td>0.627898</td>\n",
       "      <td>0.625817</td>\n",
       "      <td>0.465116</td>\n",
       "      <td>0.786517</td>\n",
       "      <td>1632.0</td>\n",
       "      <td>2664.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Implicit_hate_corpus_glove_lstm</td>\n",
       "      <td>0.741387</td>\n",
       "      <td>0.725872</td>\n",
       "      <td>0.727983</td>\n",
       "      <td>0.726852</td>\n",
       "      <td>0.663843</td>\n",
       "      <td>0.789862</td>\n",
       "      <td>1632.0</td>\n",
       "      <td>2664.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Implicit_hate_corpus_glove_gru</td>\n",
       "      <td>0.745577</td>\n",
       "      <td>0.729943</td>\n",
       "      <td>0.729225</td>\n",
       "      <td>0.729576</td>\n",
       "      <td>0.663796</td>\n",
       "      <td>0.795357</td>\n",
       "      <td>1632.0</td>\n",
       "      <td>2664.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Implicit_hate_corpus_no_train_cnn</td>\n",
       "      <td>0.706937</td>\n",
       "      <td>0.688021</td>\n",
       "      <td>0.674094</td>\n",
       "      <td>0.678238</td>\n",
       "      <td>0.582144</td>\n",
       "      <td>0.774332</td>\n",
       "      <td>1632.0</td>\n",
       "      <td>2664.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Implicit_hate_corpus_no_train_rnn</td>\n",
       "      <td>0.691574</td>\n",
       "      <td>0.671871</td>\n",
       "      <td>0.669777</td>\n",
       "      <td>0.670725</td>\n",
       "      <td>0.587869</td>\n",
       "      <td>0.753580</td>\n",
       "      <td>1632.0</td>\n",
       "      <td>2664.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Implicit_hate_corpus_no_train_lstm</td>\n",
       "      <td>0.692039</td>\n",
       "      <td>0.672879</td>\n",
       "      <td>0.672170</td>\n",
       "      <td>0.672513</td>\n",
       "      <td>0.592547</td>\n",
       "      <td>0.752479</td>\n",
       "      <td>1632.0</td>\n",
       "      <td>2664.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Implicit_hate_corpus_no_train_gru</td>\n",
       "      <td>0.675745</td>\n",
       "      <td>0.658397</td>\n",
       "      <td>0.661762</td>\n",
       "      <td>0.659694</td>\n",
       "      <td>0.585787</td>\n",
       "      <td>0.733601</td>\n",
       "      <td>1632.0</td>\n",
       "      <td>2664.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Model  Accuracy  precision    recall  \\\n",
       "0    Implicit_hate_corpus_word2vec_trained_cnn  0.747439   0.732340  0.724198   \n",
       "1    Implicit_hate_corpus_word2vec_trained_rnn  0.672719   0.649471  0.641875   \n",
       "2   Implicit_hate_corpus_word2vec_trained_lstm  0.728818   0.712166  0.711913   \n",
       "3    Implicit_hate_corpus_word2vec_trained_gru  0.726723   0.722606  0.676518   \n",
       "4    Implicit_hate_corpus_fasttext_trained_cnn  0.726723   0.724219  0.675093   \n",
       "5    Implicit_hate_corpus_fasttext_trained_rnn  0.694600   0.674907  0.653465   \n",
       "6   Implicit_hate_corpus_fasttext_trained_lstm  0.709730   0.691233  0.688096   \n",
       "7    Implicit_hate_corpus_fasttext_trained_gru  0.709730   0.701970  0.656170   \n",
       "8               Implicit_hate_corpus_glove_cnn  0.738594   0.729065  0.698789   \n",
       "9               Implicit_hate_corpus_glove_rnn  0.694832   0.695267  0.627898   \n",
       "10             Implicit_hate_corpus_glove_lstm  0.741387   0.725872  0.727983   \n",
       "11              Implicit_hate_corpus_glove_gru  0.745577   0.729943  0.729225   \n",
       "12           Implicit_hate_corpus_no_train_cnn  0.706937   0.688021  0.674094   \n",
       "13           Implicit_hate_corpus_no_train_rnn  0.691574   0.671871  0.669777   \n",
       "14          Implicit_hate_corpus_no_train_lstm  0.692039   0.672879  0.672170   \n",
       "15           Implicit_hate_corpus_no_train_gru  0.675745   0.658397  0.661762   \n",
       "\n",
       "    f1-score   hate f1  non-hate f1  hate support  non-hate support  \n",
       "0   0.727466  0.653687     0.801246        1632.0            2664.0  \n",
       "1   0.644315  0.543803     0.744828        1632.0            2664.0  \n",
       "2   0.712039  0.642528     0.781549        1632.0            2664.0  \n",
       "3   0.682966  0.565185     0.800747        1632.0            2664.0  \n",
       "4   0.681420  0.561286     0.801555        1632.0            2664.0  \n",
       "5   0.657729  0.545392     0.770067        1632.0            2664.0  \n",
       "6   0.689483  0.610191     0.768774        1632.0            2664.0  \n",
       "7   0.660564  0.531379     0.789749        1632.0            2664.0  \n",
       "8   0.705867  0.607754     0.803980        1632.0            2664.0  \n",
       "9   0.625817  0.465116     0.786517        1632.0            2664.0  \n",
       "10  0.726852  0.663843     0.789862        1632.0            2664.0  \n",
       "11  0.729576  0.663796     0.795357        1632.0            2664.0  \n",
       "12  0.678238  0.582144     0.774332        1632.0            2664.0  \n",
       "13  0.670725  0.587869     0.753580        1632.0            2664.0  \n",
       "14  0.672513  0.592547     0.752479        1632.0            2664.0  \n",
       "15  0.659694  0.585787     0.733601        1632.0            2664.0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "Non-Hate    0.578737\n",
      "Hate        0.421263\n",
      "Name: proportion, dtype: float64\n",
      "Train Set : (10384,) (10384,)\n",
      "Test Set  : (2596,) (2596,)\n",
      "Total  12980\n",
      "SE2019\n",
      "total vocab 19393\n",
      "total vector 3000000\n",
      "Converted 13078 words (6315 misses)\n",
      "word2vec_trained\n",
      "SE2019\n",
      "cnn\n",
      "Epoch 1/8\n",
      "41/41 [==============================] - 2s 36ms/step - loss: 0.6483 - accuracy: 0.6385 - precision: 0.5727 - recall: 0.4427 - val_loss: 0.5962 - val_accuracy: 0.6760 - val_precision: 0.6703 - val_recall: 0.4915\n",
      "Epoch 2/8\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.5630 - accuracy: 0.7122 - precision: 0.7070 - recall: 0.5339 - val_loss: 0.5655 - val_accuracy: 0.6984 - val_precision: 0.6825 - val_recall: 0.5638\n",
      "Epoch 3/8\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.5216 - accuracy: 0.7458 - precision: 0.7309 - recall: 0.6216 - val_loss: 0.5492 - val_accuracy: 0.7200 - val_precision: 0.6994 - val_recall: 0.6164\n",
      "Epoch 4/8\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.4911 - accuracy: 0.7680 - precision: 0.7592 - recall: 0.6529 - val_loss: 0.5427 - val_accuracy: 0.7261 - val_precision: 0.6645 - val_recall: 0.7386\n",
      "Epoch 5/8\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.4660 - accuracy: 0.7828 - precision: 0.7656 - recall: 0.6936 - val_loss: 0.5349 - val_accuracy: 0.7327 - val_precision: 0.6769 - val_recall: 0.7288\n",
      "Epoch 6/8\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.4381 - accuracy: 0.8035 - precision: 0.7845 - recall: 0.7318 - val_loss: 0.5262 - val_accuracy: 0.7408 - val_precision: 0.7166 - val_recall: 0.6610\n",
      "Epoch 7/8\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.4106 - accuracy: 0.8239 - precision: 0.8141 - recall: 0.7506 - val_loss: 0.5262 - val_accuracy: 0.7392 - val_precision: 0.7122 - val_recall: 0.6646\n",
      "Epoch 8/8\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.3900 - accuracy: 0.8323 - precision: 0.8173 - recall: 0.7720 - val_loss: 0.5275 - val_accuracy: 0.7446 - val_precision: 0.6886 - val_recall: 0.7458\n",
      "acc 0.6760400533676147\n",
      "Score:  0.5274875164031982\n",
      "Accuracy:  0.7446070909500122\n",
      "rnn\n",
      "Epoch 1/8\n",
      "41/41 [==============================] - 6s 118ms/step - loss: 0.6467 - accuracy: 0.6442 - precision: 0.5906 - recall: 0.5066 - val_loss: 0.6014 - val_accuracy: 0.6714 - val_precision: 0.6234 - val_recall: 0.6039\n",
      "Epoch 2/8\n",
      "41/41 [==============================] - 4s 105ms/step - loss: 0.5702 - accuracy: 0.7042 - precision: 0.6707 - recall: 0.5763 - val_loss: 0.5928 - val_accuracy: 0.6772 - val_precision: 0.6192 - val_recall: 0.6557\n",
      "Epoch 3/8\n",
      "41/41 [==============================] - 5s 114ms/step - loss: 0.5444 - accuracy: 0.7209 - precision: 0.6840 - recall: 0.6195 - val_loss: 0.6198 - val_accuracy: 0.6626 - val_precision: 0.5871 - val_recall: 0.7368\n",
      "Epoch 4/8\n",
      "41/41 [==============================] - 5s 111ms/step - loss: 0.5222 - accuracy: 0.7435 - precision: 0.7084 - recall: 0.6579 - val_loss: 0.6097 - val_accuracy: 0.6799 - val_precision: 0.6214 - val_recall: 0.6619\n",
      "Epoch 5/8\n",
      "41/41 [==============================] - 4s 105ms/step - loss: 0.4944 - accuracy: 0.7631 - precision: 0.7300 - recall: 0.6890 - val_loss: 0.6350 - val_accuracy: 0.6572 - val_precision: 0.6133 - val_recall: 0.5575\n",
      "Epoch 6/8\n",
      "41/41 [==============================] - 4s 106ms/step - loss: 0.4635 - accuracy: 0.7759 - precision: 0.7502 - recall: 0.6966 - val_loss: 0.6549 - val_accuracy: 0.6760 - val_precision: 0.6474 - val_recall: 0.5486\n",
      "Epoch 7/8\n",
      "41/41 [==============================] - 4s 105ms/step - loss: 0.4402 - accuracy: 0.7967 - precision: 0.7699 - recall: 0.7336 - val_loss: 0.6831 - val_accuracy: 0.6564 - val_precision: 0.6230 - val_recall: 0.5174\n",
      "Epoch 8/8\n",
      "41/41 [==============================] - 5s 110ms/step - loss: 0.4078 - accuracy: 0.8156 - precision: 0.7906 - recall: 0.7610 - val_loss: 0.7357 - val_accuracy: 0.6529 - val_precision: 0.6170 - val_recall: 0.5174\n",
      "acc 0.6714175939559937\n",
      "Score:  0.7357242107391357\n",
      "Accuracy:  0.652927577495575\n",
      "lstm\n",
      "Epoch 1/8\n",
      "41/41 [==============================] - 6s 68ms/step - loss: 0.6339 - accuracy: 0.6371 - precision: 0.6020 - recall: 0.4086 - val_loss: 0.6133 - val_accuracy: 0.6633 - val_precision: 0.5917 - val_recall: 0.7110\n",
      "Epoch 2/8\n",
      "41/41 [==============================] - 1s 27ms/step - loss: 0.5722 - accuracy: 0.7018 - precision: 0.6668 - recall: 0.5749 - val_loss: 0.5743 - val_accuracy: 0.6945 - val_precision: 0.7124 - val_recall: 0.4906\n",
      "Epoch 3/8\n",
      "41/41 [==============================] - 1s 24ms/step - loss: 0.5491 - accuracy: 0.7168 - precision: 0.6868 - recall: 0.5947 - val_loss: 0.5610 - val_accuracy: 0.7034 - val_precision: 0.7092 - val_recall: 0.5308\n",
      "Epoch 4/8\n",
      "41/41 [==============================] - 1s 25ms/step - loss: 0.5366 - accuracy: 0.7255 - precision: 0.6940 - recall: 0.6161 - val_loss: 0.5552 - val_accuracy: 0.7088 - val_precision: 0.6634 - val_recall: 0.6610\n",
      "Epoch 5/8\n",
      "41/41 [==============================] - 1s 25ms/step - loss: 0.5264 - accuracy: 0.7339 - precision: 0.6975 - recall: 0.6434 - val_loss: 0.5525 - val_accuracy: 0.7146 - val_precision: 0.7008 - val_recall: 0.5914\n",
      "Epoch 6/8\n",
      "41/41 [==============================] - 1s 24ms/step - loss: 0.5096 - accuracy: 0.7485 - precision: 0.7179 - recall: 0.6575 - val_loss: 0.5489 - val_accuracy: 0.7122 - val_precision: 0.6826 - val_recall: 0.6236\n",
      "Epoch 7/8\n",
      "41/41 [==============================] - 1s 25ms/step - loss: 0.4944 - accuracy: 0.7568 - precision: 0.7274 - recall: 0.6703 - val_loss: 0.5574 - val_accuracy: 0.7146 - val_precision: 0.7065 - val_recall: 0.5798\n",
      "Epoch 8/8\n",
      "41/41 [==============================] - 1s 24ms/step - loss: 0.4872 - accuracy: 0.7646 - precision: 0.7390 - recall: 0.6768 - val_loss: 0.5492 - val_accuracy: 0.7099 - val_precision: 0.6581 - val_recall: 0.6833\n",
      "acc 0.6633281707763672\n",
      "Score:  0.5491708517074585\n",
      "Accuracy:  0.7099383473396301\n",
      "gru\n",
      "Epoch 1/8\n",
      "41/41 [==============================] - 6s 58ms/step - loss: 0.6478 - accuracy: 0.6389 - precision: 0.6178 - recall: 0.3745 - val_loss: 0.6114 - val_accuracy: 0.6599 - val_precision: 0.6820 - val_recall: 0.3979\n",
      "Epoch 2/8\n",
      "41/41 [==============================] - 1s 25ms/step - loss: 0.5882 - accuracy: 0.6902 - precision: 0.6570 - recall: 0.5438 - val_loss: 0.5860 - val_accuracy: 0.6787 - val_precision: 0.7077 - val_recall: 0.4362\n",
      "Epoch 3/8\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.5486 - accuracy: 0.7219 - precision: 0.6967 - recall: 0.5944 - val_loss: 0.5677 - val_accuracy: 0.7034 - val_precision: 0.6444 - val_recall: 0.6985\n",
      "Epoch 4/8\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.5361 - accuracy: 0.7297 - precision: 0.6993 - recall: 0.6216 - val_loss: 0.5558 - val_accuracy: 0.7107 - val_precision: 0.6726 - val_recall: 0.6432\n",
      "Epoch 5/8\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.5226 - accuracy: 0.7381 - precision: 0.7068 - recall: 0.6395 - val_loss: 0.5621 - val_accuracy: 0.7065 - val_precision: 0.6395 - val_recall: 0.7342\n",
      "Epoch 6/8\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.5154 - accuracy: 0.7433 - precision: 0.7088 - recall: 0.6563 - val_loss: 0.5551 - val_accuracy: 0.7080 - val_precision: 0.6482 - val_recall: 0.7083\n",
      "Epoch 7/8\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.5036 - accuracy: 0.7500 - precision: 0.7145 - recall: 0.6708 - val_loss: 0.5537 - val_accuracy: 0.7134 - val_precision: 0.7025 - val_recall: 0.5834\n",
      "Epoch 8/8\n",
      "41/41 [==============================] - 1s 24ms/step - loss: 0.4945 - accuracy: 0.7601 - precision: 0.7258 - recall: 0.6862 - val_loss: 0.5536 - val_accuracy: 0.7115 - val_precision: 0.6488 - val_recall: 0.7235\n",
      "acc 0.6598613262176514\n",
      "Score:  0.5536291003227234\n",
      "Accuracy:  0.7114791870117188\n",
      "total vocab 19393\n",
      "total vector 999994\n",
      "Converted 13608 words (5785 misses)\n",
      "fasttext_trained\n",
      "SE2019\n",
      "cnn\n",
      "Epoch 1/8\n",
      "41/41 [==============================] - 2s 23ms/step - loss: 0.6714 - accuracy: 0.6181 - precision: 0.6296 - recall: 0.2270 - val_loss: 0.6631 - val_accuracy: 0.6013 - val_precision: 0.6593 - val_recall: 0.1588\n",
      "Epoch 2/8\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6457 - accuracy: 0.6303 - precision: 0.6484 - recall: 0.2553 - val_loss: 0.6486 - val_accuracy: 0.6036 - val_precision: 0.6345 - val_recall: 0.1936\n",
      "Epoch 3/8\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6253 - accuracy: 0.6514 - precision: 0.6676 - recall: 0.3331 - val_loss: 0.6337 - val_accuracy: 0.6163 - val_precision: 0.6492 - val_recall: 0.2426\n",
      "Epoch 4/8\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6059 - accuracy: 0.6685 - precision: 0.6805 - recall: 0.3925 - val_loss: 0.6156 - val_accuracy: 0.6549 - val_precision: 0.6510 - val_recall: 0.4326\n",
      "Epoch 5/8\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.5846 - accuracy: 0.6975 - precision: 0.7018 - recall: 0.4824 - val_loss: 0.5999 - val_accuracy: 0.6680 - val_precision: 0.6671 - val_recall: 0.4612\n",
      "Epoch 6/8\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.5669 - accuracy: 0.7106 - precision: 0.7075 - recall: 0.5263 - val_loss: 0.5924 - val_accuracy: 0.6737 - val_precision: 0.7188 - val_recall: 0.4014\n",
      "Epoch 7/8\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.5460 - accuracy: 0.7303 - precision: 0.7323 - recall: 0.5606 - val_loss: 0.5760 - val_accuracy: 0.6965 - val_precision: 0.6743 - val_recall: 0.5745\n",
      "Epoch 8/8\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.5328 - accuracy: 0.7371 - precision: 0.7308 - recall: 0.5889 - val_loss: 0.5695 - val_accuracy: 0.6953 - val_precision: 0.6993 - val_recall: 0.5165\n",
      "acc 0.6013097167015076\n",
      "Score:  0.569503903388977\n",
      "Accuracy:  0.6953004598617554\n",
      "rnn\n",
      "Epoch 1/8\n",
      "41/41 [==============================] - 6s 119ms/step - loss: 0.6745 - accuracy: 0.6025 - precision: 0.5559 - recall: 0.2811 - val_loss: 0.6702 - val_accuracy: 0.5755 - val_precision: 0.5102 - val_recall: 0.4255\n",
      "Epoch 2/8\n",
      "41/41 [==============================] - 5s 114ms/step - loss: 0.6303 - accuracy: 0.6433 - precision: 0.6107 - recall: 0.4081 - val_loss: 0.6668 - val_accuracy: 0.6063 - val_precision: 0.5296 - val_recall: 0.7895\n",
      "Epoch 3/8\n",
      "41/41 [==============================] - 5s 115ms/step - loss: 0.6289 - accuracy: 0.6472 - precision: 0.6214 - recall: 0.4028 - val_loss: 0.6507 - val_accuracy: 0.6294 - val_precision: 0.6544 - val_recall: 0.3006\n",
      "Epoch 4/8\n",
      "41/41 [==============================] - 5s 116ms/step - loss: 0.5784 - accuracy: 0.7043 - precision: 0.6783 - recall: 0.5583 - val_loss: 0.6894 - val_accuracy: 0.6437 - val_precision: 0.7322 - val_recall: 0.2756\n",
      "Epoch 5/8\n",
      "41/41 [==============================] - 5s 112ms/step - loss: 0.5679 - accuracy: 0.7102 - precision: 0.6897 - recall: 0.5595 - val_loss: 0.6169 - val_accuracy: 0.6726 - val_precision: 0.6440 - val_recall: 0.5406\n",
      "Epoch 6/8\n",
      "41/41 [==============================] - 5s 113ms/step - loss: 0.5578 - accuracy: 0.7159 - precision: 0.6868 - recall: 0.5908 - val_loss: 0.6502 - val_accuracy: 0.6244 - val_precision: 0.6113 - val_recall: 0.3577\n",
      "Epoch 7/8\n",
      "41/41 [==============================] - 5s 124ms/step - loss: 0.5294 - accuracy: 0.7401 - precision: 0.7258 - recall: 0.6094 - val_loss: 0.6368 - val_accuracy: 0.6849 - val_precision: 0.6433 - val_recall: 0.6066\n",
      "Epoch 8/8\n",
      "41/41 [==============================] - 5s 130ms/step - loss: 0.5017 - accuracy: 0.7552 - precision: 0.7342 - recall: 0.6508 - val_loss: 0.6406 - val_accuracy: 0.6780 - val_precision: 0.6145 - val_recall: 0.6824\n",
      "acc 0.5755007863044739\n",
      "Score:  0.6406092643737793\n",
      "Accuracy:  0.6779661178588867\n",
      "lstm\n",
      "Epoch 1/8\n",
      "41/41 [==============================] - 7s 71ms/step - loss: 0.6701 - accuracy: 0.6106 - precision: 0.6043 - recall: 0.2193 - val_loss: 0.6502 - val_accuracy: 0.6183 - val_precision: 0.5735 - val_recall: 0.4523\n",
      "Epoch 2/8\n",
      "41/41 [==============================] - 1s 26ms/step - loss: 0.6485 - accuracy: 0.6290 - precision: 0.5979 - recall: 0.3476 - val_loss: 0.6359 - val_accuracy: 0.6341 - val_precision: 0.6592 - val_recall: 0.3158\n",
      "Epoch 3/8\n",
      "41/41 [==============================] - 1s 25ms/step - loss: 0.6090 - accuracy: 0.6673 - precision: 0.6481 - recall: 0.4490 - val_loss: 0.6058 - val_accuracy: 0.6699 - val_precision: 0.6976 - val_recall: 0.4157\n",
      "Epoch 4/8\n",
      "41/41 [==============================] - 1s 24ms/step - loss: 0.5927 - accuracy: 0.6857 - precision: 0.6573 - recall: 0.5206 - val_loss: 0.5882 - val_accuracy: 0.6826 - val_precision: 0.6686 - val_recall: 0.5254\n",
      "Epoch 5/8\n",
      "41/41 [==============================] - 1s 25ms/step - loss: 0.5765 - accuracy: 0.6992 - precision: 0.6738 - recall: 0.5454 - val_loss: 0.5968 - val_accuracy: 0.6776 - val_precision: 0.6111 - val_recall: 0.6967\n",
      "Epoch 6/8\n",
      "41/41 [==============================] - 1s 24ms/step - loss: 0.5667 - accuracy: 0.7088 - precision: 0.6820 - recall: 0.5703 - val_loss: 0.5802 - val_accuracy: 0.6938 - val_precision: 0.7032 - val_recall: 0.5031\n",
      "Epoch 7/8\n",
      "41/41 [==============================] - 1s 24ms/step - loss: 0.5612 - accuracy: 0.7096 - precision: 0.6783 - recall: 0.5825 - val_loss: 0.5717 - val_accuracy: 0.7018 - val_precision: 0.6673 - val_recall: 0.6173\n",
      "Epoch 8/8\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.5530 - accuracy: 0.7150 - precision: 0.6907 - recall: 0.5783 - val_loss: 0.5712 - val_accuracy: 0.6992 - val_precision: 0.6805 - val_recall: 0.5718\n",
      "acc 0.6182588338851929\n",
      "Score:  0.571227490901947\n",
      "Accuracy:  0.6991525292396545\n",
      "gru\n",
      "Epoch 1/8\n",
      "41/41 [==============================] - 6s 55ms/step - loss: 0.6712 - accuracy: 0.6114 - precision: 0.6417 - recall: 0.1756 - val_loss: 0.6621 - val_accuracy: 0.5917 - val_precision: 0.5572 - val_recall: 0.2649\n",
      "Epoch 2/8\n",
      "41/41 [==============================] - 1s 25ms/step - loss: 0.6447 - accuracy: 0.6267 - precision: 0.5922 - recall: 0.3481 - val_loss: 0.6250 - val_accuracy: 0.6445 - val_precision: 0.6518 - val_recall: 0.3791\n",
      "Epoch 3/8\n",
      "41/41 [==============================] - 1s 21ms/step - loss: 0.6042 - accuracy: 0.6701 - precision: 0.6446 - recall: 0.4723 - val_loss: 0.5918 - val_accuracy: 0.6857 - val_precision: 0.7015 - val_recall: 0.4737\n",
      "Epoch 4/8\n",
      "41/41 [==============================] - 1s 21ms/step - loss: 0.5750 - accuracy: 0.6977 - precision: 0.6709 - recall: 0.5454 - val_loss: 0.5812 - val_accuracy: 0.6941 - val_precision: 0.6433 - val_recall: 0.6548\n",
      "Epoch 5/8\n",
      "41/41 [==============================] - 1s 22ms/step - loss: 0.5704 - accuracy: 0.7027 - precision: 0.6679 - recall: 0.5765 - val_loss: 0.5958 - val_accuracy: 0.6818 - val_precision: 0.7259 - val_recall: 0.4228\n",
      "Epoch 6/8\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.5640 - accuracy: 0.7093 - precision: 0.6760 - recall: 0.5866 - val_loss: 0.5677 - val_accuracy: 0.7015 - val_precision: 0.6657 - val_recall: 0.6200\n",
      "Epoch 7/8\n",
      "41/41 [==============================] - 1s 22ms/step - loss: 0.5520 - accuracy: 0.7185 - precision: 0.6875 - recall: 0.6006 - val_loss: 0.5734 - val_accuracy: 0.6980 - val_precision: 0.6360 - val_recall: 0.7029\n",
      "Epoch 8/8\n",
      "41/41 [==============================] - 1s 22ms/step - loss: 0.5513 - accuracy: 0.7162 - precision: 0.6822 - recall: 0.6029 - val_loss: 0.5618 - val_accuracy: 0.7049 - val_precision: 0.6589 - val_recall: 0.6566\n",
      "acc 0.5916795134544373\n",
      "Score:  0.5618173480033875\n",
      "Accuracy:  0.7049306631088257\n",
      "total vocab 19393\n",
      "Total words  1917494\n",
      "Converted 15043 words (4350 misses)\n",
      "glove\n",
      "SE2019\n",
      "cnn\n",
      "Epoch 1/8\n",
      "41/41 [==============================] - 2s 24ms/step - loss: 0.6519 - accuracy: 0.6408 - precision: 0.5855 - recall: 0.5046 - val_loss: 0.5727 - val_accuracy: 0.6930 - val_precision: 0.6489 - val_recall: 0.6298\n",
      "Epoch 2/8\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.5334 - accuracy: 0.7325 - precision: 0.7055 - recall: 0.6195 - val_loss: 0.5466 - val_accuracy: 0.7122 - val_precision: 0.7232 - val_recall: 0.5406\n",
      "Epoch 3/8\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.4868 - accuracy: 0.7669 - precision: 0.7450 - recall: 0.6736 - val_loss: 0.5388 - val_accuracy: 0.7196 - val_precision: 0.6623 - val_recall: 0.7154\n",
      "Epoch 4/8\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.4452 - accuracy: 0.7987 - precision: 0.7800 - recall: 0.7233 - val_loss: 0.5353 - val_accuracy: 0.7223 - val_precision: 0.7481 - val_recall: 0.5379\n",
      "Epoch 5/8\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.4112 - accuracy: 0.8172 - precision: 0.8032 - recall: 0.7463 - val_loss: 0.5201 - val_accuracy: 0.7361 - val_precision: 0.6996 - val_recall: 0.6815\n",
      "Epoch 6/8\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.3835 - accuracy: 0.8421 - precision: 0.8299 - recall: 0.7833 - val_loss: 0.5281 - val_accuracy: 0.7269 - val_precision: 0.7429 - val_recall: 0.5620\n",
      "Epoch 7/8\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.3505 - accuracy: 0.8643 - precision: 0.8536 - recall: 0.8157 - val_loss: 0.5205 - val_accuracy: 0.7415 - val_precision: 0.7143 - val_recall: 0.6690\n",
      "Epoch 8/8\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.3298 - accuracy: 0.8709 - precision: 0.8613 - recall: 0.8242 - val_loss: 0.5240 - val_accuracy: 0.7438 - val_precision: 0.7231 - val_recall: 0.6592\n",
      "acc 0.6929892301559448\n",
      "Score:  0.5240195989608765\n",
      "Accuracy:  0.7438367009162903\n",
      "rnn\n",
      "Epoch 1/8\n",
      "41/41 [==============================] - 6s 119ms/step - loss: 0.6719 - accuracy: 0.6305 - precision: 0.5746 - recall: 0.4731 - val_loss: 0.6398 - val_accuracy: 0.6271 - val_precision: 0.5534 - val_recall: 0.7074\n",
      "Epoch 2/8\n",
      "41/41 [==============================] - 5s 114ms/step - loss: 0.5891 - accuracy: 0.6848 - precision: 0.6373 - recall: 0.5733 - val_loss: 0.6138 - val_accuracy: 0.6741 - val_precision: 0.6871 - val_recall: 0.4505\n",
      "Epoch 3/8\n",
      "41/41 [==============================] - 5s 118ms/step - loss: 0.5487 - accuracy: 0.7232 - precision: 0.6852 - recall: 0.6269 - val_loss: 0.5960 - val_accuracy: 0.6864 - val_precision: 0.6155 - val_recall: 0.7297\n",
      "Epoch 4/8\n",
      "41/41 [==============================] - 5s 116ms/step - loss: 0.5208 - accuracy: 0.7434 - precision: 0.7068 - recall: 0.6611 - val_loss: 0.5914 - val_accuracy: 0.6888 - val_precision: 0.6450 - val_recall: 0.6209\n",
      "Epoch 5/8\n",
      "41/41 [==============================] - 5s 119ms/step - loss: 0.4977 - accuracy: 0.7587 - precision: 0.7249 - recall: 0.6825 - val_loss: 0.6021 - val_accuracy: 0.6814 - val_precision: 0.6476 - val_recall: 0.5754\n",
      "Epoch 6/8\n",
      "41/41 [==============================] - 5s 118ms/step - loss: 0.4727 - accuracy: 0.7721 - precision: 0.7376 - recall: 0.7074 - val_loss: 0.6503 - val_accuracy: 0.6791 - val_precision: 0.6778 - val_recall: 0.4897\n",
      "Epoch 7/8\n",
      "41/41 [==============================] - 5s 119ms/step - loss: 0.4453 - accuracy: 0.7916 - precision: 0.7624 - recall: 0.7295 - val_loss: 0.6330 - val_accuracy: 0.6934 - val_precision: 0.6455 - val_recall: 0.6432\n",
      "Epoch 8/8\n",
      "41/41 [==============================] - 5s 133ms/step - loss: 0.4117 - accuracy: 0.8127 - precision: 0.7833 - recall: 0.7640 - val_loss: 0.6582 - val_accuracy: 0.6626 - val_precision: 0.6197 - val_recall: 0.5656\n",
      "acc 0.6271186470985413\n",
      "Score:  0.6581751108169556\n",
      "Accuracy:  0.6625577807426453\n",
      "lstm\n",
      "Epoch 1/8\n",
      "41/41 [==============================] - 6s 63ms/step - loss: 0.6237 - accuracy: 0.6449 - precision: 0.6027 - recall: 0.4610 - val_loss: 0.5781 - val_accuracy: 0.6918 - val_precision: 0.6789 - val_recall: 0.5433\n",
      "Epoch 2/8\n",
      "41/41 [==============================] - 1s 28ms/step - loss: 0.5488 - accuracy: 0.7230 - precision: 0.6889 - recall: 0.6170 - val_loss: 0.5605 - val_accuracy: 0.6976 - val_precision: 0.6282 - val_recall: 0.7342\n",
      "Epoch 3/8\n",
      "41/41 [==============================] - 1s 25ms/step - loss: 0.5208 - accuracy: 0.7377 - precision: 0.6975 - recall: 0.6593 - val_loss: 0.5413 - val_accuracy: 0.7169 - val_precision: 0.7292 - val_recall: 0.5477\n",
      "Epoch 4/8\n",
      "41/41 [==============================] - 1s 24ms/step - loss: 0.5039 - accuracy: 0.7507 - precision: 0.7177 - recall: 0.6667 - val_loss: 0.5318 - val_accuracy: 0.7211 - val_precision: 0.6749 - val_recall: 0.6833\n",
      "Epoch 5/8\n",
      "41/41 [==============================] - 1s 24ms/step - loss: 0.4767 - accuracy: 0.7696 - precision: 0.7367 - recall: 0.6998 - val_loss: 0.5250 - val_accuracy: 0.7277 - val_precision: 0.6829 - val_recall: 0.6896\n",
      "Epoch 6/8\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.4549 - accuracy: 0.7830 - precision: 0.7507 - recall: 0.7212 - val_loss: 0.5280 - val_accuracy: 0.7361 - val_precision: 0.6909 - val_recall: 0.7038\n",
      "Epoch 7/8\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.4374 - accuracy: 0.7914 - precision: 0.7570 - recall: 0.7389 - val_loss: 0.5368 - val_accuracy: 0.7307 - val_precision: 0.6965 - val_recall: 0.6673\n",
      "Epoch 8/8\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.4110 - accuracy: 0.8114 - precision: 0.7837 - recall: 0.7591 - val_loss: 0.5338 - val_accuracy: 0.7242 - val_precision: 0.6891 - val_recall: 0.6583\n",
      "acc 0.6918336153030396\n",
      "Score:  0.5337504148483276\n",
      "Accuracy:  0.7241910696029663\n",
      "gru\n",
      "Epoch 1/8\n",
      "41/41 [==============================] - 6s 57ms/step - loss: 0.6553 - accuracy: 0.6346 - precision: 0.5962 - recall: 0.4109 - val_loss: 0.6296 - val_accuracy: 0.6414 - val_precision: 0.5825 - val_recall: 0.5986\n",
      "Epoch 2/8\n",
      "41/41 [==============================] - 1s 24ms/step - loss: 0.5781 - accuracy: 0.6918 - precision: 0.6504 - recall: 0.5705 - val_loss: 0.5653 - val_accuracy: 0.6988 - val_precision: 0.6631 - val_recall: 0.6146\n",
      "Epoch 3/8\n",
      "41/41 [==============================] - 1s 22ms/step - loss: 0.5352 - accuracy: 0.7305 - precision: 0.6953 - recall: 0.6342 - val_loss: 0.5493 - val_accuracy: 0.7107 - val_precision: 0.6710 - val_recall: 0.6476\n",
      "Epoch 4/8\n",
      "41/41 [==============================] - 1s 22ms/step - loss: 0.5189 - accuracy: 0.7427 - precision: 0.7080 - recall: 0.6559 - val_loss: 0.5407 - val_accuracy: 0.7196 - val_precision: 0.6731 - val_recall: 0.6815\n",
      "Epoch 5/8\n",
      "41/41 [==============================] - 1s 22ms/step - loss: 0.4998 - accuracy: 0.7553 - precision: 0.7207 - recall: 0.6784 - val_loss: 0.5384 - val_accuracy: 0.7230 - val_precision: 0.7051 - val_recall: 0.6164\n",
      "Epoch 6/8\n",
      "41/41 [==============================] - 1s 22ms/step - loss: 0.4831 - accuracy: 0.7656 - precision: 0.7346 - recall: 0.6890 - val_loss: 0.5361 - val_accuracy: 0.7261 - val_precision: 0.6830 - val_recall: 0.6824\n",
      "Epoch 7/8\n",
      "41/41 [==============================] - 1s 22ms/step - loss: 0.4557 - accuracy: 0.7852 - precision: 0.7512 - recall: 0.7281 - val_loss: 0.5394 - val_accuracy: 0.7196 - val_precision: 0.7058 - val_recall: 0.6012\n",
      "Epoch 8/8\n",
      "41/41 [==============================] - 1s 22ms/step - loss: 0.4409 - accuracy: 0.7928 - precision: 0.7627 - recall: 0.7329 - val_loss: 0.5311 - val_accuracy: 0.7292 - val_precision: 0.6730 - val_recall: 0.7252\n",
      "acc 0.6413713693618774\n",
      "Score:  0.531122624874115\n",
      "Accuracy:  0.7291987538337708\n",
      "total vocab 19393\n",
      "no_train\n",
      "SE2019\n",
      "cnn\n",
      "Epoch 1/8\n",
      "41/41 [==============================] - 2s 28ms/step - loss: 0.6555 - accuracy: 0.6304 - precision: 0.6222 - recall: 0.3120 - val_loss: 0.6122 - val_accuracy: 0.6691 - val_precision: 0.6882 - val_recall: 0.4273\n",
      "Epoch 2/8\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.5284 - accuracy: 0.7405 - precision: 0.7390 - recall: 0.5875 - val_loss: 0.5326 - val_accuracy: 0.7253 - val_precision: 0.7094 - val_recall: 0.6164\n",
      "Epoch 3/8\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.3866 - accuracy: 0.8380 - precision: 0.8249 - recall: 0.7782 - val_loss: 0.5518 - val_accuracy: 0.7227 - val_precision: 0.7003 - val_recall: 0.6253\n",
      "Epoch 4/8\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.2562 - accuracy: 0.9062 - precision: 0.8919 - recall: 0.8829 - val_loss: 0.6322 - val_accuracy: 0.7227 - val_precision: 0.6760 - val_recall: 0.6869\n",
      "Epoch 5/8\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1538 - accuracy: 0.9490 - precision: 0.9372 - recall: 0.9411 - val_loss: 0.7517 - val_accuracy: 0.7045 - val_precision: 0.6536 - val_recall: 0.6717\n",
      "Epoch 6/8\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.0992 - accuracy: 0.9673 - precision: 0.9592 - recall: 0.9627 - val_loss: 0.8741 - val_accuracy: 0.6945 - val_precision: 0.6580 - val_recall: 0.6093\n",
      "Epoch 7/8\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.0659 - accuracy: 0.9793 - precision: 0.9745 - recall: 0.9761 - val_loss: 0.9618 - val_accuracy: 0.6961 - val_precision: 0.6501 - val_recall: 0.6414\n",
      "Epoch 8/8\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.0509 - accuracy: 0.9839 - precision: 0.9800 - recall: 0.9816 - val_loss: 1.0523 - val_accuracy: 0.6895 - val_precision: 0.6343 - val_recall: 0.6637\n",
      "acc 0.6691063046455383\n",
      "Score:  1.0523117780685425\n",
      "Accuracy:  0.6895223259925842\n",
      "rnn\n",
      "Epoch 1/8\n",
      "41/41 [==============================] - 7s 147ms/step - loss: 0.3430 - accuracy: 0.8166 - precision: 0.7895 - recall: 0.7701 - val_loss: 0.6917 - val_accuracy: 0.6934 - val_precision: 0.6757 - val_recall: 0.5575\n",
      "Epoch 2/8\n",
      "41/41 [==============================] - 6s 141ms/step - loss: 0.1450 - accuracy: 0.9500 - precision: 0.9384 - recall: 0.9425 - val_loss: 0.8060 - val_accuracy: 0.6834 - val_precision: 0.6380 - val_recall: 0.6164\n",
      "Epoch 3/8\n",
      "41/41 [==============================] - 6s 136ms/step - loss: 0.0860 - accuracy: 0.9736 - precision: 0.9674 - recall: 0.9696 - val_loss: 1.0437 - val_accuracy: 0.6930 - val_precision: 0.6500 - val_recall: 0.6262\n",
      "Epoch 4/8\n",
      "41/41 [==============================] - 6s 145ms/step - loss: 0.0604 - accuracy: 0.9825 - precision: 0.9773 - recall: 0.9809 - val_loss: 1.0671 - val_accuracy: 0.6826 - val_precision: 0.6381 - val_recall: 0.6120\n",
      "Epoch 5/8\n",
      "41/41 [==============================] - 6s 145ms/step - loss: 0.0478 - accuracy: 0.9885 - precision: 0.9862 - recall: 0.9864 - val_loss: 1.2105 - val_accuracy: 0.6872 - val_precision: 0.6429 - val_recall: 0.6200\n",
      "Epoch 6/8\n",
      "41/41 [==============================] - 6s 140ms/step - loss: 0.0385 - accuracy: 0.9897 - precision: 0.9860 - recall: 0.9894 - val_loss: 1.1645 - val_accuracy: 0.6853 - val_precision: 0.6319 - val_recall: 0.6494\n",
      "Epoch 7/8\n",
      "41/41 [==============================] - 6s 148ms/step - loss: 0.0329 - accuracy: 0.9909 - precision: 0.9901 - recall: 0.9883 - val_loss: 1.3207 - val_accuracy: 0.6787 - val_precision: 0.6230 - val_recall: 0.6485\n",
      "Epoch 8/8\n",
      "41/41 [==============================] - 6s 135ms/step - loss: 0.0340 - accuracy: 0.9909 - precision: 0.9885 - recall: 0.9899 - val_loss: 1.1893 - val_accuracy: 0.6822 - val_precision: 0.6407 - val_recall: 0.6012\n",
      "acc 0.6933743953704834\n",
      "Score:  1.189263105392456\n",
      "Accuracy:  0.6822034120559692\n",
      "lstm\n",
      "Epoch 1/8\n",
      "41/41 [==============================] - 6s 64ms/step - loss: 0.3632 - accuracy: 0.8256 - precision: 0.8384 - recall: 0.7259 - val_loss: 0.7655 - val_accuracy: 0.6988 - val_precision: 0.6501 - val_recall: 0.6548\n",
      "Epoch 2/8\n",
      "41/41 [==============================] - 1s 27ms/step - loss: 0.1339 - accuracy: 0.9524 - precision: 0.9362 - recall: 0.9512 - val_loss: 0.9167 - val_accuracy: 0.6764 - val_precision: 0.6200 - val_recall: 0.6476\n",
      "Epoch 3/8\n",
      "41/41 [==============================] - 1s 27ms/step - loss: 0.0921 - accuracy: 0.9679 - precision: 0.9570 - recall: 0.9669 - val_loss: 1.1008 - val_accuracy: 0.6757 - val_precision: 0.6225 - val_recall: 0.6325\n",
      "Epoch 4/8\n",
      "41/41 [==============================] - 1s 26ms/step - loss: 0.0708 - accuracy: 0.9762 - precision: 0.9676 - recall: 0.9758 - val_loss: 1.2413 - val_accuracy: 0.6768 - val_precision: 0.6246 - val_recall: 0.6307\n",
      "Epoch 5/8\n",
      "41/41 [==============================] - 1s 26ms/step - loss: 0.0592 - accuracy: 0.9795 - precision: 0.9717 - recall: 0.9795 - val_loss: 1.2278 - val_accuracy: 0.6745 - val_precision: 0.6213 - val_recall: 0.6307\n",
      "Epoch 6/8\n",
      "41/41 [==============================] - 1s 26ms/step - loss: 0.0498 - accuracy: 0.9836 - precision: 0.9809 - recall: 0.9800 - val_loss: 1.3650 - val_accuracy: 0.6737 - val_precision: 0.6138 - val_recall: 0.6592\n",
      "Epoch 7/8\n",
      "41/41 [==============================] - 1s 26ms/step - loss: 0.0459 - accuracy: 0.9845 - precision: 0.9814 - recall: 0.9816 - val_loss: 1.2783 - val_accuracy: 0.6656 - val_precision: 0.6120 - val_recall: 0.6164\n",
      "Epoch 8/8\n",
      "41/41 [==============================] - 1s 26ms/step - loss: 0.0384 - accuracy: 0.9858 - precision: 0.9823 - recall: 0.9839 - val_loss: 1.4891 - val_accuracy: 0.6691 - val_precision: 0.6125 - val_recall: 0.6360\n",
      "acc 0.698767364025116\n",
      "Score:  1.4890806674957275\n",
      "Accuracy:  0.6691063046455383\n",
      "gru\n",
      "Epoch 1/8\n",
      "41/41 [==============================] - 6s 61ms/step - loss: 0.3281 - accuracy: 0.8195 - precision: 0.8230 - recall: 0.7281 - val_loss: 1.0396 - val_accuracy: 0.6853 - val_precision: 0.6352 - val_recall: 0.6369\n",
      "Epoch 2/8\n",
      "41/41 [==============================] - 1s 24ms/step - loss: 0.0716 - accuracy: 0.9748 - precision: 0.9649 - recall: 0.9752 - val_loss: 1.3435 - val_accuracy: 0.6680 - val_precision: 0.6057 - val_recall: 0.6619\n",
      "Epoch 3/8\n",
      "41/41 [==============================] - 1s 24ms/step - loss: 0.0478 - accuracy: 0.9837 - precision: 0.9783 - recall: 0.9830 - val_loss: 1.4818 - val_accuracy: 0.6726 - val_precision: 0.6167 - val_recall: 0.6387\n",
      "Epoch 4/8\n",
      "41/41 [==============================] - 1s 24ms/step - loss: 0.0405 - accuracy: 0.9860 - precision: 0.9803 - recall: 0.9864 - val_loss: 1.4918 - val_accuracy: 0.6641 - val_precision: 0.6105 - val_recall: 0.6137\n",
      "Epoch 5/8\n",
      "41/41 [==============================] - 1s 24ms/step - loss: 0.0357 - accuracy: 0.9871 - precision: 0.9841 - recall: 0.9850 - val_loss: 1.4763 - val_accuracy: 0.6660 - val_precision: 0.6138 - val_recall: 0.6111\n",
      "Epoch 6/8\n",
      "41/41 [==============================] - 1s 25ms/step - loss: 0.0342 - accuracy: 0.9875 - precision: 0.9855 - recall: 0.9846 - val_loss: 1.7437 - val_accuracy: 0.6649 - val_precision: 0.6043 - val_recall: 0.6485\n",
      "Epoch 7/8\n",
      "41/41 [==============================] - 1s 24ms/step - loss: 0.0304 - accuracy: 0.9883 - precision: 0.9860 - recall: 0.9862 - val_loss: 1.5733 - val_accuracy: 0.6637 - val_precision: 0.6142 - val_recall: 0.5950\n",
      "Epoch 8/8\n",
      "41/41 [==============================] - 1s 24ms/step - loss: 0.0277 - accuracy: 0.9892 - precision: 0.9871 - recall: 0.9871 - val_loss: 1.7641 - val_accuracy: 0.6606 - val_precision: 0.6040 - val_recall: 0.6218\n",
      "acc 0.6852850317955017\n",
      "Score:  1.7640690803527832\n",
      "Accuracy:  0.6606317162513733\n"
     ]
    }
   ],
   "source": [
    "df_result = get_result_table()\n",
    "# train se2019 dataset\n",
    "df, dataset_name = load_dataset(3) \n",
    "train_dataset_all_model(df, dataset_name, df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>hate f1</th>\n",
       "      <th>non-hate f1</th>\n",
       "      <th>hate support</th>\n",
       "      <th>non-hate support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SE2019_word2vec_trained_cnn</td>\n",
       "      <td>0.744607</td>\n",
       "      <td>0.741205</td>\n",
       "      <td>0.744746</td>\n",
       "      <td>0.741999</td>\n",
       "      <td>0.716060</td>\n",
       "      <td>0.767938</td>\n",
       "      <td>1121.0</td>\n",
       "      <td>1475.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SE2019_word2vec_trained_rnn</td>\n",
       "      <td>0.652928</td>\n",
       "      <td>0.645165</td>\n",
       "      <td>0.636664</td>\n",
       "      <td>0.637533</td>\n",
       "      <td>0.562834</td>\n",
       "      <td>0.712233</td>\n",
       "      <td>1121.0</td>\n",
       "      <td>1475.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SE2019_word2vec_trained_lstm</td>\n",
       "      <td>0.709938</td>\n",
       "      <td>0.705085</td>\n",
       "      <td>0.706744</td>\n",
       "      <td>0.705715</td>\n",
       "      <td>0.670460</td>\n",
       "      <td>0.740970</td>\n",
       "      <td>1121.0</td>\n",
       "      <td>1475.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SE2019_word2vec_trained_gru</td>\n",
       "      <td>0.711479</td>\n",
       "      <td>0.709244</td>\n",
       "      <td>0.712917</td>\n",
       "      <td>0.709295</td>\n",
       "      <td>0.684100</td>\n",
       "      <td>0.734491</td>\n",
       "      <td>1121.0</td>\n",
       "      <td>1475.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SE2019_fasttext_trained_cnn</td>\n",
       "      <td>0.695300</td>\n",
       "      <td>0.696357</td>\n",
       "      <td>0.673845</td>\n",
       "      <td>0.675120</td>\n",
       "      <td>0.594151</td>\n",
       "      <td>0.756090</td>\n",
       "      <td>1121.0</td>\n",
       "      <td>1475.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SE2019_fasttext_trained_rnn</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.675475</td>\n",
       "      <td>0.678501</td>\n",
       "      <td>0.675418</td>\n",
       "      <td>0.646661</td>\n",
       "      <td>0.704176</td>\n",
       "      <td>1121.0</td>\n",
       "      <td>1475.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SE2019_fasttext_trained_lstm</td>\n",
       "      <td>0.699153</td>\n",
       "      <td>0.695131</td>\n",
       "      <td>0.683872</td>\n",
       "      <td>0.685912</td>\n",
       "      <td>0.621425</td>\n",
       "      <td>0.750399</td>\n",
       "      <td>1121.0</td>\n",
       "      <td>1475.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SE2019_fasttext_trained_gru</td>\n",
       "      <td>0.704931</td>\n",
       "      <td>0.699298</td>\n",
       "      <td>0.699126</td>\n",
       "      <td>0.699210</td>\n",
       "      <td>0.657730</td>\n",
       "      <td>0.740691</td>\n",
       "      <td>1121.0</td>\n",
       "      <td>1475.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SE2019_glove_cnn</td>\n",
       "      <td>0.743837</td>\n",
       "      <td>0.740199</td>\n",
       "      <td>0.733684</td>\n",
       "      <td>0.735792</td>\n",
       "      <td>0.689687</td>\n",
       "      <td>0.781896</td>\n",
       "      <td>1121.0</td>\n",
       "      <td>1475.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SE2019_glove_rnn</td>\n",
       "      <td>0.662558</td>\n",
       "      <td>0.655073</td>\n",
       "      <td>0.650919</td>\n",
       "      <td>0.652008</td>\n",
       "      <td>0.591418</td>\n",
       "      <td>0.712598</td>\n",
       "      <td>1121.0</td>\n",
       "      <td>1475.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SE2019_glove_lstm</td>\n",
       "      <td>0.724191</td>\n",
       "      <td>0.718964</td>\n",
       "      <td>0.716289</td>\n",
       "      <td>0.717345</td>\n",
       "      <td>0.673358</td>\n",
       "      <td>0.761333</td>\n",
       "      <td>1121.0</td>\n",
       "      <td>1475.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SE2019_glove_gru</td>\n",
       "      <td>0.729199</td>\n",
       "      <td>0.725556</td>\n",
       "      <td>0.728724</td>\n",
       "      <td>0.726304</td>\n",
       "      <td>0.698154</td>\n",
       "      <td>0.754453</td>\n",
       "      <td>1121.0</td>\n",
       "      <td>1475.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SE2019_no_train_cnn</td>\n",
       "      <td>0.689522</td>\n",
       "      <td>0.684669</td>\n",
       "      <td>0.686423</td>\n",
       "      <td>0.685263</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.721877</td>\n",
       "      <td>1121.0</td>\n",
       "      <td>1475.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SE2019_no_train_rnn</td>\n",
       "      <td>0.682203</td>\n",
       "      <td>0.675588</td>\n",
       "      <td>0.672489</td>\n",
       "      <td>0.673536</td>\n",
       "      <td>0.620341</td>\n",
       "      <td>0.726731</td>\n",
       "      <td>1121.0</td>\n",
       "      <td>1475.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SE2019_no_train_lstm</td>\n",
       "      <td>0.669106</td>\n",
       "      <td>0.663813</td>\n",
       "      <td>0.665138</td>\n",
       "      <td>0.664288</td>\n",
       "      <td>0.624070</td>\n",
       "      <td>0.704506</td>\n",
       "      <td>1121.0</td>\n",
       "      <td>1475.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SE2019_no_train_gru</td>\n",
       "      <td>0.660632</td>\n",
       "      <td>0.654975</td>\n",
       "      <td>0.655968</td>\n",
       "      <td>0.655362</td>\n",
       "      <td>0.612747</td>\n",
       "      <td>0.697977</td>\n",
       "      <td>1121.0</td>\n",
       "      <td>1475.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Model  Accuracy  precision    recall  f1-score  \\\n",
       "0    SE2019_word2vec_trained_cnn  0.744607   0.741205  0.744746  0.741999   \n",
       "1    SE2019_word2vec_trained_rnn  0.652928   0.645165  0.636664  0.637533   \n",
       "2   SE2019_word2vec_trained_lstm  0.709938   0.705085  0.706744  0.705715   \n",
       "3    SE2019_word2vec_trained_gru  0.711479   0.709244  0.712917  0.709295   \n",
       "4    SE2019_fasttext_trained_cnn  0.695300   0.696357  0.673845  0.675120   \n",
       "5    SE2019_fasttext_trained_rnn  0.677966   0.675475  0.678501  0.675418   \n",
       "6   SE2019_fasttext_trained_lstm  0.699153   0.695131  0.683872  0.685912   \n",
       "7    SE2019_fasttext_trained_gru  0.704931   0.699298  0.699126  0.699210   \n",
       "8               SE2019_glove_cnn  0.743837   0.740199  0.733684  0.735792   \n",
       "9               SE2019_glove_rnn  0.662558   0.655073  0.650919  0.652008   \n",
       "10             SE2019_glove_lstm  0.724191   0.718964  0.716289  0.717345   \n",
       "11              SE2019_glove_gru  0.729199   0.725556  0.728724  0.726304   \n",
       "12           SE2019_no_train_cnn  0.689522   0.684669  0.686423  0.685263   \n",
       "13           SE2019_no_train_rnn  0.682203   0.675588  0.672489  0.673536   \n",
       "14          SE2019_no_train_lstm  0.669106   0.663813  0.665138  0.664288   \n",
       "15           SE2019_no_train_gru  0.660632   0.654975  0.655968  0.655362   \n",
       "\n",
       "     hate f1  non-hate f1  hate support  non-hate support  \n",
       "0   0.716060     0.767938        1121.0            1475.0  \n",
       "1   0.562834     0.712233        1121.0            1475.0  \n",
       "2   0.670460     0.740970        1121.0            1475.0  \n",
       "3   0.684100     0.734491        1121.0            1475.0  \n",
       "4   0.594151     0.756090        1121.0            1475.0  \n",
       "5   0.646661     0.704176        1121.0            1475.0  \n",
       "6   0.621425     0.750399        1121.0            1475.0  \n",
       "7   0.657730     0.740691        1121.0            1475.0  \n",
       "8   0.689687     0.781896        1121.0            1475.0  \n",
       "9   0.591418     0.712598        1121.0            1475.0  \n",
       "10  0.673358     0.761333        1121.0            1475.0  \n",
       "11  0.698154     0.754453        1121.0            1475.0  \n",
       "12  0.648649     0.721877        1121.0            1475.0  \n",
       "13  0.620341     0.726731        1121.0            1475.0  \n",
       "14  0.624070     0.704506        1121.0            1475.0  \n",
       "15  0.612747     0.697977        1121.0            1475.0  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "Hate        0.500427\n",
      "Non-Hate    0.499573\n",
      "Name: proportion, dtype: float64\n",
      "Train Set : (27178,) (27178,)\n",
      "Test Set  : (6795,) (6795,)\n",
      "Total  33973\n",
      "Balanced\n",
      "total vocab 33332\n",
      "total vector 3000000\n",
      "Converted 21403 words (11929 misses)\n",
      "word2vec_trained\n",
      "Balanced\n",
      "cnn\n",
      "Epoch 1/8\n",
      "107/107 [==============================] - 3s 20ms/step - loss: 0.6125 - accuracy: 0.6638 - precision: 0.6581 - recall: 0.6678 - val_loss: 0.5743 - val_accuracy: 0.7027 - val_precision: 0.6875 - val_recall: 0.7391\n",
      "Epoch 2/8\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.5482 - accuracy: 0.7227 - precision: 0.7264 - recall: 0.7163 - val_loss: 0.5563 - val_accuracy: 0.7163 - val_precision: 0.7301 - val_recall: 0.6826\n",
      "Epoch 3/8\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.5183 - accuracy: 0.7480 - precision: 0.7517 - recall: 0.7421 - val_loss: 0.5506 - val_accuracy: 0.7195 - val_precision: 0.7016 - val_recall: 0.7600\n",
      "Epoch 4/8\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 0.4920 - accuracy: 0.7636 - precision: 0.7655 - recall: 0.7614 - val_loss: 0.5441 - val_accuracy: 0.7283 - val_precision: 0.7347 - val_recall: 0.7113\n",
      "Epoch 5/8\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.4666 - accuracy: 0.7830 - precision: 0.7842 - recall: 0.7823 - val_loss: 0.5447 - val_accuracy: 0.7288 - val_precision: 0.7301 - val_recall: 0.7225\n",
      "Epoch 6/8\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 0.4433 - accuracy: 0.7981 - precision: 0.7996 - recall: 0.7967 - val_loss: 0.5485 - val_accuracy: 0.7285 - val_precision: 0.7112 - val_recall: 0.7657\n",
      "Epoch 7/8\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.4220 - accuracy: 0.8114 - precision: 0.8127 - recall: 0.8102 - val_loss: 0.5537 - val_accuracy: 0.7283 - val_precision: 0.7076 - val_recall: 0.7745\n",
      "Epoch 8/8\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 0.3968 - accuracy: 0.8264 - precision: 0.8296 - recall: 0.8225 - val_loss: 0.5521 - val_accuracy: 0.7292 - val_precision: 0.7229 - val_recall: 0.7400\n",
      "acc 0.7027226090431213\n",
      "Score:  0.5521321296691895\n",
      "Accuracy:  0.7292126417160034\n",
      "rnn\n",
      "Epoch 1/8\n",
      "107/107 [==============================] - 14s 115ms/step - loss: 0.6407 - accuracy: 0.6483 - precision: 0.6436 - recall: 0.6656 - val_loss: 0.6101 - val_accuracy: 0.6768 - val_precision: 0.6457 - val_recall: 0.7778\n",
      "Epoch 2/8\n",
      "107/107 [==============================] - 12s 113ms/step - loss: 0.5995 - accuracy: 0.6842 - precision: 0.6786 - recall: 0.7021 - val_loss: 0.5994 - val_accuracy: 0.6814 - val_precision: 0.7032 - val_recall: 0.6232\n",
      "Epoch 3/8\n",
      "107/107 [==============================] - 12s 111ms/step - loss: 0.5817 - accuracy: 0.6996 - precision: 0.6968 - recall: 0.7092 - val_loss: 0.6157 - val_accuracy: 0.6609 - val_precision: 0.7139 - val_recall: 0.5325\n",
      "Epoch 4/8\n",
      "107/107 [==============================] - 12s 110ms/step - loss: 0.5680 - accuracy: 0.7079 - precision: 0.7048 - recall: 0.7173 - val_loss: 0.6006 - val_accuracy: 0.6805 - val_precision: 0.6797 - val_recall: 0.6779\n",
      "Epoch 5/8\n",
      "107/107 [==============================] - 12s 111ms/step - loss: 0.5567 - accuracy: 0.7177 - precision: 0.7150 - recall: 0.7260 - val_loss: 0.6046 - val_accuracy: 0.6834 - val_precision: 0.6765 - val_recall: 0.6983\n",
      "Epoch 6/8\n",
      "107/107 [==============================] - 12s 109ms/step - loss: 0.5478 - accuracy: 0.7218 - precision: 0.7182 - recall: 0.7320 - val_loss: 0.6082 - val_accuracy: 0.6830 - val_precision: 0.6643 - val_recall: 0.7349\n",
      "Epoch 7/8\n",
      "107/107 [==============================] - 12s 110ms/step - loss: 0.5311 - accuracy: 0.7332 - precision: 0.7292 - recall: 0.7437 - val_loss: 0.6278 - val_accuracy: 0.6655 - val_precision: 0.6833 - val_recall: 0.6120\n",
      "Epoch 8/8\n",
      "107/107 [==============================] - 12s 109ms/step - loss: 0.5207 - accuracy: 0.7420 - precision: 0.7399 - recall: 0.7481 - val_loss: 0.6205 - val_accuracy: 0.6770 - val_precision: 0.6721 - val_recall: 0.6862\n",
      "acc 0.6768211722373962\n",
      "Score:  0.620477557182312\n",
      "Accuracy:  0.6769683361053467\n",
      "lstm\n",
      "Epoch 1/8\n",
      "107/107 [==============================] - 8s 43ms/step - loss: 0.6214 - accuracy: 0.6584 - precision: 0.6532 - recall: 0.6766 - val_loss: 0.6005 - val_accuracy: 0.6867 - val_precision: 0.6490 - val_recall: 0.8076\n",
      "Epoch 2/8\n",
      "107/107 [==============================] - 3s 24ms/step - loss: 0.5849 - accuracy: 0.6935 - precision: 0.6907 - recall: 0.7032 - val_loss: 0.5807 - val_accuracy: 0.6980 - val_precision: 0.6810 - val_recall: 0.7405\n",
      "Epoch 3/8\n",
      "107/107 [==============================] - 3s 24ms/step - loss: 0.5707 - accuracy: 0.7023 - precision: 0.6958 - recall: 0.7209 - val_loss: 0.5713 - val_accuracy: 0.6971 - val_precision: 0.7086 - val_recall: 0.6655\n",
      "Epoch 4/8\n",
      "107/107 [==============================] - 3s 24ms/step - loss: 0.5568 - accuracy: 0.7124 - precision: 0.7073 - recall: 0.7268 - val_loss: 0.5665 - val_accuracy: 0.7076 - val_precision: 0.7121 - val_recall: 0.6930\n",
      "Epoch 5/8\n",
      "107/107 [==============================] - 3s 24ms/step - loss: 0.5471 - accuracy: 0.7208 - precision: 0.7120 - recall: 0.7435 - val_loss: 0.5641 - val_accuracy: 0.7079 - val_precision: 0.6745 - val_recall: 0.7991\n",
      "Epoch 6/8\n",
      "107/107 [==============================] - 3s 24ms/step - loss: 0.5335 - accuracy: 0.7305 - precision: 0.7217 - recall: 0.7521 - val_loss: 0.5613 - val_accuracy: 0.7121 - val_precision: 0.6785 - val_recall: 0.8020\n",
      "Epoch 7/8\n",
      "107/107 [==============================] - 3s 24ms/step - loss: 0.5199 - accuracy: 0.7402 - precision: 0.7344 - recall: 0.7544 - val_loss: 0.5577 - val_accuracy: 0.7174 - val_precision: 0.7030 - val_recall: 0.7491\n",
      "Epoch 8/8\n",
      "107/107 [==============================] - 3s 24ms/step - loss: 0.5075 - accuracy: 0.7467 - precision: 0.7371 - recall: 0.7685 - val_loss: 0.5635 - val_accuracy: 0.7130 - val_precision: 0.7077 - val_recall: 0.7219\n",
      "acc 0.6866813898086548\n",
      "Score:  0.5634526014328003\n",
      "Accuracy:  0.7130242586135864\n",
      "gru\n",
      "Epoch 1/8\n",
      "107/107 [==============================] - 8s 37ms/step - loss: 0.6248 - accuracy: 0.6598 - precision: 0.6581 - recall: 0.6665 - val_loss: 0.5975 - val_accuracy: 0.6879 - val_precision: 0.6423 - val_recall: 0.8422\n",
      "Epoch 2/8\n",
      "107/107 [==============================] - 3s 25ms/step - loss: 0.5800 - accuracy: 0.6958 - precision: 0.6892 - recall: 0.7156 - val_loss: 0.5744 - val_accuracy: 0.7010 - val_precision: 0.6962 - val_recall: 0.7089\n",
      "Epoch 3/8\n",
      "107/107 [==============================] - 2s 22ms/step - loss: 0.5648 - accuracy: 0.7097 - precision: 0.7041 - recall: 0.7256 - val_loss: 0.5658 - val_accuracy: 0.7040 - val_precision: 0.7119 - val_recall: 0.6814\n",
      "Epoch 4/8\n",
      "107/107 [==============================] - 3s 23ms/step - loss: 0.5519 - accuracy: 0.7177 - precision: 0.7117 - recall: 0.7338 - val_loss: 0.5619 - val_accuracy: 0.7123 - val_precision: 0.7288 - val_recall: 0.6726\n",
      "Epoch 5/8\n",
      "107/107 [==============================] - 2s 22ms/step - loss: 0.5414 - accuracy: 0.7274 - precision: 0.7219 - recall: 0.7418 - val_loss: 0.5566 - val_accuracy: 0.7180 - val_precision: 0.7238 - val_recall: 0.7015\n",
      "Epoch 6/8\n",
      "107/107 [==============================] - 2s 22ms/step - loss: 0.5273 - accuracy: 0.7346 - precision: 0.7269 - recall: 0.7534 - val_loss: 0.5647 - val_accuracy: 0.7089 - val_precision: 0.7475 - val_recall: 0.6274\n",
      "Epoch 7/8\n",
      "107/107 [==============================] - 2s 22ms/step - loss: 0.5181 - accuracy: 0.7420 - precision: 0.7361 - recall: 0.7562 - val_loss: 0.5684 - val_accuracy: 0.7130 - val_precision: 0.7550 - val_recall: 0.6274\n",
      "Epoch 8/8\n",
      "107/107 [==============================] - 2s 21ms/step - loss: 0.5060 - accuracy: 0.7498 - precision: 0.7463 - recall: 0.7582 - val_loss: 0.5631 - val_accuracy: 0.7201 - val_precision: 0.7140 - val_recall: 0.7305\n",
      "acc 0.6878587007522583\n",
      "Score:  0.5630669593811035\n",
      "Accuracy:  0.7200883030891418\n",
      "total vocab 33332\n",
      "total vector 999994\n",
      "Converted 22650 words (10682 misses)\n",
      "fasttext_trained\n",
      "Balanced\n",
      "cnn\n",
      "Epoch 1/8\n",
      "107/107 [==============================] - 3s 18ms/step - loss: 0.6689 - accuracy: 0.6216 - precision: 0.6117 - recall: 0.6680 - val_loss: 0.6424 - val_accuracy: 0.6515 - val_precision: 0.6150 - val_recall: 0.8026\n",
      "Epoch 2/8\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 0.6175 - accuracy: 0.6712 - precision: 0.6720 - recall: 0.6717 - val_loss: 0.6029 - val_accuracy: 0.6876 - val_precision: 0.6886 - val_recall: 0.6803\n",
      "Epoch 3/8\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 0.5884 - accuracy: 0.6962 - precision: 0.7017 - recall: 0.6845 - val_loss: 0.5907 - val_accuracy: 0.6943 - val_precision: 0.6699 - val_recall: 0.7615\n",
      "Epoch 4/8\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 0.5692 - accuracy: 0.7086 - precision: 0.7130 - recall: 0.7002 - val_loss: 0.5772 - val_accuracy: 0.7067 - val_precision: 0.7151 - val_recall: 0.6832\n",
      "Epoch 5/8\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 0.5553 - accuracy: 0.7228 - precision: 0.7297 - recall: 0.7097 - val_loss: 0.5696 - val_accuracy: 0.7089 - val_precision: 0.7139 - val_recall: 0.6933\n",
      "Epoch 6/8\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 0.5439 - accuracy: 0.7277 - precision: 0.7353 - recall: 0.7133 - val_loss: 0.5665 - val_accuracy: 0.7096 - val_precision: 0.7258 - val_recall: 0.6702\n",
      "Epoch 7/8\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 0.5338 - accuracy: 0.7357 - precision: 0.7433 - recall: 0.7220 - val_loss: 0.5694 - val_accuracy: 0.7118 - val_precision: 0.6866 - val_recall: 0.7751\n",
      "Epoch 8/8\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 0.5226 - accuracy: 0.7442 - precision: 0.7501 - recall: 0.7340 - val_loss: 0.5599 - val_accuracy: 0.7135 - val_precision: 0.7270 - val_recall: 0.6800\n",
      "acc 0.6515084505081177\n",
      "Score:  0.5599115490913391\n",
      "Accuracy:  0.7134658098220825\n",
      "rnn\n",
      "Epoch 1/8\n",
      "107/107 [==============================] - 15s 129ms/step - loss: 0.6617 - accuracy: 0.6245 - precision: 0.6267 - recall: 0.6174 - val_loss: 0.6106 - val_accuracy: 0.6777 - val_precision: 0.6712 - val_recall: 0.6918\n",
      "Epoch 2/8\n",
      "107/107 [==============================] - 12s 116ms/step - loss: 0.6146 - accuracy: 0.6674 - precision: 0.6684 - recall: 0.6671 - val_loss: 0.6200 - val_accuracy: 0.6606 - val_precision: 0.6696 - val_recall: 0.6288\n",
      "Epoch 3/8\n",
      "107/107 [==============================] - 12s 115ms/step - loss: 0.6030 - accuracy: 0.6793 - precision: 0.6823 - recall: 0.6736 - val_loss: 0.6047 - val_accuracy: 0.6796 - val_precision: 0.6621 - val_recall: 0.7284\n",
      "Epoch 4/8\n",
      "107/107 [==============================] - 12s 115ms/step - loss: 0.5911 - accuracy: 0.6932 - precision: 0.6978 - recall: 0.6840 - val_loss: 0.6000 - val_accuracy: 0.6877 - val_precision: 0.6768 - val_recall: 0.7139\n",
      "Epoch 5/8\n",
      "107/107 [==============================] - 13s 123ms/step - loss: 0.5796 - accuracy: 0.6985 - precision: 0.7019 - recall: 0.6924 - val_loss: 0.6043 - val_accuracy: 0.6751 - val_precision: 0.7168 - val_recall: 0.5745\n",
      "Epoch 6/8\n",
      "107/107 [==============================] - 14s 131ms/step - loss: 0.5721 - accuracy: 0.7034 - precision: 0.7063 - recall: 0.6985 - val_loss: 0.6269 - val_accuracy: 0.6689 - val_precision: 0.6316 - val_recall: 0.8041\n",
      "Epoch 7/8\n",
      "107/107 [==============================] - 13s 123ms/step - loss: 0.5732 - accuracy: 0.7056 - precision: 0.7072 - recall: 0.7040 - val_loss: 0.6196 - val_accuracy: 0.6671 - val_precision: 0.7117 - val_recall: 0.5573\n",
      "Epoch 8/8\n",
      "107/107 [==============================] - 13s 120ms/step - loss: 0.5649 - accuracy: 0.7109 - precision: 0.7133 - recall: 0.7074 - val_loss: 0.5994 - val_accuracy: 0.6784 - val_precision: 0.6766 - val_recall: 0.6788\n",
      "acc 0.6777042150497437\n",
      "Score:  0.5993573069572449\n",
      "Accuracy:  0.6784400343894958\n",
      "lstm\n",
      "Epoch 1/8\n",
      "107/107 [==============================] - 8s 37ms/step - loss: 0.6551 - accuracy: 0.6289 - precision: 0.6156 - recall: 0.6882 - val_loss: 0.6136 - val_accuracy: 0.6793 - val_precision: 0.6729 - val_recall: 0.6930\n",
      "Epoch 2/8\n",
      "107/107 [==============================] - 2s 23ms/step - loss: 0.6071 - accuracy: 0.6741 - precision: 0.6767 - recall: 0.6693 - val_loss: 0.5956 - val_accuracy: 0.6862 - val_precision: 0.6659 - val_recall: 0.7426\n",
      "Epoch 3/8\n",
      "107/107 [==============================] - 3s 26ms/step - loss: 0.5957 - accuracy: 0.6863 - precision: 0.6847 - recall: 0.6928 - val_loss: 0.5906 - val_accuracy: 0.6889 - val_precision: 0.6920 - val_recall: 0.6764\n",
      "Epoch 4/8\n",
      "107/107 [==============================] - 2s 23ms/step - loss: 0.5922 - accuracy: 0.6893 - precision: 0.6861 - recall: 0.7001 - val_loss: 0.5863 - val_accuracy: 0.6954 - val_precision: 0.6767 - val_recall: 0.7435\n",
      "Epoch 5/8\n",
      "107/107 [==============================] - 3s 26ms/step - loss: 0.5841 - accuracy: 0.6941 - precision: 0.6919 - recall: 0.7022 - val_loss: 0.5966 - val_accuracy: 0.6848 - val_precision: 0.6448 - val_recall: 0.8174\n",
      "Epoch 6/8\n",
      "107/107 [==============================] - 3s 25ms/step - loss: 0.5792 - accuracy: 0.6969 - precision: 0.6937 - recall: 0.7074 - val_loss: 0.5890 - val_accuracy: 0.6898 - val_precision: 0.6631 - val_recall: 0.7665\n",
      "Epoch 7/8\n",
      "107/107 [==============================] - 3s 25ms/step - loss: 0.5762 - accuracy: 0.6992 - precision: 0.6948 - recall: 0.7127 - val_loss: 0.5823 - val_accuracy: 0.6954 - val_precision: 0.6719 - val_recall: 0.7589\n",
      "Epoch 8/8\n",
      "107/107 [==============================] - 3s 25ms/step - loss: 0.5718 - accuracy: 0.7026 - precision: 0.6967 - recall: 0.7198 - val_loss: 0.5778 - val_accuracy: 0.6949 - val_precision: 0.6797 - val_recall: 0.7326\n",
      "acc 0.6793230175971985\n",
      "Score:  0.5778403282165527\n",
      "Accuracy:  0.6949227452278137\n",
      "gru\n",
      "Epoch 1/8\n",
      "107/107 [==============================] - 7s 35ms/step - loss: 0.6527 - accuracy: 0.6312 - precision: 0.6289 - recall: 0.6418 - val_loss: 0.6161 - val_accuracy: 0.6793 - val_precision: 0.6432 - val_recall: 0.7996\n",
      "Epoch 2/8\n",
      "107/107 [==============================] - 2s 23ms/step - loss: 0.6019 - accuracy: 0.6785 - precision: 0.6795 - recall: 0.6781 - val_loss: 0.5937 - val_accuracy: 0.6830 - val_precision: 0.7105 - val_recall: 0.6135\n",
      "Epoch 3/8\n",
      "107/107 [==============================] - 3s 24ms/step - loss: 0.5867 - accuracy: 0.6941 - precision: 0.6952 - recall: 0.6935 - val_loss: 0.5838 - val_accuracy: 0.6961 - val_precision: 0.6803 - val_recall: 0.7352\n",
      "Epoch 4/8\n",
      "107/107 [==============================] - 3s 24ms/step - loss: 0.5812 - accuracy: 0.6962 - precision: 0.6902 - recall: 0.7142 - val_loss: 0.5843 - val_accuracy: 0.6909 - val_precision: 0.6611 - val_recall: 0.7787\n",
      "Epoch 5/8\n",
      "107/107 [==============================] - 2s 22ms/step - loss: 0.5768 - accuracy: 0.6998 - precision: 0.6915 - recall: 0.7239 - val_loss: 0.5758 - val_accuracy: 0.6993 - val_precision: 0.6913 - val_recall: 0.7160\n",
      "Epoch 6/8\n",
      "107/107 [==============================] - 3s 24ms/step - loss: 0.5705 - accuracy: 0.7039 - precision: 0.6955 - recall: 0.7275 - val_loss: 0.5800 - val_accuracy: 0.6977 - val_precision: 0.7185 - val_recall: 0.6463\n",
      "Epoch 7/8\n",
      "107/107 [==============================] - 3s 24ms/step - loss: 0.5684 - accuracy: 0.7036 - precision: 0.6925 - recall: 0.7347 - val_loss: 0.6039 - val_accuracy: 0.6774 - val_precision: 0.7580 - val_recall: 0.5174\n",
      "Epoch 8/8\n",
      "107/107 [==============================] - 2s 23ms/step - loss: 0.5650 - accuracy: 0.7067 - precision: 0.6977 - recall: 0.7319 - val_loss: 0.5905 - val_accuracy: 0.6936 - val_precision: 0.7386 - val_recall: 0.5954\n",
      "acc 0.6793230175971985\n",
      "Score:  0.5905112624168396\n",
      "Accuracy:  0.693598210811615\n",
      "total vocab 33332\n",
      "Total words  1917494\n",
      "Converted 25654 words (7678 misses)\n",
      "glove\n",
      "Balanced\n",
      "cnn\n",
      "Epoch 1/8\n",
      "107/107 [==============================] - 3s 19ms/step - loss: 0.6280 - accuracy: 0.6589 - precision: 0.6640 - recall: 0.6446 - val_loss: 0.5700 - val_accuracy: 0.7076 - val_precision: 0.7359 - val_recall: 0.6439\n",
      "Epoch 2/8\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.5415 - accuracy: 0.7269 - precision: 0.7322 - recall: 0.7174 - val_loss: 0.5549 - val_accuracy: 0.7183 - val_precision: 0.7366 - val_recall: 0.6761\n",
      "Epoch 3/8\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 0.5006 - accuracy: 0.7602 - precision: 0.7624 - recall: 0.7574 - val_loss: 0.5492 - val_accuracy: 0.7239 - val_precision: 0.7165 - val_recall: 0.7373\n",
      "Epoch 4/8\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 0.4665 - accuracy: 0.7835 - precision: 0.7850 - recall: 0.7821 - val_loss: 0.5486 - val_accuracy: 0.7210 - val_precision: 0.7420 - val_recall: 0.6741\n",
      "Epoch 5/8\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 0.4357 - accuracy: 0.8013 - precision: 0.8050 - recall: 0.7965 - val_loss: 0.5467 - val_accuracy: 0.7238 - val_precision: 0.7197 - val_recall: 0.7293\n",
      "Epoch 6/8\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.4062 - accuracy: 0.8199 - precision: 0.8208 - recall: 0.8193 - val_loss: 0.5557 - val_accuracy: 0.7201 - val_precision: 0.7332 - val_recall: 0.6885\n",
      "Epoch 7/8\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 0.3745 - accuracy: 0.8381 - precision: 0.8388 - recall: 0.8379 - val_loss: 0.5570 - val_accuracy: 0.7204 - val_precision: 0.7313 - val_recall: 0.6933\n",
      "Epoch 8/8\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 0.3520 - accuracy: 0.8514 - precision: 0.8545 - recall: 0.8477 - val_loss: 0.5701 - val_accuracy: 0.7141 - val_precision: 0.7390 - val_recall: 0.6584\n",
      "acc 0.7075790762901306\n",
      "Score:  0.5701037049293518\n",
      "Accuracy:  0.7140544652938843\n",
      "rnn\n",
      "Epoch 1/8\n",
      "107/107 [==============================] - 14s 115ms/step - loss: 0.6566 - accuracy: 0.6377 - precision: 0.6377 - recall: 0.6391 - val_loss: 0.6026 - val_accuracy: 0.6781 - val_precision: 0.6475 - val_recall: 0.7766\n",
      "Epoch 2/8\n",
      "107/107 [==============================] - 12s 113ms/step - loss: 0.5895 - accuracy: 0.6944 - precision: 0.6875 - recall: 0.7149 - val_loss: 0.5928 - val_accuracy: 0.6876 - val_precision: 0.6925 - val_recall: 0.6702\n",
      "Epoch 3/8\n",
      "107/107 [==============================] - 12s 112ms/step - loss: 0.5732 - accuracy: 0.7064 - precision: 0.7005 - recall: 0.7233 - val_loss: 0.5880 - val_accuracy: 0.6923 - val_precision: 0.6862 - val_recall: 0.7042\n",
      "Epoch 4/8\n",
      "107/107 [==============================] - 12s 112ms/step - loss: 0.5585 - accuracy: 0.7168 - precision: 0.7120 - recall: 0.7300 - val_loss: 0.5848 - val_accuracy: 0.6954 - val_precision: 0.6826 - val_recall: 0.7258\n",
      "Epoch 5/8\n",
      "107/107 [==============================] - 13s 123ms/step - loss: 0.5398 - accuracy: 0.7307 - precision: 0.7254 - recall: 0.7444 - val_loss: 0.5891 - val_accuracy: 0.6979 - val_precision: 0.6799 - val_recall: 0.7432\n",
      "Epoch 6/8\n",
      "107/107 [==============================] - 15s 136ms/step - loss: 0.5325 - accuracy: 0.7373 - precision: 0.7319 - recall: 0.7507 - val_loss: 0.5914 - val_accuracy: 0.6958 - val_precision: 0.6920 - val_recall: 0.7012\n",
      "Epoch 7/8\n",
      "107/107 [==============================] - 13s 126ms/step - loss: 0.5099 - accuracy: 0.7523 - precision: 0.7473 - recall: 0.7639 - val_loss: 0.5992 - val_accuracy: 0.6929 - val_precision: 0.6877 - val_recall: 0.7021\n",
      "Epoch 8/8\n",
      "107/107 [==============================] - 14s 129ms/step - loss: 0.4990 - accuracy: 0.7596 - precision: 0.7537 - recall: 0.7727 - val_loss: 0.6063 - val_accuracy: 0.6893 - val_precision: 0.6630 - val_recall: 0.7651\n",
      "acc 0.678145706653595\n",
      "Score:  0.6062979102134705\n",
      "Accuracy:  0.6893303990364075\n",
      "lstm\n",
      "Epoch 1/8\n",
      "107/107 [==============================] - 8s 41ms/step - loss: 0.6110 - accuracy: 0.6709 - precision: 0.6614 - recall: 0.7015 - val_loss: 0.5720 - val_accuracy: 0.7005 - val_precision: 0.6877 - val_recall: 0.7302\n",
      "Epoch 2/8\n",
      "107/107 [==============================] - 3s 28ms/step - loss: 0.5640 - accuracy: 0.7117 - precision: 0.7062 - recall: 0.7271 - val_loss: 0.5718 - val_accuracy: 0.7007 - val_precision: 0.6613 - val_recall: 0.8177\n",
      "Epoch 3/8\n",
      "107/107 [==============================] - 3s 26ms/step - loss: 0.5407 - accuracy: 0.7267 - precision: 0.7206 - recall: 0.7424 - val_loss: 0.5521 - val_accuracy: 0.7146 - val_precision: 0.6867 - val_recall: 0.7852\n",
      "Epoch 4/8\n",
      "107/107 [==============================] - 3s 25ms/step - loss: 0.5185 - accuracy: 0.7410 - precision: 0.7336 - recall: 0.7586 - val_loss: 0.5487 - val_accuracy: 0.7204 - val_precision: 0.6879 - val_recall: 0.8026\n",
      "Epoch 5/8\n",
      "107/107 [==============================] - 3s 24ms/step - loss: 0.4978 - accuracy: 0.7548 - precision: 0.7460 - recall: 0.7744 - val_loss: 0.5468 - val_accuracy: 0.7294 - val_precision: 0.7133 - val_recall: 0.7633\n",
      "Epoch 6/8\n",
      "107/107 [==============================] - 3s 23ms/step - loss: 0.4781 - accuracy: 0.7678 - precision: 0.7590 - recall: 0.7860 - val_loss: 0.5424 - val_accuracy: 0.7307 - val_precision: 0.7314 - val_recall: 0.7258\n",
      "Epoch 7/8\n",
      "107/107 [==============================] - 2s 23ms/step - loss: 0.4540 - accuracy: 0.7826 - precision: 0.7740 - recall: 0.7996 - val_loss: 0.5381 - val_accuracy: 0.7277 - val_precision: 0.7247 - val_recall: 0.7311\n",
      "Epoch 8/8\n",
      "107/107 [==============================] - 3s 24ms/step - loss: 0.4284 - accuracy: 0.8002 - precision: 0.7932 - recall: 0.8134 - val_loss: 0.5547 - val_accuracy: 0.7307 - val_precision: 0.7124 - val_recall: 0.7701\n",
      "acc 0.70051509141922\n",
      "Score:  0.5546802878379822\n",
      "Accuracy:  0.7306843400001526\n",
      "gru\n",
      "Epoch 1/8\n",
      "107/107 [==============================] - 7s 37ms/step - loss: 0.6199 - accuracy: 0.6689 - precision: 0.6660 - recall: 0.6788 - val_loss: 0.5670 - val_accuracy: 0.7039 - val_precision: 0.6917 - val_recall: 0.7314\n",
      "Epoch 2/8\n",
      "107/107 [==============================] - 3s 24ms/step - loss: 0.5584 - accuracy: 0.7138 - precision: 0.7082 - recall: 0.7294 - val_loss: 0.5574 - val_accuracy: 0.7114 - val_precision: 0.6833 - val_recall: 0.7837\n",
      "Epoch 3/8\n",
      "107/107 [==============================] - 3s 25ms/step - loss: 0.5370 - accuracy: 0.7308 - precision: 0.7215 - recall: 0.7538 - val_loss: 0.5503 - val_accuracy: 0.7145 - val_precision: 0.6854 - val_recall: 0.7887\n",
      "Epoch 4/8\n",
      "107/107 [==============================] - 3s 25ms/step - loss: 0.5202 - accuracy: 0.7388 - precision: 0.7298 - recall: 0.7601 - val_loss: 0.5418 - val_accuracy: 0.7289 - val_precision: 0.7210 - val_recall: 0.7432\n",
      "Epoch 5/8\n",
      "107/107 [==============================] - 3s 24ms/step - loss: 0.5023 - accuracy: 0.7544 - precision: 0.7462 - recall: 0.7724 - val_loss: 0.5667 - val_accuracy: 0.7073 - val_precision: 0.6582 - val_recall: 0.8576\n",
      "Epoch 6/8\n",
      "107/107 [==============================] - 3s 24ms/step - loss: 0.4863 - accuracy: 0.7620 - precision: 0.7527 - recall: 0.7817 - val_loss: 0.5591 - val_accuracy: 0.7255 - val_precision: 0.7494 - val_recall: 0.6743\n",
      "Epoch 7/8\n",
      "107/107 [==============================] - 3s 24ms/step - loss: 0.4666 - accuracy: 0.7756 - precision: 0.7671 - recall: 0.7929 - val_loss: 0.5401 - val_accuracy: 0.7285 - val_precision: 0.7390 - val_recall: 0.7030\n",
      "Epoch 8/8\n",
      "107/107 [==============================] - 3s 24ms/step - loss: 0.4428 - accuracy: 0.7908 - precision: 0.7842 - recall: 0.8036 - val_loss: 0.5454 - val_accuracy: 0.7261 - val_precision: 0.7248 - val_recall: 0.7255\n",
      "acc 0.7038999199867249\n",
      "Score:  0.5453683733940125\n",
      "Accuracy:  0.7261221408843994\n",
      "total vocab 33332\n",
      "no_train\n",
      "Balanced\n",
      "cnn\n",
      "Epoch 1/8\n",
      "107/107 [==============================] - 3s 22ms/step - loss: 0.6310 - accuracy: 0.6622 - precision: 0.6560 - recall: 0.6833 - val_loss: 0.5613 - val_accuracy: 0.7142 - val_precision: 0.7154 - val_recall: 0.7077\n",
      "Epoch 2/8\n",
      "107/107 [==============================] - 2s 18ms/step - loss: 0.4768 - accuracy: 0.7745 - precision: 0.7744 - recall: 0.7759 - val_loss: 0.5577 - val_accuracy: 0.7135 - val_precision: 0.7108 - val_recall: 0.7160\n",
      "Epoch 3/8\n",
      "107/107 [==============================] - 2s 18ms/step - loss: 0.3302 - accuracy: 0.8607 - precision: 0.8570 - recall: 0.8666 - val_loss: 0.6416 - val_accuracy: 0.6946 - val_precision: 0.7074 - val_recall: 0.6596\n",
      "Epoch 4/8\n",
      "107/107 [==============================] - 2s 19ms/step - loss: 0.2117 - accuracy: 0.9186 - precision: 0.9163 - recall: 0.9217 - val_loss: 0.7681 - val_accuracy: 0.6859 - val_precision: 0.6933 - val_recall: 0.6625\n",
      "Epoch 5/8\n",
      "107/107 [==============================] - 2s 18ms/step - loss: 0.1384 - accuracy: 0.9505 - precision: 0.9497 - recall: 0.9516 - val_loss: 0.8891 - val_accuracy: 0.6829 - val_precision: 0.6761 - val_recall: 0.6971\n",
      "Epoch 6/8\n",
      "107/107 [==============================] - 2s 18ms/step - loss: 0.0950 - accuracy: 0.9682 - precision: 0.9679 - recall: 0.9686 - val_loss: 1.0386 - val_accuracy: 0.6776 - val_precision: 0.6775 - val_recall: 0.6729\n",
      "Epoch 7/8\n",
      "107/107 [==============================] - 2s 19ms/step - loss: 0.0692 - accuracy: 0.9770 - precision: 0.9777 - recall: 0.9764 - val_loss: 1.1556 - val_accuracy: 0.6783 - val_precision: 0.6839 - val_recall: 0.6584\n",
      "Epoch 8/8\n",
      "107/107 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9819 - precision: 0.9820 - recall: 0.9818 - val_loss: 1.2510 - val_accuracy: 0.6703 - val_precision: 0.6654 - val_recall: 0.6800\n",
      "acc 0.7142016291618347\n",
      "Score:  1.2510353326797485\n",
      "Accuracy:  0.6703458428382874\n",
      "rnn\n",
      "Epoch 1/8\n",
      "107/107 [==============================] - 19s 169ms/step - loss: 0.3336 - accuracy: 0.8214 - precision: 0.8152 - recall: 0.8315 - val_loss: 0.7686 - val_accuracy: 0.6759 - val_precision: 0.6743 - val_recall: 0.6755\n",
      "Epoch 2/8\n",
      "107/107 [==============================] - 17s 161ms/step - loss: 0.1759 - accuracy: 0.9342 - precision: 0.9306 - recall: 0.9386 - val_loss: 0.8978 - val_accuracy: 0.6646 - val_precision: 0.6532 - val_recall: 0.6962\n",
      "Epoch 3/8\n",
      "107/107 [==============================] - 15s 140ms/step - loss: 0.0908 - accuracy: 0.9698 - precision: 0.9681 - recall: 0.9718 - val_loss: 1.1177 - val_accuracy: 0.6624 - val_precision: 0.6584 - val_recall: 0.6693\n",
      "Epoch 4/8\n",
      "107/107 [==============================] - 16s 152ms/step - loss: 0.0532 - accuracy: 0.9848 - precision: 0.9847 - recall: 0.9850 - val_loss: 1.3540 - val_accuracy: 0.6628 - val_precision: 0.6623 - val_recall: 0.6590\n",
      "Epoch 5/8\n",
      "107/107 [==============================] - 16s 152ms/step - loss: 0.0379 - accuracy: 0.9905 - precision: 0.9903 - recall: 0.9907 - val_loss: 1.4780 - val_accuracy: 0.6559 - val_precision: 0.6599 - val_recall: 0.6377\n",
      "Epoch 6/8\n",
      "107/107 [==============================] - 16s 151ms/step - loss: 0.0293 - accuracy: 0.9927 - precision: 0.9923 - recall: 0.9932 - val_loss: 1.4592 - val_accuracy: 0.6547 - val_precision: 0.6397 - val_recall: 0.7024\n",
      "Epoch 7/8\n",
      "107/107 [==============================] - 17s 160ms/step - loss: 0.0266 - accuracy: 0.9933 - precision: 0.9929 - recall: 0.9937 - val_loss: 1.5789 - val_accuracy: 0.6614 - val_precision: 0.6565 - val_recall: 0.6714\n",
      "Epoch 8/8\n",
      "107/107 [==============================] - 16s 146ms/step - loss: 0.0232 - accuracy: 0.9936 - precision: 0.9939 - recall: 0.9933 - val_loss: 1.5797 - val_accuracy: 0.6577 - val_precision: 0.6509 - val_recall: 0.6743\n",
      "acc 0.6759381890296936\n",
      "Score:  1.5797187089920044\n",
      "Accuracy:  0.6576894521713257\n",
      "lstm\n",
      "Epoch 1/8\n",
      "107/107 [==============================] - 9s 44ms/step - loss: 0.3246 - accuracy: 0.8356 - precision: 0.8267 - recall: 0.8497 - val_loss: 0.7985 - val_accuracy: 0.6784 - val_precision: 0.6495 - val_recall: 0.7695\n",
      "Epoch 2/8\n",
      "107/107 [==============================] - 3s 28ms/step - loss: 0.1713 - accuracy: 0.9340 - precision: 0.9310 - recall: 0.9379 - val_loss: 0.9102 - val_accuracy: 0.6687 - val_precision: 0.6687 - val_recall: 0.6637\n",
      "Epoch 3/8\n",
      "107/107 [==============================] - 3s 28ms/step - loss: 0.1170 - accuracy: 0.9565 - precision: 0.9539 - recall: 0.9596 - val_loss: 1.1310 - val_accuracy: 0.6624 - val_precision: 0.6533 - val_recall: 0.6862\n",
      "Epoch 4/8\n",
      "107/107 [==============================] - 3s 26ms/step - loss: 0.0862 - accuracy: 0.9699 - precision: 0.9691 - recall: 0.9708 - val_loss: 1.3023 - val_accuracy: 0.6623 - val_precision: 0.6665 - val_recall: 0.6442\n",
      "Epoch 5/8\n",
      "107/107 [==============================] - 3s 26ms/step - loss: 0.0668 - accuracy: 0.9774 - precision: 0.9772 - recall: 0.9776 - val_loss: 1.4039 - val_accuracy: 0.6581 - val_precision: 0.6411 - val_recall: 0.7125\n",
      "Epoch 6/8\n",
      "107/107 [==============================] - 3s 27ms/step - loss: 0.0541 - accuracy: 0.9813 - precision: 0.9809 - recall: 0.9819 - val_loss: 1.4876 - val_accuracy: 0.6519 - val_precision: 0.6414 - val_recall: 0.6829\n",
      "Epoch 7/8\n",
      "107/107 [==============================] - 3s 26ms/step - loss: 0.0418 - accuracy: 0.9868 - precision: 0.9859 - recall: 0.9878 - val_loss: 1.7292 - val_accuracy: 0.6490 - val_precision: 0.6377 - val_recall: 0.6835\n",
      "Epoch 8/8\n",
      "107/107 [==============================] - 3s 26ms/step - loss: 0.0382 - accuracy: 0.9870 - precision: 0.9869 - recall: 0.9873 - val_loss: 1.6459 - val_accuracy: 0.6477 - val_precision: 0.6385 - val_recall: 0.6743\n",
      "acc 0.6784400343894958\n",
      "Score:  1.6459084749221802\n",
      "Accuracy:  0.6476821303367615\n",
      "gru\n",
      "Epoch 1/8\n",
      "107/107 [==============================] - 8s 37ms/step - loss: 0.2429 - accuracy: 0.8510 - precision: 0.8503 - recall: 0.8523 - val_loss: 1.1199 - val_accuracy: 0.6564 - val_precision: 0.6510 - val_recall: 0.6681\n",
      "Epoch 2/8\n",
      "107/107 [==============================] - 3s 25ms/step - loss: 0.0928 - accuracy: 0.9656 - precision: 0.9651 - recall: 0.9663 - val_loss: 1.2996 - val_accuracy: 0.6490 - val_precision: 0.6290 - val_recall: 0.7199\n",
      "Epoch 3/8\n",
      "107/107 [==============================] - 3s 25ms/step - loss: 0.0584 - accuracy: 0.9794 - precision: 0.9789 - recall: 0.9800 - val_loss: 1.7253 - val_accuracy: 0.6512 - val_precision: 0.6391 - val_recall: 0.6885\n",
      "Epoch 4/8\n",
      "107/107 [==============================] - 3s 25ms/step - loss: 0.0420 - accuracy: 0.9856 - precision: 0.9861 - recall: 0.9851 - val_loss: 1.7622 - val_accuracy: 0.6453 - val_precision: 0.6450 - val_recall: 0.6401\n",
      "Epoch 5/8\n",
      "107/107 [==============================] - 3s 25ms/step - loss: 0.0347 - accuracy: 0.9883 - precision: 0.9885 - recall: 0.9881 - val_loss: 2.0011 - val_accuracy: 0.6493 - val_precision: 0.6435 - val_recall: 0.6631\n",
      "Epoch 6/8\n",
      "107/107 [==============================] - 3s 25ms/step - loss: 0.0276 - accuracy: 0.9905 - precision: 0.9907 - recall: 0.9904 - val_loss: 2.0830 - val_accuracy: 0.6459 - val_precision: 0.6415 - val_recall: 0.6551\n",
      "Epoch 7/8\n",
      "107/107 [==============================] - 3s 26ms/step - loss: 0.0243 - accuracy: 0.9914 - precision: 0.9910 - recall: 0.9918 - val_loss: 2.3294 - val_accuracy: 0.6387 - val_precision: 0.6304 - val_recall: 0.6634\n",
      "Epoch 8/8\n",
      "107/107 [==============================] - 3s 27ms/step - loss: 0.0245 - accuracy: 0.9914 - precision: 0.9919 - recall: 0.9909 - val_loss: 2.1490 - val_accuracy: 0.6408 - val_precision: 0.6345 - val_recall: 0.6572\n",
      "acc 0.6563649773597717\n",
      "Score:  2.149035930633545\n",
      "Accuracy:  0.6407652497291565\n"
     ]
    }
   ],
   "source": [
    "df_result = get_result_table()\n",
    "# train balance dataset\n",
    "df, dataset_name = load_dataset(4) \n",
    "train_dataset_all_model(df, dataset_name, df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>hate f1</th>\n",
       "      <th>non-hate f1</th>\n",
       "      <th>hate support</th>\n",
       "      <th>non-hate support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Balanced_word2vec_trained_cnn</td>\n",
       "      <td>0.729213</td>\n",
       "      <td>0.729339</td>\n",
       "      <td>0.729255</td>\n",
       "      <td>0.729196</td>\n",
       "      <td>0.731308</td>\n",
       "      <td>0.727084</td>\n",
       "      <td>3384.0</td>\n",
       "      <td>3411.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Balanced_word2vec_trained_rnn</td>\n",
       "      <td>0.676968</td>\n",
       "      <td>0.677053</td>\n",
       "      <td>0.677005</td>\n",
       "      <td>0.676955</td>\n",
       "      <td>0.679047</td>\n",
       "      <td>0.674863</td>\n",
       "      <td>3384.0</td>\n",
       "      <td>3411.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Balanced_word2vec_trained_lstm</td>\n",
       "      <td>0.713024</td>\n",
       "      <td>0.713111</td>\n",
       "      <td>0.713060</td>\n",
       "      <td>0.713014</td>\n",
       "      <td>0.714745</td>\n",
       "      <td>0.711282</td>\n",
       "      <td>3384.0</td>\n",
       "      <td>3411.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Balanced_word2vec_trained_gru</td>\n",
       "      <td>0.720088</td>\n",
       "      <td>0.720205</td>\n",
       "      <td>0.720129</td>\n",
       "      <td>0.720073</td>\n",
       "      <td>0.722174</td>\n",
       "      <td>0.717972</td>\n",
       "      <td>3384.0</td>\n",
       "      <td>3411.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Balanced_fasttext_trained_cnn</td>\n",
       "      <td>0.713466</td>\n",
       "      <td>0.714334</td>\n",
       "      <td>0.713333</td>\n",
       "      <td>0.713090</td>\n",
       "      <td>0.702703</td>\n",
       "      <td>0.723477</td>\n",
       "      <td>3384.0</td>\n",
       "      <td>3411.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Balanced_fasttext_trained_rnn</td>\n",
       "      <td>0.678440</td>\n",
       "      <td>0.678439</td>\n",
       "      <td>0.678441</td>\n",
       "      <td>0.678438</td>\n",
       "      <td>0.677681</td>\n",
       "      <td>0.679195</td>\n",
       "      <td>3384.0</td>\n",
       "      <td>3411.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Balanced_fasttext_trained_lstm</td>\n",
       "      <td>0.694923</td>\n",
       "      <td>0.696126</td>\n",
       "      <td>0.695072</td>\n",
       "      <td>0.694554</td>\n",
       "      <td>0.705163</td>\n",
       "      <td>0.683946</td>\n",
       "      <td>3384.0</td>\n",
       "      <td>3411.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Balanced_fasttext_trained_gru</td>\n",
       "      <td>0.693598</td>\n",
       "      <td>0.701012</td>\n",
       "      <td>0.693210</td>\n",
       "      <td>0.690471</td>\n",
       "      <td>0.659359</td>\n",
       "      <td>0.721583</td>\n",
       "      <td>3384.0</td>\n",
       "      <td>3411.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Balanced_glove_cnn</td>\n",
       "      <td>0.714054</td>\n",
       "      <td>0.716576</td>\n",
       "      <td>0.713834</td>\n",
       "      <td>0.713080</td>\n",
       "      <td>0.696359</td>\n",
       "      <td>0.729801</td>\n",
       "      <td>3384.0</td>\n",
       "      <td>3411.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Balanced_glove_rnn</td>\n",
       "      <td>0.689330</td>\n",
       "      <td>0.693955</td>\n",
       "      <td>0.689630</td>\n",
       "      <td>0.687680</td>\n",
       "      <td>0.710386</td>\n",
       "      <td>0.664974</td>\n",
       "      <td>3384.0</td>\n",
       "      <td>3411.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Balanced_glove_lstm</td>\n",
       "      <td>0.730684</td>\n",
       "      <td>0.732202</td>\n",
       "      <td>0.730840</td>\n",
       "      <td>0.730328</td>\n",
       "      <td>0.740131</td>\n",
       "      <td>0.720525</td>\n",
       "      <td>3384.0</td>\n",
       "      <td>3411.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced_glove_gru</td>\n",
       "      <td>0.726122</td>\n",
       "      <td>0.726118</td>\n",
       "      <td>0.726120</td>\n",
       "      <td>0.726119</td>\n",
       "      <td>0.725151</td>\n",
       "      <td>0.727086</td>\n",
       "      <td>3384.0</td>\n",
       "      <td>3411.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced_no_train_cnn</td>\n",
       "      <td>0.670346</td>\n",
       "      <td>0.670435</td>\n",
       "      <td>0.670384</td>\n",
       "      <td>0.670330</td>\n",
       "      <td>0.672610</td>\n",
       "      <td>0.668050</td>\n",
       "      <td>3384.0</td>\n",
       "      <td>3411.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Balanced_no_train_rnn</td>\n",
       "      <td>0.657689</td>\n",
       "      <td>0.657914</td>\n",
       "      <td>0.657755</td>\n",
       "      <td>0.657623</td>\n",
       "      <td>0.662409</td>\n",
       "      <td>0.652836</td>\n",
       "      <td>3384.0</td>\n",
       "      <td>3411.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Balanced_no_train_lstm</td>\n",
       "      <td>0.647682</td>\n",
       "      <td>0.648185</td>\n",
       "      <td>0.647788</td>\n",
       "      <td>0.647479</td>\n",
       "      <td>0.655936</td>\n",
       "      <td>0.639023</td>\n",
       "      <td>3384.0</td>\n",
       "      <td>3411.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Balanced_no_train_gru</td>\n",
       "      <td>0.640765</td>\n",
       "      <td>0.640969</td>\n",
       "      <td>0.640830</td>\n",
       "      <td>0.640697</td>\n",
       "      <td>0.645667</td>\n",
       "      <td>0.635726</td>\n",
       "      <td>3384.0</td>\n",
       "      <td>3411.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Model  Accuracy  precision    recall  f1-score  \\\n",
       "0    Balanced_word2vec_trained_cnn  0.729213   0.729339  0.729255  0.729196   \n",
       "1    Balanced_word2vec_trained_rnn  0.676968   0.677053  0.677005  0.676955   \n",
       "2   Balanced_word2vec_trained_lstm  0.713024   0.713111  0.713060  0.713014   \n",
       "3    Balanced_word2vec_trained_gru  0.720088   0.720205  0.720129  0.720073   \n",
       "4    Balanced_fasttext_trained_cnn  0.713466   0.714334  0.713333  0.713090   \n",
       "5    Balanced_fasttext_trained_rnn  0.678440   0.678439  0.678441  0.678438   \n",
       "6   Balanced_fasttext_trained_lstm  0.694923   0.696126  0.695072  0.694554   \n",
       "7    Balanced_fasttext_trained_gru  0.693598   0.701012  0.693210  0.690471   \n",
       "8               Balanced_glove_cnn  0.714054   0.716576  0.713834  0.713080   \n",
       "9               Balanced_glove_rnn  0.689330   0.693955  0.689630  0.687680   \n",
       "10             Balanced_glove_lstm  0.730684   0.732202  0.730840  0.730328   \n",
       "11              Balanced_glove_gru  0.726122   0.726118  0.726120  0.726119   \n",
       "12           Balanced_no_train_cnn  0.670346   0.670435  0.670384  0.670330   \n",
       "13           Balanced_no_train_rnn  0.657689   0.657914  0.657755  0.657623   \n",
       "14          Balanced_no_train_lstm  0.647682   0.648185  0.647788  0.647479   \n",
       "15           Balanced_no_train_gru  0.640765   0.640969  0.640830  0.640697   \n",
       "\n",
       "     hate f1  non-hate f1  hate support  non-hate support  \n",
       "0   0.731308     0.727084        3384.0            3411.0  \n",
       "1   0.679047     0.674863        3384.0            3411.0  \n",
       "2   0.714745     0.711282        3384.0            3411.0  \n",
       "3   0.722174     0.717972        3384.0            3411.0  \n",
       "4   0.702703     0.723477        3384.0            3411.0  \n",
       "5   0.677681     0.679195        3384.0            3411.0  \n",
       "6   0.705163     0.683946        3384.0            3411.0  \n",
       "7   0.659359     0.721583        3384.0            3411.0  \n",
       "8   0.696359     0.729801        3384.0            3411.0  \n",
       "9   0.710386     0.664974        3384.0            3411.0  \n",
       "10  0.740131     0.720525        3384.0            3411.0  \n",
       "11  0.725151     0.727086        3384.0            3411.0  \n",
       "12  0.672610     0.668050        3384.0            3411.0  \n",
       "13  0.662409     0.652836        3384.0            3411.0  \n",
       "14  0.655936     0.639023        3384.0            3411.0  \n",
       "15  0.645667     0.635726        3384.0            3411.0  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
